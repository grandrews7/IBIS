{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b173f532-ef4a-4da1-b3bf-ba7a390be0a2",
   "metadata": {},
   "source": [
    "# PWM-A2G Pipeline\n",
    "This notebook will walk you through how to reproduce our results for a single TF and cycle of HT-SELEX. For each combination of TF and cycle, we perform our CNN-based motif discovery algorithm (adapted here for HT-SELEX data) and add an additional step to derive a new threshold for inclusion of seqlets in the final motif. This helps increase the information content of the final motif, especially for earlier HT-SELEX cycles. The notebook is self-contained, so there are a decent amount of helper functions and classes. The notebook as is presented takes abount 20 minutes to run top to bottom, but this is only training the model for 200 epochs, but we trained for 1000 epochs in the actual pipeline. This is so you can more quickly ensure it runs to completion. I chose to highlight LEF1 C3 in this notebook as it demonstrates both the sensitivity of CNNs and the large gain information content of the final motif after applying the Gaussian mixture model derived threshold. Outline of what is performed below\n",
    "1. HT-SELEX reads are imported (all replicates for a given TF / cycle combination are used)\n",
    "2. Gapped kmer enrichment is performed to find the single most enriched gapped kmer\n",
    "3. Data generators are initilized (each batch consists of a random subset of reads with an equal number of negative sequences generated by dinucleotide shuffling)\n",
    "4. Single kernel CNN is initialized and seeded with the one-hot encoded, enriched gapped kmer\n",
    "5. Model is trained for 200 epochs\n",
    "6. Input sequeces are scanned with the convolution kernel and any seqlet with an activation > 0 is saved\n",
    "7. A 2-component Gaussian mixture model is used to derive a new threshold for the inclusion of seqlets in the final motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "497c6f76-4065-4460-99c3-449d6f7a76e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:17:32.714178Z",
     "iopub.status.busy": "2024-10-03T12:17:32.714000Z",
     "iopub.status.idle": "2024-10-03T12:17:33.780608Z",
     "shell.execute_reply": "2024-10-03T12:17:33.780090Z",
     "shell.execute_reply.started": "2024-10-03T12:17:32.714155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv1D, maximum, GlobalMaxPooling1D, Dense, GaussianNoise, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "\n",
    "import random\n",
    "from tqdm import trange\n",
    "from subprocess import Popen, PIPE, run\n",
    "import sys\n",
    "import pickle\n",
    "from pyfaidx import Fasta\n",
    "from  tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "import glob\n",
    "import bioframe\n",
    "import os\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import gzip\n",
    "import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce66087-fa31-4836-a096-54d3753e5a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.010075Z",
     "iopub.status.busy": "2024-10-03T11:51:21.009891Z",
     "iopub.status.idle": "2024-10-03T11:51:21.027847Z",
     "shell.execute_reply": "2024-10-03T11:51:21.027194Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.010050Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Models\n",
    "def construct_model(num_kernels=32,\n",
    "                    kernel_width=24,\n",
    "                    seq_len=None,\n",
    "                    dropout_prop=0.0,\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.0001, seed=12),\n",
    "                    optimizer='adam',\n",
    "                    activation='linear',\n",
    "                    num_classes=1,\n",
    "                    l1_reg=0.0,\n",
    "                    l2_reg= 0.0,\n",
    "                    gaussian_noise = 0.1,\n",
    "                    spatial_dropout = 0.0,\n",
    "                    rc = True,\n",
    "                    padding=\"same\",\n",
    "                    conv_name=\"shared_conv\"):\n",
    "    if rc:\n",
    "        seq_input = Input(shape=(seq_len,4))\n",
    "        rc_op = Lambda(lambda x: K.reverse(x,axes=(1,2)))\n",
    "        seq_rc = rc_op(seq_input)\n",
    "        if gaussian_noise > 0.0:\n",
    "            noisy_seq = GaussianNoise(gaussian_noise)(seq_input)\n",
    "            noisy_seq_rc = rc_op(noisy_seq)\n",
    "        \n",
    "        shared_conv = Conv1D(num_kernels, kernel_width,\n",
    "                             strides=1,\n",
    "                             padding=padding, \n",
    "                             activation=activation,\n",
    "                             use_bias=use_bias,\n",
    "                             kernel_initializer=kernel_initializer,\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=l1_reg,\n",
    "                                                                   l2=l2_reg),\n",
    "                             bias_initializer='zeros',\n",
    "                             name=conv_name)\n",
    "\n",
    "        if gaussian_noise > 0:\n",
    "            conv_for = shared_conv(noisy_seq)\n",
    "            conv_rc = shared_conv(noisy_seq_rc)\n",
    "        else:\n",
    "            conv_for = shared_conv(seq_input)\n",
    "            conv_rc = shared_conv(seq_rc)\n",
    "            \n",
    "\n",
    "        merged = maximum([conv_for, conv_rc])\n",
    "        pooled = GlobalMaxPooling1D()(merged)\n",
    "        if dropout_prop > 0.0:\n",
    "            dropout = Dropout(dropout_prop)(pooled)\n",
    "            output = Dense(1, activation='sigmoid',\n",
    "                       use_bias=True,\n",
    "                       kernel_initializer=initializers.RandomUniform(minval=0.0, maxval=0.001, seed=12), \n",
    "                       kernel_constraint=non_neg(), \n",
    "                       bias_initializer='zeros',\n",
    "                       name=\"dense_1\")(dropout)\n",
    "        else:\n",
    "            output = Dense(1, activation='sigmoid',\n",
    "                           use_bias=True,\n",
    "                           kernel_initializer=initializers.RandomUniform(minval=0.0, maxval=0.001, seed=12), \n",
    "                           kernel_constraint=non_neg(), \n",
    "                           bias_initializer='zeros',\n",
    "                           name=\"dense_1\")(pooled)\n",
    "        model = Model(inputs=seq_input, outputs=output)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "def construct_scan_model(conv_weights):\n",
    "    kernel_width = conv_weights.shape[0]\n",
    "    num_kernels = conv_weights.shape[2]\n",
    "    seq = Input(shape=(None,4))\n",
    "    conv = Conv1D(num_kernels, kernel_width, \n",
    "                  name = 'scan_conv',\n",
    "                  strides=1, \n",
    "                  padding='valid', \n",
    "                  activation='linear', \n",
    "                  use_bias=False, \n",
    "                  kernel_initializer='zeros', \n",
    "                  bias_initializer='zeros',\n",
    "                  trainable=False)\n",
    "    \n",
    "    conv_seq = conv(seq)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=seq, outputs=conv_seq)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.get_layer('scan_conv').set_weights([conv_weights])\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_score_model(conv_weights):\n",
    "    kernel_width = conv_weights.shape[0]\n",
    "    num_kernels = conv_weights.shape[2]\n",
    "    seq = Input(shape=(None,4))\n",
    "    rc_op = Lambda(lambda x: K.reverse(x,axes=(1,2)))\n",
    "    seq_rc = rc_op(seq)\n",
    "    \n",
    "    conv = Conv1D(num_kernels, kernel_width, \n",
    "                  name = 'score_conv',\n",
    "                  strides=1, \n",
    "                  padding='valid', \n",
    "                  activation='linear', \n",
    "                  use_bias=use_bias, \n",
    "                  kernel_initializer='zeros', \n",
    "                  bias_initializer='zeros',\n",
    "                  trainable=False)\n",
    "    \n",
    "    conv_for = conv(seq)\n",
    "    conv_rc = conv(seq_rc)\n",
    "    \n",
    "    merged = maximum([conv_for, conv_rc])\n",
    "    pooled = GlobalMaxPooling1D()(merged)\n",
    "    model = Model(inputs=seq, outputs=pooled)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.get_layer(\"score_conv\").set_weights([conv_weights])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a24514-8408-4aa0-b633-2f761a277a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.029564Z",
     "iopub.status.busy": "2024-10-03T11:51:21.029384Z",
     "iopub.status.idle": "2024-10-03T11:51:21.224617Z",
     "shell.execute_reply": "2024-10-03T11:51:21.224100Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.029538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# P. Clote, Oct 2003\n",
    "\n",
    "def computeCountAndLists(s):\n",
    "\n",
    "    #Initialize lists and mono- and dinucleotide dictionaries\n",
    "    List = {} #List is a dictionary of lists\n",
    "    List['A'] = []; List['C'] = [];\n",
    "    List['G'] = []; List['T'] = [];\n",
    "    # FIXME: is this ok?\n",
    "    List['N'] = []\n",
    "    nuclList   = [\"A\",\"C\",\"G\",\"T\",\"N\"]\n",
    "    s       = s.upper()\n",
    "    #s       = s.replace(\"U\",\"T\")\n",
    "    nuclCnt    = {}  #empty dictionary\n",
    "    dinuclCnt  = {}  #empty dictionary\n",
    "    for x in nuclList:\n",
    "        nuclCnt[x]=0\n",
    "        dinuclCnt[x]={}\n",
    "        for y in nuclList:\n",
    "            dinuclCnt[x][y]=0\n",
    "\n",
    "    #Compute count and lists\n",
    "    nuclCnt[s[0]] = 1\n",
    "    nuclTotal     = 1\n",
    "    dinuclTotal   = 0\n",
    "    for i in range(len(s)-1):\n",
    "        x = s[i]; y = s[i+1]\n",
    "        List[x].append( y )\n",
    "        nuclCnt[y] += 1; nuclTotal  += 1\n",
    "        dinuclCnt[x][y] += 1; dinuclTotal += 1\n",
    "    assert (nuclTotal==len(s))\n",
    "    assert (dinuclTotal==len(s)-1)\n",
    "    return nuclCnt,dinuclCnt,List\n",
    "\n",
    "\n",
    "def chooseEdge(x,dinuclCnt):\n",
    "    z = random.random()\n",
    "    denom=dinuclCnt[x]['A']+dinuclCnt[x]['C']+dinuclCnt[x]['G']+dinuclCnt[x]['T']+dinuclCnt[x]['N']\n",
    "    numerator = dinuclCnt[x]['A']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['A'] -= 1\n",
    "        return 'A'\n",
    "    numerator += dinuclCnt[x]['C']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['C'] -= 1\n",
    "        return 'C'\n",
    "    numerator += dinuclCnt[x]['G']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['G'] -= 1\n",
    "        return 'G'\n",
    "    numerator += dinuclCnt[x]['T']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['T'] -= 1\n",
    "        return 'T'\n",
    "    dinuclCnt[x]['N'] -= 1\n",
    "    return 'N'\n",
    "\n",
    "def connectedToLast(edgeList,nuclList,lastCh):\n",
    "    D = {}\n",
    "    for x in nuclList: D[x]=0\n",
    "    for edge in edgeList:\n",
    "        a = edge[0]; b = edge[1]\n",
    "        if b==lastCh: D[a]=1\n",
    "    for i in range(3):\n",
    "        for edge in edgeList:\n",
    "            a = edge[0]; b = edge[1]\n",
    "            if D[b]==1: D[a]=1\n",
    "    ok = 0\n",
    "    for x in nuclList:\n",
    "        if x!=lastCh and D[x]==0: return 0\n",
    "    return 1\n",
    "\n",
    "def eulerian(s):\n",
    "    nuclCnt,dinuclCnt,List = computeCountAndLists(s)\n",
    "    #compute nucleotides appearing in s\n",
    "    nuclList = []\n",
    "    for x in [\"A\",\"C\",\"G\",\"T\",\"N\"]:\n",
    "        if x in s: nuclList.append(x)\n",
    "    #create dinucleotide shuffle L\n",
    "    firstCh = s[0]  #start with first letter of s\n",
    "    lastCh  = s[-1]\n",
    "    edgeList = []\n",
    "    for x in nuclList:\n",
    "        if x!= lastCh: edgeList.append( [x,chooseEdge(x,dinuclCnt)] )\n",
    "    ok = connectedToLast(edgeList,nuclList,lastCh)\n",
    "    return ok,edgeList,nuclList,lastCh\n",
    "\n",
    "\n",
    "def shuffleEdgeList(L):\n",
    "    n = len(L); barrier = n\n",
    "    for i in range(n-1):\n",
    "        z = int(random.random() * barrier)\n",
    "        tmp = L[z]\n",
    "        L[z]= L[barrier-1]\n",
    "        L[barrier-1] = tmp\n",
    "        barrier -= 1\n",
    "    return L\n",
    "\n",
    "def dinuclShuffle(s):\n",
    "    ok = 0\n",
    "    while not ok:\n",
    "        ok,edgeList,nuclList,lastCh = eulerian(s)\n",
    "    nuclCnt,dinuclCnt,List = computeCountAndLists(s)\n",
    "\n",
    "    #remove last edges from each vertex list, shuffle, then add back\n",
    "    #the removed edges at end of vertex lists.\n",
    "    for [x,y] in edgeList: List[x].remove(y)\n",
    "    for x in nuclList: shuffleEdgeList(List[x])\n",
    "    for [x,y] in edgeList: List[x].append(y)\n",
    "\n",
    "    #construct the eulerian path\n",
    "    L = [s[0]]; prevCh = s[0]\n",
    "    for i in range(len(s)-2):\n",
    "        ch = List[prevCh][0]\n",
    "        L.append( ch )\n",
    "        del List[prevCh][0]\n",
    "        prevCh = ch\n",
    "    L.append(s[-1])\n",
    "    #t = string.join(L,\"\")\n",
    "    t = \"\".join(L)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566cfc97-5802-4c1b-a51e-799cad8c6b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.225923Z",
     "iopub.status.busy": "2024-10-03T11:51:21.225747Z",
     "iopub.status.idle": "2024-10-03T11:51:21.239765Z",
     "shell.execute_reply": "2024-10-03T11:51:21.239217Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.225897Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_information_content(x):\n",
    "    ic = x * np.log2((x + .001) / .25)\n",
    "    if ic > 0:\n",
    "        return(ic)\n",
    "    else:\n",
    "        return(0.0)\n",
    "    \n",
    "def get_info_content(ppm):\n",
    "    w = ppm.shape[0]\n",
    "    info = np.zeros(w)\n",
    "    for i in range(w):\n",
    "        for j in range(4):\n",
    "            info[i] += ppm[i,j] * np.log2((ppm[i,j] + .001) / 0.25)\n",
    "    return(info)\n",
    "    \n",
    "def trim_ppm(ppm, min_info=0.0):\n",
    "    info = get_info_content(ppm)\n",
    "    start_index = 0\n",
    "    w = ppm.shape[0]\n",
    "    stop_index = w\n",
    "    for i in range(w):\n",
    "        if info[i] < min_info:\n",
    "            start_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for i in range(w):\n",
    "        if info[w-i-1] < 0.25:\n",
    "            stop_index -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if np.max(info) < 0.25:\n",
    "        return(ppm, 0, w)\n",
    "    else:\n",
    "        return(ppm[start_index:stop_index,:], start_index, stop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1ec541-b340-4148-ab86-95280c9bd2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.240820Z",
     "iopub.status.busy": "2024-10-03T11:51:21.240647Z",
     "iopub.status.idle": "2024-10-03T11:51:21.253919Z",
     "shell.execute_reply": "2024-10-03T11:51:21.253369Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.240795Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DNA_SEQ_DICT = {\n",
    "    'A' : [1, 0, 0, 0],\n",
    "    'C' : [0, 1, 0, 0],\n",
    "    'G' : [0, 0, 1, 0],\n",
    "    'T' : [0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "def encode_sequence(seq, N = [0, 0, 0, 0], seq_dict = None, useN = None):\n",
    "    if seq_dict is None:\n",
    "        seq_dict = DNA_SEQ_DICT\n",
    "    if useN == 'uniform':\n",
    "        N = [(1/len(seq_dict)) for _ in seq_dict]\n",
    "    elif useN == 'zeros':\n",
    "        N = [0 for _ in seq_dict]\n",
    "    d = { **seq_dict, 'N' : N }\n",
    "    return np.array([d[nuc] for nuc in list(seq)]).astype('float32')\n",
    " \n",
    "def decode_sequence(encoded_seq, seq_dict = None):\n",
    "    if seq_dict is None:\n",
    "        seq_dict = DNA_SEQ_DICT\n",
    "    seq_list = encoded_seq.astype('int').tolist()\n",
    "    def decode_base(encoded_base):\n",
    "        for letter,onehot in seq_dict.items():\n",
    "            if np.array_equal(encoded_base, onehot):\n",
    "                return letter\n",
    "        return \"N\"\n",
    "    return \"\".join(decode_base(b) for b in encoded_seq.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875247d2-9421-43b3-9cca-7f0e09fc7041",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.254979Z",
     "iopub.status.busy": "2024-10-03T11:51:21.254797Z",
     "iopub.status.idle": "2024-10-03T11:51:21.273020Z",
     "shell.execute_reply": "2024-10-03T11:51:21.272472Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.254953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from  tensorflow.keras.callbacks import Callback\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2,\n",
    "                 shape=\"cosine\"):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.history = {}\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        #print(fraction_to_restart)\n",
    "        if self.shape == \"cosine\":\n",
    "            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        else:\n",
    "            if fraction_to_restart < 0.5:\n",
    "                lr = fraction_to_restart * (self.max_lr - self.min_lr) / 0.5 + self.min_lr\n",
    "            else:\n",
    "                lr = (1 - fraction_to_restart) * (self.max_lr - self.min_lr) / 0.5 + self.min_lr\n",
    "        self.learning_rates.append(lr)\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        if self.shape == \"cosine\":\n",
    "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "class SWA(Callback):\n",
    "\n",
    "    def __init__(self, epochs_to_train, prop = 0.2, interval = 1):\n",
    "        super(SWA, self).__init__()\n",
    "        self.epochs_to_train = epochs_to_train\n",
    "        self.prop = prop\n",
    "        self.interval = interval\n",
    "        self.n_models = 0\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        self.weights = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        if epoch % self.interval == 0:\n",
    "            self.weights.append(self.model.get_weights())\n",
    "            self.n_models += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        num_models_to_average = int(np.ceil(self.prop * self.epoch))\n",
    "        new_weights = list()\n",
    "        for weights_list_tuple in zip(*self.weights[-num_models_to_average:]): \n",
    "            new_weights.append(\n",
    "                np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n",
    "            )\n",
    "        self.model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6773dd6-0fec-4722-b42a-d0e43d1c4d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.274024Z",
     "iopub.status.busy": "2024-10-03T11:51:21.273847Z",
     "iopub.status.idle": "2024-10-03T11:51:21.294387Z",
     "shell.execute_reply": "2024-10-03T11:51:21.293850Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.273998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rc(re):\n",
    "    \"\"\"\n",
    "    Return the reverse complement of a DNA/RNA RE.\n",
    "    \"\"\"\n",
    "    return re.translate(str.maketrans('ACGTURYKMBVDHSWN', 'TGCAAYRMKVBHDSWN'))[::-1]\n",
    "\n",
    "\n",
    "def count_seqs_with_words(seqs, halflength, ming, maxg, alpha, revcomp, desc):\n",
    "    if alpha == 'protein':\n",
    "        ambiguous_character = 'X'\n",
    "    else:\n",
    "        ambiguous_character = 'N'\n",
    "    gapped_kmer_dict = {}  # each key is the gapped k-mer word\n",
    "    for g in trange(ming, maxg + 1, 1, desc=desc):\n",
    "        w = g+2*halflength # length of the word\n",
    "        gap = g * ambiguous_character\n",
    "        for seq in seqs:\n",
    "            slen = len(seq)\n",
    "            for i in range(0, slen-w+1):\n",
    "                word = seq[i : i+w]\n",
    "                # skip word if it contains an ambiguous character\n",
    "                if ambiguous_character in word:\n",
    "                    continue\n",
    "                # convert word to a gapped word. Only the first and last half-length letters are preserved\n",
    "                word = word[0:halflength] + gap + word[-halflength:]\n",
    "                update_gapped_kmer_dict(gapped_kmer_dict, word, revcomp)\n",
    "    return gapped_kmer_dict\n",
    "\n",
    "\n",
    "def update_gapped_kmer_dict(gapped_kmer_dict, word, revcomp):\n",
    "    # use the lower alphabet word for rc\n",
    "    if revcomp:\n",
    "        word = min(word, get_rc(word))\n",
    "    if word in gapped_kmer_dict:  # word has been encountered before, add 1\n",
    "        gapped_kmer_dict[word] += 1\n",
    "    else:  # word has not been encountered before, create new key\n",
    "        gapped_kmer_dict[word] = 1\n",
    "\n",
    "\n",
    "def get_zscores(pos_seq_counts, neg_seq_counts):\n",
    "    zscores_dict = {}\n",
    "    for word in pos_seq_counts:\n",
    "        p = pos_seq_counts[word]\n",
    "        if word in neg_seq_counts:\n",
    "            n = neg_seq_counts[word]\n",
    "        else:\n",
    "            n = 1\n",
    "        zscore = 1.0*(p - n)/np.sqrt(n)\n",
    "        zscores_dict[word] = zscore\n",
    "    return zscores_dict\n",
    "\n",
    "\n",
    "# returns the words in order, from largest to smallest, by z-scores\n",
    "def sorted_zscore_keys(zscores_dict):\n",
    "    sorted_keys = sorted(zscores_dict, key=zscores_dict.__getitem__, reverse=True)\n",
    "    return sorted_keys\n",
    "\n",
    "\n",
    "def find_n_top_words(zscores_dict, num_find):\n",
    "    keys = np.array(list(zscores_dict.keys()))\n",
    "    values = np.array(list(zscores_dict.values()))\n",
    "    ind = np.argpartition(values, -num_find)[-num_find:]\n",
    "    top_words = list(keys[ind])\n",
    "    return top_words\n",
    "\n",
    "\n",
    "def find_enriched_gapped_kmers(pos_seqs, neg_seqs, halflength, ming, maxg, alpha, revcomp, num_find):\n",
    "    pos_seq_counts = count_seqs_with_words(pos_seqs, halflength, ming, maxg, alpha, revcomp,\n",
    "                                           'Searching positive sequences')\n",
    "    neg_seq_counts = count_seqs_with_words(neg_seqs, halflength, ming, maxg, alpha, revcomp,\n",
    "                                           'Searching negative sequences')\n",
    "    zscores = get_zscores(pos_seq_counts,neg_seq_counts)\n",
    "    top_words = find_n_top_words(zscores, num_find)\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351a4b1f-8407-4c42-a324-d35870141e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.296095Z",
     "iopub.status.busy": "2024-10-03T11:51:21.295903Z",
     "iopub.status.idle": "2024-10-03T11:51:21.310277Z",
     "shell.execute_reply": "2024-10-03T11:51:21.309763Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.296070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppm_to_pwm(ppm):\n",
    "    pwm = ppm + 1e-5\n",
    "    pwm = pwm / 0.25\n",
    "    pwm = np.log2(pwm)\n",
    "    return(pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fbce72-7ce3-4f49-9208-a9bc12eb2ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:21.311350Z",
     "iopub.status.busy": "2024-10-03T11:51:21.311176Z",
     "iopub.status.idle": "2024-10-03T11:51:21.361168Z",
     "shell.execute_reply": "2024-10-03T11:51:21.360663Z",
     "shell.execute_reply.started": "2024-10-03T11:51:21.311325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataGen(Sequence):\n",
    "    def __init__(self, posSeqs, \n",
    "                 negSeqs=None,\n",
    "                 batchSize = 32,\n",
    "                 seqsPerEpoch=20000,\n",
    "                 padBy = 24):\n",
    "        \n",
    "        self.posSeqs = posSeqs\n",
    "        self.L = np.max([len(x) for x in self.posSeqs])\n",
    "        print(\"Maximum sequence length = {}\".format(self.L))\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.b2 = self.batchSize // 2\n",
    "        self.seqsPerEpoch = seqsPerEpoch\n",
    "        self.padBy = padBy\n",
    "        \n",
    "        self.labels = np.array([1 for i in range(self.b2)] + [0 for i in range(self.b2)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(int(np.floor(self.seqsPerEpoch / self.batchSize)))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        posSample = random.sample(self.posSeqs, self.b2)\n",
    "        negSample = [dinuclShuffle(_) for _ in posSample]\n",
    "            \n",
    "        X = 0.25 * np.ones((self.batchSize, 2*self.padBy + self.L, 4))\n",
    "            \n",
    "        for i,seq in enumerate(posSample + negSample):\n",
    "            l = len(seq)\n",
    "            start = self.padBy + (self.L - l) // 2\n",
    "            stop = start + l\n",
    "            X[i,start:stop,:] = encode_sequence(seq)\n",
    "            \n",
    "        return(X, self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82baf254-86af-4380-9701-16319e532778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:18:50.037129Z",
     "iopub.status.busy": "2024-10-02T15:18:50.036854Z",
     "iopub.status.idle": "2024-10-02T15:18:50.039781Z",
     "shell.execute_reply": "2024-10-02T15:18:50.039231Z",
     "shell.execute_reply.started": "2024-10-02T15:18:50.037091Z"
    }
   },
   "source": [
    "## Parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48191a4-35f5-4190-ad3c-23bf45c5e28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:42.102524Z",
     "iopub.status.busy": "2024-10-03T11:51:42.102294Z",
     "iopub.status.idle": "2024-10-03T11:51:42.106229Z",
     "shell.execute_reply": "2024-10-03T11:51:42.105669Z",
     "shell.execute_reply.started": "2024-10-03T11:51:42.102497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change me\n",
    "TF = \"LEF1\"\n",
    "cycle = \"C3\"\n",
    "epochs = 200\n",
    "\n",
    "# you can leave me alone\n",
    "n_kmers = 1\n",
    "w = 30\n",
    "n_motifs = 1\n",
    "if n_kmers > n_motifs:\n",
    "    n_kmers = n_motifs\n",
    "use_bias = True\n",
    "l2 = 0.00001\n",
    "l1 = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b977dd7-d1de-484f-981b-fee23aeefeb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:31:07.538895Z",
     "iopub.status.busy": "2024-10-03T12:31:07.538687Z",
     "iopub.status.idle": "2024-10-03T12:31:07.544196Z",
     "shell.execute_reply": "2024-10-03T12:31:07.543651Z",
     "shell.execute_reply.started": "2024-10-03T12:31:07.538871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../demo/PWM-A2G/LEF1-C3.ibis-formatted.txt\n",
      "FASTQ files:\n",
      " ../data//HTS/LEF1/LEF1_R0_C3_lf5ACGACGCTCTTCCGATCTAT_rf3AGCCTCAGATCGGAAGAGCA.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../demo/PWM-A2G\"\n",
    "if not os.path.exists(output_dir):\n",
    "    run(\"mkdir -p {}\".format(output_dir), shell=True)\n",
    "\n",
    "weights_file = output_dir + \"/weights.h5\"\n",
    "# temporary file to dump motif instances\n",
    "motifs_bed = output_dir + \"/motifs.bed\"\n",
    "\n",
    "# final output file of motifs instances\n",
    "motifs_file = output_dir + \"/motifs.txt.gz\"\n",
    "\n",
    "# final motif in IBIS format\n",
    "motif_out = output_dir + \"/{}-{}.ibis-formatted.txt\".format(TF, cycle)\n",
    "print(motif_out)\n",
    "\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "data_dir = \"{}/HTS/{}/\".format(data_dir, TF)\n",
    "dataFiles = glob.glob(data_dir + TF + \"_*_\" + cycle + \"*\")\n",
    "print(\"FASTQ files:\\n\", *dataFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "962bfa97-f36f-4b77-b99d-7ef56a48ad20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:54.871175Z",
     "iopub.status.busy": "2024-10-03T11:51:54.870996Z",
     "iopub.status.idle": "2024-10-03T11:51:56.244181Z",
     "shell.execute_reply": "2024-10-03T11:51:56.243499Z",
     "shell.execute_reply.started": "2024-10-03T11:51:54.871149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 199083 total sequences\n"
     ]
    }
   ],
   "source": [
    "seqs = []\n",
    "for fastq in dataFiles:\n",
    "    with gzip.open(fastq) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i % 4 == 1:\n",
    "                seqs.append(str(line, encoding=\"utf-8\").strip().split()[0])\n",
    "\n",
    "print(\"There are {} total sequences\".format(len(seqs)))\n",
    "n = len(seqs)\n",
    "random.shuffle(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c22d75c9-5cef-42c8-ab7c-405851386b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:51:56.973272Z",
     "iopub.status.busy": "2024-10-03T11:51:56.973084Z",
     "iopub.status.idle": "2024-10-03T11:52:00.870001Z",
     "shell.execute_reply": "2024-10-03T11:52:00.869409Z",
     "shell.execute_reply.started": "2024-10-03T11:51:56.973246Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding enriched gapped kmers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching positive sequences: 100%|██████████| 19/19 [00:01<00:00, 12.20it/s]\n",
      "Searching negative sequences: 100%|██████████| 19/19 [00:01<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched kmers...\n",
      "\tCTTTGA\n"
     ]
    }
   ],
   "source": [
    "if n_kmers > 0:\n",
    "    print(\"Finding enriched gapped kmers\")\n",
    "    tmpPosSeqs = seqs[:5000]\n",
    "    kmers = find_enriched_gapped_kmers(tmpPosSeqs, [dinuclShuffle(_) for _ in tmpPosSeqs],  3, 0, 18, \"dna\", False, n_kmers)\n",
    "print(\"Enriched kmers...\\n\" + \"\\n\".join([\"\\t\" + _ for _ in kmers[::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2963718e-6371-4252-9e5c-47d986a86ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:52:03.804305Z",
     "iopub.status.busy": "2024-10-03T11:52:03.804079Z",
     "iopub.status.idle": "2024-10-03T11:52:03.858497Z",
     "shell.execute_reply": "2024-10-03T11:52:03.857877Z",
     "shell.execute_reply.started": "2024-10-03T11:52:03.804262Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length = 40\n",
      "Maximum sequence length = 40\n"
     ]
    }
   ],
   "source": [
    "holdOutP = .1\n",
    "n = len(seqs)\n",
    "\n",
    "\n",
    "trainGen = dataGen(seqs[:int((1 - holdOutP)*n)],\n",
    "                   seqsPerEpoch=5000, \n",
    "                   padBy=w)\n",
    "testGen = dataGen(seqs[int((1 - holdOutP)*n):],\n",
    "                  seqsPerEpoch=1000, padBy=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd01abb-1300-47c1-a21e-b1a4fe9d0beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:52:04.996654Z",
     "iopub.status.busy": "2024-10-03T11:52:04.996458Z",
     "iopub.status.idle": "2024-10-03T11:52:05.268189Z",
     "shell.execute_reply": "2024-10-03T11:52:05.267633Z",
     "shell.execute_reply.started": "2024-10-03T11:52:04.996627Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 4)      0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_conv (Conv1D)            (None, None, 1)      121         gaussian_noise[0][0]             \n",
      "                                                                 lambda[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maximum (Maximum)               (None, None, 1)      0           shared_conv[0][0]                \n",
      "                                                                 shared_conv[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 1)            0           maximum[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           global_max_pooling1d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = construct_model(num_kernels=n_motifs,\n",
    "                        kernel_width=w,\n",
    "                        use_bias=use_bias, \n",
    "                        l2_reg=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97719c9f-4ce0-4af7-b2a9-7b304edecbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:52:20.127590Z",
     "iopub.status.busy": "2024-10-03T11:52:20.127336Z",
     "iopub.status.idle": "2024-10-03T11:52:20.292084Z",
     "shell.execute_reply": "2024-10-03T11:52:20.291489Z",
     "shell.execute_reply.started": "2024-10-03T11:52:20.127560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_kmers > 0:\n",
    "    if use_bias:\n",
    "        conv_weights, conv_bias = model.get_layer(\"shared_conv\").get_weights()\n",
    "    else:\n",
    "        conv_weights = model.get_layer(\"shared_conv\").get_weights()[0]\n",
    "\n",
    "\n",
    "    for i in range(n_kmers):\n",
    "        kmer = kmers[i]\n",
    "        l = len(kmer)\n",
    "        conv_weights[((w - l)//2):(((w - l)//2)+l),:,i] = encode_sequence(kmer)\n",
    "\n",
    "\n",
    "    if use_bias:\n",
    "        model.get_layer(\"shared_conv\").set_weights([conv_weights, conv_bias])\n",
    "    else:\n",
    "        model.get_layer(\"shared_conv\").set_weights([conv_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a7bcbf-e71d-499c-b7bd-c0274d42500c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:52:24.119436Z",
     "iopub.status.busy": "2024-10-03T11:52:24.119202Z",
     "iopub.status.idle": "2024-10-03T11:52:24.123394Z",
     "shell.execute_reply": "2024-10-03T11:52:24.122803Z",
     "shell.execute_reply.started": "2024-10-03T11:52:24.119396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_lr = .001\n",
    "max_lr = .1\n",
    "lr_decay = (min_lr / max_lr) ** (1 / epochs)\n",
    "schedule = SGDRScheduler(min_lr=min_lr,\n",
    "                             max_lr=max_lr,\n",
    "                             steps_per_epoch=trainGen.__len__(),\n",
    "                             lr_decay=lr_decay,\n",
    "                             cycle_length=1,\n",
    "                             mult_factor=1.0, \n",
    "                             shape=\"triangular\")\n",
    "\n",
    "swa = SWA(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d592a27d-c46f-42f8-8033-45e48f6d18be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:52:31.444471Z",
     "iopub.status.busy": "2024-10-03T11:52:31.444247Z",
     "iopub.status.idle": "2024-10-03T11:56:55.275195Z",
     "shell.execute_reply": "2024-10-03T11:56:55.274529Z",
     "shell.execute_reply.started": "2024-10-03T11:52:31.444441Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6874 - acc: 0.5398Epoch 1/200\n",
      "156/156 [==============================] - 2s 11ms/step - loss: 0.6872 - acc: 0.5397 - val_loss: 0.6757 - val_acc: 0.5625\n",
      "Epoch 2/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6770 - acc: 0.5550Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6774 - acc: 0.5529 - val_loss: 0.6753 - val_acc: 0.5675\n",
      "Epoch 3/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6807 - acc: 0.5514Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6800 - acc: 0.5497 - val_loss: 0.6661 - val_acc: 0.5776\n",
      "Epoch 4/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6739 - acc: 0.5621Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6747 - acc: 0.5591 - val_loss: 0.6813 - val_acc: 0.5494\n",
      "Epoch 5/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6811 - acc: 0.5408Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6812 - acc: 0.5413 - val_loss: 0.6710 - val_acc: 0.5635\n",
      "Epoch 6/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6825 - acc: 0.5463Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6816 - acc: 0.5487 - val_loss: 0.6614 - val_acc: 0.5958\n",
      "Epoch 7/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6813 - acc: 0.5500Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6812 - acc: 0.5493 - val_loss: 0.6769 - val_acc: 0.5383\n",
      "Epoch 8/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6769 - acc: 0.5573Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6778 - acc: 0.5555 - val_loss: 0.6831 - val_acc: 0.5464\n",
      "Epoch 9/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6777 - acc: 0.5621Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6775 - acc: 0.5619 - val_loss: 0.6778 - val_acc: 0.5565\n",
      "Epoch 10/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6755 - acc: 0.5553Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6763 - acc: 0.5541 - val_loss: 0.6719 - val_acc: 0.5625\n",
      "Epoch 11/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6784 - acc: 0.5564Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6779 - acc: 0.5573 - val_loss: 0.6757 - val_acc: 0.5595\n",
      "Epoch 12/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6757 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6754 - acc: 0.5587 - val_loss: 0.6691 - val_acc: 0.5595\n",
      "Epoch 13/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6789 - acc: 0.5550Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6778 - acc: 0.5555 - val_loss: 0.6648 - val_acc: 0.5726\n",
      "Epoch 14/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6725 - acc: 0.5616Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6728 - acc: 0.5611 - val_loss: 0.6743 - val_acc: 0.5585\n",
      "Epoch 15/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6766 - acc: 0.5513Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6772 - acc: 0.5511 - val_loss: 0.6723 - val_acc: 0.5585\n",
      "Epoch 16/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6707 - acc: 0.5728Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6715 - acc: 0.5717 - val_loss: 0.6667 - val_acc: 0.5837\n",
      "Epoch 17/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6784 - acc: 0.5566Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6785 - acc: 0.5551 - val_loss: 0.6714 - val_acc: 0.5706\n",
      "Epoch 18/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6802 - acc: 0.5552Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6793 - acc: 0.5559 - val_loss: 0.6652 - val_acc: 0.5675\n",
      "Epoch 19/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6773 - acc: 0.5553Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6776 - acc: 0.5515 - val_loss: 0.6615 - val_acc: 0.5887\n",
      "Epoch 20/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6786 - acc: 0.5489Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6792 - acc: 0.5455 - val_loss: 0.6729 - val_acc: 0.5776\n",
      "Epoch 21/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6818 - acc: 0.5440Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6821 - acc: 0.5423 - val_loss: 0.6722 - val_acc: 0.5746\n",
      "Epoch 22/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6802 - acc: 0.5543Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6804 - acc: 0.5537 - val_loss: 0.6783 - val_acc: 0.5706\n",
      "Epoch 23/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6790 - acc: 0.5474Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6792 - acc: 0.5461 - val_loss: 0.6612 - val_acc: 0.5867\n",
      "Epoch 24/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6681 - acc: 0.5728Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5695 - val_loss: 0.6652 - val_acc: 0.5696\n",
      "Epoch 25/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6766 - acc: 0.5532Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6780 - acc: 0.5519 - val_loss: 0.6704 - val_acc: 0.5665\n",
      "Epoch 26/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6726 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6726 - acc: 0.5607 - val_loss: 0.6726 - val_acc: 0.5685\n",
      "Epoch 27/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5686Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6717 - acc: 0.5689 - val_loss: 0.6585 - val_acc: 0.5907\n",
      "Epoch 28/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6805 - acc: 0.5467Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6807 - acc: 0.5463 - val_loss: 0.6725 - val_acc: 0.5585\n",
      "Epoch 29/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6767 - acc: 0.5659Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6770 - acc: 0.5663 - val_loss: 0.6664 - val_acc: 0.5857\n",
      "Epoch 30/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6782 - acc: 0.5558Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6779 - acc: 0.5557 - val_loss: 0.6652 - val_acc: 0.6008\n",
      "Epoch 31/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6746 - acc: 0.5526Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6746 - acc: 0.5531 - val_loss: 0.6780 - val_acc: 0.5635\n",
      "Epoch 32/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6758 - acc: 0.5554Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6768 - acc: 0.5533 - val_loss: 0.6790 - val_acc: 0.5524\n",
      "Epoch 33/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6762 - acc: 0.5476Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6768 - acc: 0.5481 - val_loss: 0.6664 - val_acc: 0.5655\n",
      "Epoch 34/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6740 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6741 - acc: 0.5601 - val_loss: 0.6848 - val_acc: 0.5282\n",
      "Epoch 35/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6763 - acc: 0.5526Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6770 - acc: 0.5517 - val_loss: 0.6592 - val_acc: 0.5968\n",
      "Epoch 36/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6731 - acc: 0.5696Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6745 - acc: 0.5659 - val_loss: 0.6745 - val_acc: 0.5464\n",
      "Epoch 37/200\n",
      "150/156 [===========================>..] - ETA: 0s - loss: 0.6720 - acc: 0.5548Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6722 - acc: 0.5533 - val_loss: 0.6641 - val_acc: 0.5806\n",
      "Epoch 38/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6649 - acc: 0.5793Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6653 - acc: 0.5775 - val_loss: 0.6742 - val_acc: 0.5514\n",
      "Epoch 39/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6749 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6750 - acc: 0.5607 - val_loss: 0.6745 - val_acc: 0.5615\n",
      "Epoch 40/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6788 - acc: 0.5520Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6782 - acc: 0.5537 - val_loss: 0.6657 - val_acc: 0.5847\n",
      "Epoch 41/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6713 - acc: 0.5677Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6721 - acc: 0.5655 - val_loss: 0.6706 - val_acc: 0.5806\n",
      "Epoch 42/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6728 - acc: 0.5610Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6727 - acc: 0.5605 - val_loss: 0.6673 - val_acc: 0.5746\n",
      "Epoch 43/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6736 - acc: 0.5619Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6745 - acc: 0.5585 - val_loss: 0.6734 - val_acc: 0.5504\n",
      "Epoch 44/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6747 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6729 - acc: 0.5643 - val_loss: 0.6635 - val_acc: 0.5776\n",
      "Epoch 45/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6712 - acc: 0.5713Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6709 - acc: 0.5719 - val_loss: 0.6710 - val_acc: 0.5756\n",
      "Epoch 46/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6718 - acc: 0.5621Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5631 - val_loss: 0.6744 - val_acc: 0.5494\n",
      "Epoch 47/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6693 - acc: 0.5674Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6688 - acc: 0.5679 - val_loss: 0.6574 - val_acc: 0.5938\n",
      "Epoch 48/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6720 - acc: 0.5633Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6711 - acc: 0.5645 - val_loss: 0.6634 - val_acc: 0.5817\n",
      "Epoch 49/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6757 - acc: 0.5538Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6757 - acc: 0.5551 - val_loss: 0.6777 - val_acc: 0.5363\n",
      "Epoch 50/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6757 - acc: 0.5517Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6764 - acc: 0.5515 - val_loss: 0.6669 - val_acc: 0.5706\n",
      "Epoch 51/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6693 - acc: 0.5688Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6701 - acc: 0.5685 - val_loss: 0.6731 - val_acc: 0.5524\n",
      "Epoch 52/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6771 - acc: 0.5505Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6764 - acc: 0.5511 - val_loss: 0.6706 - val_acc: 0.5716\n",
      "Epoch 53/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6735 - acc: 0.5616Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6738 - acc: 0.5613 - val_loss: 0.6794 - val_acc: 0.5333\n",
      "Epoch 54/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6741 - acc: 0.5490Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6744 - acc: 0.5483 - val_loss: 0.6779 - val_acc: 0.5554\n",
      "Epoch 55/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6700 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6706 - acc: 0.5665 - val_loss: 0.6819 - val_acc: 0.5383\n",
      "Epoch 56/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6723 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6711 - acc: 0.5645 - val_loss: 0.6703 - val_acc: 0.5635\n",
      "Epoch 57/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6765 - acc: 0.5550Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6763 - acc: 0.5521 - val_loss: 0.6680 - val_acc: 0.5897\n",
      "Epoch 58/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5584Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6725 - acc: 0.5589 - val_loss: 0.6735 - val_acc: 0.5554\n",
      "Epoch 59/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6699 - acc: 0.5698Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6699 - acc: 0.5697 - val_loss: 0.6677 - val_acc: 0.5746\n",
      "Epoch 60/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6710 - acc: 0.5688Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6716 - acc: 0.5659 - val_loss: 0.6695 - val_acc: 0.5776\n",
      "Epoch 61/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6717 - acc: 0.5649Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6716 - acc: 0.5651 - val_loss: 0.6601 - val_acc: 0.5927\n",
      "Epoch 62/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6672 - acc: 0.5750Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6670 - acc: 0.5739 - val_loss: 0.6695 - val_acc: 0.5615\n",
      "Epoch 63/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6738 - acc: 0.5551Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6746 - acc: 0.5533 - val_loss: 0.6698 - val_acc: 0.5726\n",
      "Epoch 64/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6769 - acc: 0.5536Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6773 - acc: 0.5529 - val_loss: 0.6613 - val_acc: 0.5665\n",
      "Epoch 65/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6709 - acc: 0.5638Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6724 - acc: 0.5601 - val_loss: 0.6700 - val_acc: 0.5675\n",
      "Epoch 66/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6699 - acc: 0.5735Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6697 - acc: 0.5711 - val_loss: 0.6676 - val_acc: 0.5746\n",
      "Epoch 67/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6699 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6689 - acc: 0.5677 - val_loss: 0.6663 - val_acc: 0.5827\n",
      "Epoch 68/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6735 - acc: 0.5595Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5593 - val_loss: 0.6593 - val_acc: 0.5817\n",
      "Epoch 69/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6707 - acc: 0.5617 - val_loss: 0.6681 - val_acc: 0.5706\n",
      "Epoch 70/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6743 - acc: 0.5562Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6748 - acc: 0.5555 - val_loss: 0.6621 - val_acc: 0.5736\n",
      "Epoch 71/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6685 - acc: 0.5722Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6696 - acc: 0.5687 - val_loss: 0.6678 - val_acc: 0.5585\n",
      "Epoch 72/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6695 - acc: 0.5694Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6691 - acc: 0.5673 - val_loss: 0.6552 - val_acc: 0.5847\n",
      "Epoch 73/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6684 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6694 - acc: 0.5645 - val_loss: 0.6743 - val_acc: 0.5585\n",
      "Epoch 74/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6695 - acc: 0.5634Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6693 - acc: 0.5649 - val_loss: 0.6692 - val_acc: 0.5665\n",
      "Epoch 75/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6653 - acc: 0.5718Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6667 - acc: 0.5695 - val_loss: 0.6691 - val_acc: 0.5615\n",
      "Epoch 76/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5547Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5555 - val_loss: 0.6657 - val_acc: 0.5917\n",
      "Epoch 77/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6722 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6711 - acc: 0.5619 - val_loss: 0.6734 - val_acc: 0.5595\n",
      "Epoch 78/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6728 - acc: 0.5627Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6715 - acc: 0.5665 - val_loss: 0.6693 - val_acc: 0.5605\n",
      "Epoch 79/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6747 - acc: 0.5554Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6751 - acc: 0.5547 - val_loss: 0.6723 - val_acc: 0.5484\n",
      "Epoch 80/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6717 - acc: 0.5567Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6727 - acc: 0.5543 - val_loss: 0.6734 - val_acc: 0.5675\n",
      "Epoch 81/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6686 - acc: 0.5642Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6677 - acc: 0.5671 - val_loss: 0.6676 - val_acc: 0.5847\n",
      "Epoch 82/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6664 - acc: 0.5795Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6655 - acc: 0.5793 - val_loss: 0.6731 - val_acc: 0.5827\n",
      "Epoch 83/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6698 - acc: 0.5698Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6709 - acc: 0.5691 - val_loss: 0.6602 - val_acc: 0.5857\n",
      "Epoch 84/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6670 - acc: 0.5608Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6670 - acc: 0.5617 - val_loss: 0.6705 - val_acc: 0.5696\n",
      "Epoch 85/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6725 - acc: 0.5578Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6725 - acc: 0.5585 - val_loss: 0.6731 - val_acc: 0.5615\n",
      "Epoch 86/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6679 - acc: 0.5659Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6683 - acc: 0.5635 - val_loss: 0.6655 - val_acc: 0.5665\n",
      "Epoch 87/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6669 - acc: 0.5748Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6667 - acc: 0.5761 - val_loss: 0.6629 - val_acc: 0.5746\n",
      "Epoch 88/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6704 - acc: 0.5696Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5697 - val_loss: 0.6810 - val_acc: 0.5302\n",
      "Epoch 89/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.5490Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6770 - acc: 0.5483 - val_loss: 0.6725 - val_acc: 0.5544\n",
      "Epoch 90/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6707 - acc: 0.5588Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6705 - acc: 0.5605 - val_loss: 0.6682 - val_acc: 0.5766\n",
      "Epoch 91/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6714 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6716 - acc: 0.5591 - val_loss: 0.6636 - val_acc: 0.5716\n",
      "Epoch 92/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6673 - acc: 0.5677Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6676 - acc: 0.5677 - val_loss: 0.6735 - val_acc: 0.5696\n",
      "Epoch 93/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6704 - acc: 0.5661Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5665 - val_loss: 0.6684 - val_acc: 0.5685\n",
      "Epoch 94/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6705 - acc: 0.5595Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6717 - acc: 0.5565 - val_loss: 0.6787 - val_acc: 0.5413\n",
      "Epoch 95/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6733 - acc: 0.5612Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6723 - acc: 0.5635 - val_loss: 0.6747 - val_acc: 0.5363\n",
      "Epoch 96/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6666 - acc: 0.5677Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6662 - acc: 0.5677 - val_loss: 0.6630 - val_acc: 0.5685\n",
      "Epoch 97/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6723 - acc: 0.5595Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6718 - acc: 0.5603 - val_loss: 0.6745 - val_acc: 0.5302\n",
      "Epoch 98/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6707 - acc: 0.5608Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6704 - acc: 0.5615 - val_loss: 0.6731 - val_acc: 0.5716\n",
      "Epoch 99/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6699 - acc: 0.5672Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6695 - acc: 0.5681 - val_loss: 0.6660 - val_acc: 0.5726\n",
      "Epoch 100/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6704 - acc: 0.5595Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6708 - acc: 0.5587 - val_loss: 0.6694 - val_acc: 0.5635\n",
      "Epoch 101/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6689 - acc: 0.5711Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6695 - acc: 0.5707 - val_loss: 0.6712 - val_acc: 0.5565\n",
      "Epoch 102/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6702 - acc: 0.5593Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6708 - acc: 0.5575 - val_loss: 0.6731 - val_acc: 0.5575\n",
      "Epoch 103/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6652 - acc: 0.5780Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6657 - acc: 0.5767 - val_loss: 0.6679 - val_acc: 0.5675\n",
      "Epoch 104/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6705 - acc: 0.5692Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5705 - val_loss: 0.6658 - val_acc: 0.5635\n",
      "Epoch 105/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6662 - acc: 0.5694Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6663 - acc: 0.5695 - val_loss: 0.6639 - val_acc: 0.5675\n",
      "Epoch 106/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6692 - acc: 0.5696Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6709 - acc: 0.5659 - val_loss: 0.6650 - val_acc: 0.5776\n",
      "Epoch 107/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6727 - acc: 0.5593Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6727 - acc: 0.5601 - val_loss: 0.6643 - val_acc: 0.5685\n",
      "Epoch 108/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6713 - acc: 0.5681 - val_loss: 0.6672 - val_acc: 0.5746\n",
      "Epoch 109/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6700 - acc: 0.5627Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6703 - acc: 0.5613 - val_loss: 0.6761 - val_acc: 0.5433\n",
      "Epoch 110/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6688 - acc: 0.5621Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6690 - acc: 0.5603 - val_loss: 0.6721 - val_acc: 0.5534\n",
      "Epoch 111/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6697 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6680 - acc: 0.5701 - val_loss: 0.6604 - val_acc: 0.5837\n",
      "Epoch 112/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6692 - acc: 0.5601Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6698 - acc: 0.5605 - val_loss: 0.6905 - val_acc: 0.5383\n",
      "Epoch 113/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6691 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6687 - acc: 0.5673 - val_loss: 0.6590 - val_acc: 0.5938\n",
      "Epoch 114/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6701 - acc: 0.5653Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6708 - acc: 0.5647 - val_loss: 0.6559 - val_acc: 0.5998\n",
      "Epoch 115/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6666 - acc: 0.5723Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6665 - acc: 0.5725 - val_loss: 0.6620 - val_acc: 0.5806\n",
      "Epoch 116/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5638Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6725 - acc: 0.5655 - val_loss: 0.6714 - val_acc: 0.5716\n",
      "Epoch 117/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6692 - acc: 0.5653Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6678 - acc: 0.5679 - val_loss: 0.6780 - val_acc: 0.5403\n",
      "Epoch 118/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6725 - acc: 0.5556Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5529 - val_loss: 0.6629 - val_acc: 0.5837\n",
      "Epoch 119/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6701 - acc: 0.5582Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6691 - acc: 0.5591 - val_loss: 0.6643 - val_acc: 0.5756\n",
      "Epoch 120/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6704 - acc: 0.5617Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5613 - val_loss: 0.6670 - val_acc: 0.5605\n",
      "Epoch 121/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6720 - acc: 0.5603Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6729 - acc: 0.5603 - val_loss: 0.6707 - val_acc: 0.5474\n",
      "Epoch 122/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6720 - acc: 0.5640Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6707 - acc: 0.5665 - val_loss: 0.6670 - val_acc: 0.5706\n",
      "Epoch 123/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6664 - acc: 0.5671Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6667 - acc: 0.5675 - val_loss: 0.6752 - val_acc: 0.5514\n",
      "Epoch 124/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6673 - acc: 0.5775Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6684 - acc: 0.5737 - val_loss: 0.6735 - val_acc: 0.5474\n",
      "Epoch 125/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6651 - acc: 0.5710Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6654 - acc: 0.5691 - val_loss: 0.6753 - val_acc: 0.5575\n",
      "Epoch 126/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6695 - acc: 0.5685Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6693 - acc: 0.5693 - val_loss: 0.6730 - val_acc: 0.5625\n",
      "Epoch 127/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6717 - acc: 0.5664Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5667 - val_loss: 0.6711 - val_acc: 0.5514\n",
      "Epoch 128/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6610 - acc: 0.5853Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6611 - acc: 0.5853 - val_loss: 0.6767 - val_acc: 0.5554\n",
      "Epoch 129/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6724 - acc: 0.5653Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5617 - val_loss: 0.6709 - val_acc: 0.5565\n",
      "Epoch 130/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6666 - acc: 0.5774Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6660 - acc: 0.5783 - val_loss: 0.6636 - val_acc: 0.5716\n",
      "Epoch 131/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6699 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6694 - acc: 0.5683 - val_loss: 0.6748 - val_acc: 0.5454\n",
      "Epoch 132/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6716 - acc: 0.5629Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6705 - acc: 0.5659 - val_loss: 0.6717 - val_acc: 0.5635\n",
      "Epoch 133/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.5536Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6732 - acc: 0.5513 - val_loss: 0.6632 - val_acc: 0.5756\n",
      "Epoch 134/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6714 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6710 - acc: 0.5607 - val_loss: 0.6641 - val_acc: 0.5877\n",
      "Epoch 135/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6697 - acc: 0.5634Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6697 - acc: 0.5625 - val_loss: 0.6781 - val_acc: 0.5333\n",
      "Epoch 136/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6714 - acc: 0.5619Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6722 - acc: 0.5595 - val_loss: 0.6578 - val_acc: 0.5796\n",
      "Epoch 137/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6685 - acc: 0.5625Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6678 - acc: 0.5641 - val_loss: 0.6605 - val_acc: 0.5948\n",
      "Epoch 138/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6709 - acc: 0.5520Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6707 - acc: 0.5523 - val_loss: 0.6618 - val_acc: 0.5706\n",
      "Epoch 139/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6696 - acc: 0.5684Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5687 - val_loss: 0.6735 - val_acc: 0.5413\n",
      "Epoch 140/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6729 - acc: 0.5524Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6718 - acc: 0.5549 - val_loss: 0.6711 - val_acc: 0.5605\n",
      "Epoch 141/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6689 - acc: 0.5689Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6700 - acc: 0.5659 - val_loss: 0.6705 - val_acc: 0.5534\n",
      "Epoch 142/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6699 - acc: 0.5595Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6700 - acc: 0.5607 - val_loss: 0.6724 - val_acc: 0.5605\n",
      "Epoch 143/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6715 - acc: 0.5634Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6715 - acc: 0.5641 - val_loss: 0.6663 - val_acc: 0.5655\n",
      "Epoch 144/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6707 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6718 - acc: 0.5635 - val_loss: 0.6717 - val_acc: 0.5665\n",
      "Epoch 145/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6704 - acc: 0.5640Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6709 - acc: 0.5629 - val_loss: 0.6712 - val_acc: 0.5655\n",
      "Epoch 146/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6703 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6708 - acc: 0.5615 - val_loss: 0.6754 - val_acc: 0.5464\n",
      "Epoch 147/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6720 - acc: 0.5638Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6719 - acc: 0.5631 - val_loss: 0.6753 - val_acc: 0.5504\n",
      "Epoch 148/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6687 - acc: 0.5679Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6690 - acc: 0.5663 - val_loss: 0.6649 - val_acc: 0.5696\n",
      "Epoch 149/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6686 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6680 - acc: 0.5687 - val_loss: 0.6691 - val_acc: 0.5746\n",
      "Epoch 150/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6757 - acc: 0.5471Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6756 - acc: 0.5489 - val_loss: 0.6687 - val_acc: 0.5565\n",
      "Epoch 151/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6696 - acc: 0.5640Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5631 - val_loss: 0.6704 - val_acc: 0.5625\n",
      "Epoch 152/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6736 - acc: 0.5562Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6744 - acc: 0.5539 - val_loss: 0.6812 - val_acc: 0.5343\n",
      "Epoch 153/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6679 - acc: 0.5709Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6681 - acc: 0.5695 - val_loss: 0.6791 - val_acc: 0.5423\n",
      "Epoch 154/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6617 - acc: 0.5815Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6631 - acc: 0.5781 - val_loss: 0.6699 - val_acc: 0.5605\n",
      "Epoch 155/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6626 - acc: 0.5797Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6632 - acc: 0.5799 - val_loss: 0.6775 - val_acc: 0.5544\n",
      "Epoch 156/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6668 - acc: 0.5644Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6670 - acc: 0.5631 - val_loss: 0.6707 - val_acc: 0.5403\n",
      "Epoch 157/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6712 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6714 - acc: 0.5641 - val_loss: 0.6750 - val_acc: 0.5544\n",
      "Epoch 158/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5651Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6707 - acc: 0.5663 - val_loss: 0.6723 - val_acc: 0.5504\n",
      "Epoch 159/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6675 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6685 - acc: 0.5655 - val_loss: 0.6683 - val_acc: 0.5696\n",
      "Epoch 160/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6658 - acc: 0.5734Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6673 - acc: 0.5725 - val_loss: 0.6731 - val_acc: 0.5675\n",
      "Epoch 161/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6715 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6718 - acc: 0.5593 - val_loss: 0.6653 - val_acc: 0.5716\n",
      "Epoch 162/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6696 - acc: 0.5631Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6685 - acc: 0.5651 - val_loss: 0.6631 - val_acc: 0.5766\n",
      "Epoch 163/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6669 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6666 - acc: 0.5655 - val_loss: 0.6581 - val_acc: 0.5948\n",
      "Epoch 164/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6712 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5617 - val_loss: 0.6727 - val_acc: 0.5635\n",
      "Epoch 165/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6672 - acc: 0.5800Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6679 - acc: 0.5781 - val_loss: 0.6708 - val_acc: 0.5423\n",
      "Epoch 166/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6680 - acc: 0.5774Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6690 - acc: 0.5745 - val_loss: 0.6656 - val_acc: 0.5635\n",
      "Epoch 167/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6639 - acc: 0.5825Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6644 - acc: 0.5787 - val_loss: 0.6713 - val_acc: 0.5595\n",
      "Epoch 168/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6680 - acc: 0.5631Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6677 - acc: 0.5641 - val_loss: 0.6642 - val_acc: 0.5776\n",
      "Epoch 169/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6706 - acc: 0.5649Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6701 - acc: 0.5657 - val_loss: 0.6657 - val_acc: 0.5716\n",
      "Epoch 170/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6742 - acc: 0.5532Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5555 - val_loss: 0.6650 - val_acc: 0.5786\n",
      "Epoch 171/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6664 - acc: 0.5731Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6661 - acc: 0.5725 - val_loss: 0.6737 - val_acc: 0.5393\n",
      "Epoch 172/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6707 - acc: 0.5682Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5729 - val_loss: 0.6668 - val_acc: 0.5685\n",
      "Epoch 173/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6700 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6689 - acc: 0.5617 - val_loss: 0.6716 - val_acc: 0.5655\n",
      "Epoch 174/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6634 - acc: 0.5795Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6620 - acc: 0.5813 - val_loss: 0.6733 - val_acc: 0.5605\n",
      "Epoch 175/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6686 - acc: 0.5652Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6686 - acc: 0.5659 - val_loss: 0.6772 - val_acc: 0.5585\n",
      "Epoch 176/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6729 - acc: 0.5602Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6732 - acc: 0.5589 - val_loss: 0.6756 - val_acc: 0.5534\n",
      "Epoch 177/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6686 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6687 - acc: 0.5671 - val_loss: 0.6677 - val_acc: 0.5796\n",
      "Epoch 178/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6706 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6703 - acc: 0.5631 - val_loss: 0.6689 - val_acc: 0.5595\n",
      "Epoch 179/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6691 - acc: 0.5670Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6685 - acc: 0.5691 - val_loss: 0.6695 - val_acc: 0.5696\n",
      "Epoch 180/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6641 - acc: 0.5747Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6644 - acc: 0.5727 - val_loss: 0.6684 - val_acc: 0.5544\n",
      "Epoch 181/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6735 - acc: 0.5540Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6730 - acc: 0.5537 - val_loss: 0.6679 - val_acc: 0.5665\n",
      "Epoch 182/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6652 - acc: 0.5752Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6653 - acc: 0.5747 - val_loss: 0.6805 - val_acc: 0.5383\n",
      "Epoch 183/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6674 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6677 - acc: 0.5671 - val_loss: 0.6726 - val_acc: 0.5575\n",
      "Epoch 184/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6706 - acc: 0.5612Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6714 - acc: 0.5607 - val_loss: 0.6722 - val_acc: 0.5766\n",
      "Epoch 185/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6656 - acc: 0.5778Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6662 - acc: 0.5753 - val_loss: 0.6644 - val_acc: 0.5675\n",
      "Epoch 186/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6725 - acc: 0.5517Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6725 - acc: 0.5543 - val_loss: 0.6722 - val_acc: 0.5504\n",
      "Epoch 187/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6715 - acc: 0.5552Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6717 - acc: 0.5565 - val_loss: 0.6583 - val_acc: 0.6079\n",
      "Epoch 188/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6659 - acc: 0.5644Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6675 - acc: 0.5605 - val_loss: 0.6643 - val_acc: 0.5756\n",
      "Epoch 189/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6683 - acc: 0.5659Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6686 - acc: 0.5639 - val_loss: 0.6753 - val_acc: 0.5615\n",
      "Epoch 190/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6730 - acc: 0.5631Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6723 - acc: 0.5623 - val_loss: 0.6691 - val_acc: 0.5595\n",
      "Epoch 191/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6637 - acc: 0.5747Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6636 - acc: 0.5751 - val_loss: 0.6696 - val_acc: 0.5655\n",
      "Epoch 192/200\n",
      "149/156 [===========================>..] - ETA: 0s - loss: 0.6715 - acc: 0.5604Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6724 - acc: 0.5589 - val_loss: 0.6677 - val_acc: 0.5917\n",
      "Epoch 193/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6680 - acc: 0.5649Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6689 - acc: 0.5639 - val_loss: 0.6642 - val_acc: 0.5585\n",
      "Epoch 194/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6671 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6676 - acc: 0.5659 - val_loss: 0.6631 - val_acc: 0.5766\n",
      "Epoch 195/200\n",
      "150/156 [===========================>..] - ETA: 0s - loss: 0.6715 - acc: 0.5594Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6719 - acc: 0.5581 - val_loss: 0.6836 - val_acc: 0.5413\n",
      "Epoch 196/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6703 - acc: 0.5556Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5603 - val_loss: 0.6714 - val_acc: 0.5585\n",
      "Epoch 197/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6711 - acc: 0.5572Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6718 - acc: 0.5575 - val_loss: 0.6653 - val_acc: 0.5655\n",
      "Epoch 198/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6732 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6731 - acc: 0.5605 - val_loss: 0.6655 - val_acc: 0.5635\n",
      "Epoch 199/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6670 - acc: 0.5718Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6659 - acc: 0.5743 - val_loss: 0.6668 - val_acc: 0.5756\n",
      "Epoch 200/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6705 - acc: 0.5640Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6698 - acc: 0.5653 - val_loss: 0.6827 - val_acc: 0.5494\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(trainGen, \n",
    "                              steps_per_epoch = trainGen.__len__(), \n",
    "                              verbose=1, \n",
    "                              epochs=epochs,\n",
    "                              workers=4,\n",
    "                              callbacks = [schedule, swa], validation_data = testGen,\n",
    "                              validation_steps = testGen.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6ece8a-491a-4630-adfc-eddd87f568a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:56:55.276602Z",
     "iopub.status.busy": "2024-10-03T11:56:55.276427Z",
     "iopub.status.idle": "2024-10-03T11:56:55.299089Z",
     "shell.execute_reply": "2024-10-03T11:56:55.298525Z",
     "shell.execute_reply.started": "2024-10-03T11:56:55.276577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0169ac83-8f56-406c-8b3a-6236d03b729e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:56:55.300550Z",
     "iopub.status.busy": "2024-10-03T11:56:55.300387Z",
     "iopub.status.idle": "2024-10-03T11:56:55.303960Z",
     "shell.execute_reply": "2024-10-03T11:56:55.303437Z",
     "shell.execute_reply.started": "2024-10-03T11:56:55.300527Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_bias:\n",
    "    conv_weights, conv_bias = model.get_layer(\"shared_conv\").get_weights()\n",
    "else:\n",
    "    conv_weights = model.get_layer(\"shared_conv\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbb2eb1b-a444-4724-aecd-bb96639605ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:56:55.618477Z",
     "iopub.status.busy": "2024-10-03T11:56:55.618314Z",
     "iopub.status.idle": "2024-10-03T11:56:59.009664Z",
     "shell.execute_reply": "2024-10-03T11:56:59.009035Z",
     "shell.execute_reply.started": "2024-10-03T11:56:55.618453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6003653203124999}\n"
     ]
    }
   ],
   "source": [
    "AUCs = {}\n",
    "nSteps = 500\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for i in range(n_motifs):\n",
    "    tmp_conv_weights = np.zeros(conv_weights.shape)\n",
    "    if use_bias:\n",
    "        tmp_bias = np.zeros(n_motifs)\n",
    "    tmp_conv_weights[:,:,i] = conv_weights[:,:,i]\n",
    "    \n",
    "    if use_bias:\n",
    "        tmp_bias[i] = conv_bias[i]\n",
    "    \n",
    "    if use_bias:\n",
    "        model.get_layer(\"shared_conv\").set_weights([tmp_conv_weights, tmp_bias])\n",
    "    else:\n",
    "        model.get_layer(\"shared_conv\").set_weights([tmp_conv_weights])\n",
    "    yPred = model.predict(testGen, steps=nSteps)\n",
    "    yTest = np.array(nSteps*([1 for i in range(16)] + [0 for i in range(16)]))\n",
    "    AUCs[i] = roc_auc_score(yTest, yPred)\n",
    "print(AUCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6179085b-d928-4ea8-9e08-fb50b6f3892f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:56:59.010763Z",
     "iopub.status.busy": "2024-10-03T11:56:59.010597Z",
     "iopub.status.idle": "2024-10-03T11:56:59.096928Z",
     "shell.execute_reply": "2024-10-03T11:56:59.096323Z",
     "shell.execute_reply.started": "2024-10-03T11:56:59.010740Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_model = construct_scan_model(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bf09d91-d1f4-4c73-9ff1-a03a9df0e766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T11:56:59.098282Z",
     "iopub.status.busy": "2024-10-03T11:56:59.098075Z",
     "iopub.status.idle": "2024-10-03T12:07:18.987268Z",
     "shell.execute_reply": "2024-10-03T12:07:18.986672Z",
     "shell.execute_reply.started": "2024-10-03T11:56:59.098253Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199083/199083 [10:19<00:00, 321.25it/s]\n"
     ]
    }
   ],
   "source": [
    "anr = False\n",
    "thresh = 0\n",
    "random.shuffle(seqs)\n",
    "with open(motifs_bed, \"w\") as f:\n",
    "    for i in tqdm.trange(len(seqs)):\n",
    "        seq = seqs[i]\n",
    "        chrom = \"seq_\" + str(i)\n",
    "        start = 1\n",
    "        stop = len(seq)\n",
    "        encoded_seq = np.vstack((0.25*np.ones((w,4)), encode_sequence(seq), 0.25*np.ones((w,4))))\n",
    "        encoded_seq_rc = encoded_seq[::-1,::-1]\n",
    "\n",
    "        conv_for = scan_model.predict(np.expand_dims(encoded_seq, axis = 0), verbose=0)[0]\n",
    "        conv_rc = scan_model.predict(np.expand_dims(encoded_seq_rc, axis = 0), verbose=0)[0]\n",
    "\n",
    "        for k in range(n_motifs):\n",
    "            if anr:\n",
    "                matches_for = np.argwhere(conv_for[:,k] > thresh)[:,0].tolist()\n",
    "                matches_rc = np.argwhere(conv_rc[:,k] > thresh)[:,0].tolist()\n",
    "                for x in matches_for:\n",
    "                    motif_start = x - w \n",
    "                    motif_end = motif_start + w\n",
    "                    score = conv_for[x,k]\n",
    "                    pfms[k] += encoded_seq[x:x+w,:]\n",
    "\n",
    "                for x in matches_rc:\n",
    "                    motif_end = x + w\n",
    "                    motif_start = motif_end - w \n",
    "                    score = conv_rc[x,k] \n",
    "                    pfms[k] += encoded_seq_rc[x:x+w,:]\n",
    "                    n_instances[k] += 1\n",
    "                \n",
    "            else:\n",
    "                maxFor = np.max(conv_for[:,k])\n",
    "                maxRC = np.max(conv_rc[:,k])\n",
    "\n",
    "                if maxFor > thresh or maxRC > thresh:\n",
    "                    if maxFor > maxRC:\n",
    "                        x = np.argmax(conv_for[:,k])\n",
    "                        motif_start = x - w \n",
    "                        motif_end = motif_start + w\n",
    "                        score = conv_for[x,k]\n",
    "                        motifSeq = decode_sequence(encoded_seq[x:x+w,:])\n",
    "                        print(chrom, start+motif_start, start+motif_end, k, score, \"+\", motifSeq, seq, file=f, sep=\"\\t\")\n",
    "                    else:\n",
    "                        x = np.argmax(conv_rc[:,k])\n",
    "                        motif_end = x + w\n",
    "                        motif_start = motif_end - w \n",
    "                        score = conv_rc[x,k] \n",
    "                        motifSeq = decode_sequence(encoded_seq_rc[x:x+w,:])\n",
    "                        print(chrom, stop-motif_start, stop-motif_start+w, k, score, \"-\", motifSeq, seq, file=f, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40919af0-b641-499b-8a5b-08819eb5b3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:16:50.977597Z",
     "iopub.status.busy": "2024-10-03T12:16:50.977373Z",
     "iopub.status.idle": "2024-10-03T12:16:56.517223Z",
     "shell.execute_reply": "2024-10-03T12:16:56.516280Z",
     "shell.execute_reply.started": "2024-10-03T12:16:50.977562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(motifs_bed, sep=\"\\t\", names=[\"chrom\", \"start\", \"stop\", \"kernel\", \"score\", \"strand\", \"seq\", \"og_seq\"])\n",
    "df[\"auc\"] = df.kernel.map(AUCs)\n",
    "df.to_csv(motifs_file, sep=\"\\t\", header=True, index=False)\n",
    "os.remove(motifs_bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c73e4b-77e8-4845-8532-6fca12272889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:31:54.947632Z",
     "iopub.status.busy": "2024-10-03T12:31:54.947466Z",
     "iopub.status.idle": "2024-10-03T12:31:57.348448Z",
     "shell.execute_reply": "2024-10-03T12:31:57.347876Z",
     "shell.execute_reply.started": "2024-10-03T12:31:54.947608Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAChCAYAAADJLnTIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAutElEQVR4nO2dd5hU1fnHP+82lt6bdAFBMRZAUcRQFINGRdDYC0kMRmN6LNHERKPR5GdJggU70ShqrIioYAUMSBPRFaQoZWFpuwvLsrvA7p7fH+8d5s7szOzs7mwb3s/zzDP33nPmzLnte9/znvecK845DMMwjOQlpb4rYBiGYdQuJvSGYRhJjgm9YRhGkmNCbxiGkeSY0BuGYSQ5JvSGYRhJjgn9IYaITBORO+u7HoZh1B0m9HWEiKwXkdN96xeLSL6IjKzPeoUjIpeKyAYR2Ssir4tIuyr81onIFyKS4tt2p4hMS3Adu4rIDBHZ4v1n77D0biLyhojkiUi2iPw0LH2MiCwTkQIR+UZEJoel/1xEvvXSl4jIiAh1yBCRlSKSHaWOV3p1uzoBu1wp3gN8v4gUevs9R0QGhuXpLiLPiUiud34XicjZYXlERH4hIl96ebJF5L8i8p0E17ediLzm/ccGEbm0kvyDRWSut3/bROSXvrTeIvKhiBSJyCr/fWYoJvT1gIhcBTwEfN8593EVf5tWO7UCERkEPApcAXQGioCHq1jMYcDFCa5aOOXAO8D5UdL/A3yL7sP3gb+KyGgAEUkHXkP3szVwEXC/iBzrpQ8D7gEu8NKfBF4TkdSw/7gB2BHpz0WkLXALkFXN/asuf3fOtQC6AZvRugfq1A6YD+wHBgEdgAeA50XkAl8Z/wR+CfwCaAccAbyOHsdE8pBXl87AZcAj3vVXARHpgJ7vR4H2QD9gti/LdOAzL+1W4GUR6Zjg+jZunHP2qYMPsB44HbgG2AkM9aUFBCUHvUHvBFK9tEnAJ+hNmeulTUNvlLeAPcCnQF9feQOBOUAe8DVwoS9tGnBnlDr+FXjet94XvRlbeusPAw/H2EcH3ASsAdK8bXcC02rpmKZ5/9nbt62Ft62jb9tjwLPecmcvvZkvfTFwibd8EbDIl9bcy9/Vt60PsBI4E8iOUK+pwHXAR8DVvu2XAiti7M8PvXL3AN8A1/jSJgHzIxzvfpHOK3AWsNe3/hfgSyAlrIybgA2AAP2BMuDEWr4XmnvX1RG+bc8C98S4Lp+NknYEsC9wjXrb5gE/rc19aGwfs+jrlmuBO4DTnHNLfNunAaWopXI8cAbgb/IPQ2/8zsBd3raLgduBtsDawHYRaY6K/PNAJy/fwyJyVBz1GwR8Hlhxzq3DuyG99eucc9dVUsarQAEqTDERkZ4isivGJ2ZzPlqxYd+B5aO9fdiGWoA/FJFUETkZ6IVauwBvA6kiMsyz4n8ELAe2+sqbglrsxRH26URgKCr2ITjnnnfOHROj7tuBs4FWqOg/ICKDY+5tBLxr4BL0uggwFnjFOVcelv0loCd6jk9DH1yLqvBfD8c4fyui/OwIoNQ5t9q37XP0+ovESUCeiPxPRLaLyJsi0tNLGwR845zbE2dZhyQm9HXLWGAh8EVgg4h0Rq2vXznn9jrntqPWu9/9scU5N8U5V+qcC4jLa865Rc65UuA54Dhv+9nAeufc017+z4BXgB/EUb8WwO6wbbuBllXYRwf8EfijiGTEzOjcRudcmxif56vwv4Ey96AtoD+KSKYnlOcDzXzZpgO3oZbgPOBW59wmL20Perzme+l/AiY7z1QUkQloa+u18P/2HgwPA9dHENR46v6Wc26dUz5G3ROnVqGI34nILm8fRqAuuAAd0BZjODm+9PZR8sSq83Uxzl+0h1oL1BjwE+s66w5chbqUeqJuuem+smp6zSY9JvR1y7WoNfOEiAQszl5AOpATsIRQX2Qn3+82URG/hVmEXvCB8ob5LSvUB9oljvoVotakn1aocMSNc24WkI26qeqDy1D3yibgEdRnnw3gdVC+AFwJZKCW340iEvBB/xi1pgd56ZcDM0XkMM9S/jvqv47EdahrZmF1Ki0iZ4rIQq8zdRdqAHSoQhH3OufaAL3R1sYAX9pOoGuE33T1pedGyZNoqnqdFaOGzWLnXAnakh0uIq2rUdYhiQl93bINbR6fSrCTcxNqOXbwWUKtnHP+pmdVphjdBHwcZlm1cM5dG8dvs4BjAysicjjQBFgd9RfRuRV1bzSLlsFz3RTG+FxWjf/FObfBOXe2c66jc24YKpYBd8TRwGrn3LvOuXLn3NdoX8eZXvpxwEzn3Gov/R3Uyh2O+rB7A/NEZCvqpuoqIltFI39OAyZ461u939wnIg9WVmcRaYK2JO4FOnuCPYugC2ovvmMpIlEf3M65jaj1+08Raeptfg+YKL6IKI8L0WtmNfA+0F1EhlZWX189psY4f9E6o1cDaSLS37ftWKJ3Xq8g9B7wL2cBh4uI34KPVdahSX13EhwqH7zOWG850Px8wFt/A412aIU+fPsCI720SVTshJtGaMfbKLxOQbTJugFttqd7nxOAIyP9NqzcQWiT+lS0w+w/wAth/zstxj4e7Bz01uegVmLU39TgeGYS7CgdAGT60o70jkPAIt+J1znrHdtCYAwqon1RX/ZkL/0qVIgO99LHoi2mgWjnbxffZyKwxVtOBdqEpf8P+A3Q2ncu10fZn5ZoR+hI73/P9P73Ti890Ol4nLfvU4nRGettWwL80ltuD2wEnvbqlon68QuAi3y/mYJ2po/yjl8m6ka8OcHn7wXU/dIcOAV1twyKkncMkO/tezrq2pznS1+IPiAzgQnALnyd8faxzth6wanFNQa4QETuJuhG+Aq9oF+mmk1opz7qM9Cbcwvq4vkbaplX9tss4Keoz387Kj7+ztceqP87Xv6AhujVBsWoYAOsIrRj9Hto53U+uj/jnHM74GAH84+Af6Ei9zFqST/h/fYZVIQ+8tL/hUa/rHLa57E18EGjmsq99TLn3K6w9P1AgXMu4EOOevy88/YLtHM0H43QmeFLX4125L+HCvH8CMWE83+oW6qJcy4X9dtnotdZLvoQusI596LvN78AHkSjunYB61DxfDOO/6sK1wFN0etsOnCtd/0hIqeKSODc4pz7AG0dvuXl74cenwAXox3g+XihsYHzbSjiPRENIyZex+rnwDHOuQP1XZ/GiIjMRi3slfVdF+PQwoTeMAwjyanUdSMiPbzhxV+JSJZ/6LEvj4jIv0RkrYisqE7sr2EYhlE7xDOcvhT4rXNumdezvVRE5jjnvvLlORONSOiPDu55xPs2DMMw6plKLXrnXI5zbpm3vAcdot0tLNt44BmnLATaiEhdxOMahmEYlVClqBsvVvh4dG4VP90IHdSTTcWHgWEYhlEPxD0Tooi0QMPQfuWcCx++HG8Zk4HJAM2bNx8ycODASn5hGIZhxMPSpUt3OuciztoZl9B7U7u+AjznnHs1QpbNaIxwgO7ethCcc4+hMwkydOhQt2TJkvAshmEYRjUQkQ3R0uKJuhF0Ct2Vzrn7o2SbAVzpRd+cBOx2zlVpciTDMAyjdojHoj8FHU7/hYgs97bdgg7jxzk3FZ2T4yx0KHkROimUYRiG0QCoVOidc/MJnds7Uh4H/CxRlTIMwzASh811YxiGkeSY0BuGYSQ5JvSGYRhJjgm9YRhGkmNCbxiGkeSY0BuGYSQ5JvSGYRhJjgm9YRhGkmNCbxiGkeSY0BuGYSQ5JvSGYRhJjgm9YRhGkmNCbxiGkeSY0BuGYSQ5JvSGYRhJjgm9YRhGkmNCbxiGkeSY0BuGYSQ5JvSGYRhJjgm9YRhGklOp0IvIUyKyXUS+jJI+SkR2i8hy73Nb4qtpGIZhVJe0OPJMAx4EnomRZ55z7uyE1MgwDMNIKJVa9M65uUBeHdTFMAzDqAUS5aM/WUQ+F5G3RWRQtEwiMllElojIkh07diTorw3DMIxYJELolwG9nHPHAlOA16NldM495pwb6pwb2rFjxwT8tWEYhlEZNRZ651yBc67QW54FpItIhxrXzDAMw0gINRZ6EekiIuItn+iVmVvTcg3DMIzEUGnUjYhMB0YBHUQkG/gTkA7gnJsKXABcKyKlQDFwsXPO1VqNDcMwjCpRqdA75y6pJP1BNPzSMAzDaIDYyFjDMIwkx4TeMAwjyTGhNwzDSHJM6A3DMJIcE3rDMIwkx4TeMAwjyTGhNwzDSHJM6A3DMJIcE3rDMIwkx4TeMAwjyTGhNwzDSHJM6A3DMJIcE3rDMIwkx4TeMAwjyTGhNwzDSHJM6A3DMJIcE3rDMIwkx4TeMAwjyTGhNwzDSHIqFXoReUpEtovIl1HSRUT+JSJrRWSFiAxOfDUNQ3n3Xbj6anjqKSgtre/aGEbjIB6LfhowLkb6mUB/7zMZeKTm1TKMitx5J4wbB08+CT/+MVxxBThX37UyjIZPpULvnJsL5MXIMh54xikLgTYi0jVRFTQMgLVr4fbbQ7e98AI88UT91McwGhOJ8NF3Azb51rO9bYaRMG67LbKr5tZbYe/euq+PYTQm6rQzVkQmi8gSEVmyY8eOuvxroxGzfDlMnx45bccOePrpOq2OYTQ6EiH0m4EevvXu3rYKOOcec84Ndc4N7dixYwL+2jgUeOqp2OmbNsVON4xDnUQI/QzgSi/65iRgt3MuJwHlGgYAb79d3zUwjMZNWmUZRGQ6MAroICLZwJ+AdADn3FRgFnAWsBYoAn5YW5U1Dj3WrtXPocD8jfPJ2p4FwIAOAxjVe1T9VshIGioVeufcJZWkO+BnCauRYfiYM6e+a1B3PLb0MZ5d8SwAFxx1gQm9kTAqFXrDqE+WLauFQkXiz1uHgfq5xbnB5aLcGDkNo2rYFAhGg+bzz+u7BnVHXnFexGXDqCkm9EaDpawMvgybeKNlS43C+dvfoEmTOqqISPyfGmBCb9QW5roxGixr1kBxcei2u++GH3rd/b16wcUX1329agu/u8bvxjGMmmIWvdFgWb8+dL19+6DIA1x4IZx0Up1WqdYod+Xkl+QfXC86UERJaUk91shIJkzojQbL9u2h6xddBM2aBddFdAqEBk/hN7B+Ouz6ImqW3SW7KXflIdvyi/Oj5DaMqmFCbzRYwmfJGDmyYp5x46Bz57qpT7VY/SC8dSQsuBTePgYWXAll+ytki+STN/eNkSjMR280WMKFfvjwinnS0hqwn37XF7Ds1+B8s7GtfxbK98Pw6SGdt5GEPqEdss5B7kLIWwJNOkLX70FG28SVbzRoTOiNBovfddOyJXSLMifqYYfVTX2qRHkZLLo6VOQDbHwRup0LvS89uCmS9Z6wWPrSYpg3Aba+G9yWmgnfuQMG/q7G0UJGw8dcN0aDxW/RDxzYyPRoy0zIXRQ9PefdkNVas+jL9sPcc0NFHqCsBJbfCJ//XtfrKITUqB/MojcaLH6Lvk+fBBYcPtrVL16JGgm74YUqZa81oV/3GGx7L3r6zk9q/h9Gg8cseqPB4rfo27evJHNDskhLi2HLm1X6ScBNIwjN0jW0qMadsWX74cu/1KwMIykwi95osPgt+nbt6q8eVW4B5C2G0qq99ipgvbdt2pZm6c0oOlBU0aKv6hw9W2fDvrAY1dSm+l1WXPE3RtJiFr3RICkqCn1FYL0KfVXZubDiNol9qwWs97aZbWmbqdEwNXbdbHo5dL3nxXB+PlywB05+DtJb16x8o9FgQm80SMJDKxuV0OeGCX3b42FiLpy7HjpFGAxAqEXftqkKfY1dN3m+qT87jYbhz0FqE0hJ1Yif0+ZCequa/YfRKDChNxokhYWh641K6P0jYFMz4eT/QEYbaN4LRr4FLQdU+MlBoU+URV9eBoVrgutH3lCxVdH2GBgypfr/YTQaTOiNBsmBA6HrjUbonYPiLcH1PpOg9VHB9bTmcNK0Cj876LrxWfQ1EvqijRpCGfjPzmMi52txePX/w2g0WGes0SApDRtn1KqxeBhK90BZUXC92/iKeTqcBMWbQzb5LfqDUTc1GTBVsCq43HKAumxiURchp0a9YRa90SAJt+jT0+unHlXGb80DdIwwbwNA9wkHF8vKyw5OYNauabuDrpvi0mKKD1QzOqbg6+CyWe2HPCb0RoMk3KJPq6zt6VzoJ960RFOcE1xu0S96Z6fPX757324cWq+2mUHXDdTAfeNvMZjQH/LEJfQiMk5EvhaRtSJyc4T0SSKyQ0SWe5+rE19V41Ai3KKvVOgbCiFCH99wXr+Yt20a7IwNT6vSw6zMN5d9Zldf/bbC1g9CPwfCer6NpKPS20dEUoGHgLFANrBYRGY4574Ky/qic+76WqijkeQUFEBJCXTsGHQNh1v0qal1X69q4XfdNOkU10/8vvi2mW1pmt704Hq1LXr/gKjUzODy1jmw8MrQvGeugDbfqd7/GI2CeCz6E4G1zrlvnHP7gReACD1MhlE1Vq+GM86Atm11TvmePeHhh6G8vKKwl5dHLqPB4bfoM+MT+lgWfbVj6ct9TaKUxtIcMmqLeIS+G7DJt57tbQvnfBFZISIvi0iPSAWJyGQRWSIiS3aEj4gxDik++wxGjIA5c4Iinp0NP/sZTJoEKWFXZriF32Ap2Rpcro7QJ8pH77fiyw9Ez2ccEiSqM/ZNoLdz7hhgDvDvSJmcc48554Y654Z27NgxQX9tNDby8mDs2IqjXwM8+yzk5IRuazRC73eZxOu68VntMX30VcEv9FWcd8dIPuIR+s2A30Lv7m07iHMu1zm3z1t9AhiSmOoZycjtt0NuJR6J8M7X8M7ZBku57zWBTeIb5eUX83ZN24VY9NWOpU9rHlwu2hhcbt4Tuk+E5omc99lo6MTjvFsM9BeRPqjAXwxc6s8gIl2dcwEb7FxgZUJraSQNu3fDY49Vni9c6MOnRGiwhPjGM4LLax+HdY+H5j3tI0hrdlDMUyWVlhktEZHoM1jGiz+ksvDb4HKnkfpZfB2sfaR6ZRuNjkqF3jlXKiLXA+8CqcBTzrksEbkDWOKcmwH8QkTOBUqBPGBSLdbZaMS8/LJG2PiZNAnGj4cVK+CBB2DXropCn5fA16fWKn6hF98or+ItOn2xH6edE3klunNtMtsgXthR28y2KvQl1dxx/3w6hd9UrwwjaYirO945NwuYFbbtNt/y74HfJ7ZqRjIya1bo+o03wj33aFjleefpi77HjIHMzNB8jUbo/e+IlfhiQv0zVwZo27Qtm/dsrr7rppVP6Pd8Dft36cRqxiGJjYw16pQvvwwu9+wJf/lL6LQqRxwB//0vdArrx2w0Qh8yQ2R8o3ADYu7vhK3xDJZNOkJ6G68aZZD9evXKMZICE3qjzigpgbVrg+uXXQYZGRXznXwyHHtsqFXfeITe566JM6wxkkXfrql25FY7jl4k1KrPugv27axeWUajx4TeSBjOwTvvqOulRw99ofeFF8IHH2j6qlWhA5/GRJk5F1Sn/BG4jUboU3xC7+KLCfW/XSpAQqYq7jQquFy4Ft4bCTmzYfObsO396pdrNDpsyJyREDZsgJ/8RAdA+Vm/Xl0xv/oVDB0amnbEEbHL7NQJNnlD9SoLx2ww+CNtDuypNHtZeRm7SnYB8OH6Dxnx1AgA1uWvA6CktITiA8Uh0yLETc8fwMq/BdcLvoKPvlf1coxGjwm9UWMKC+Hss0P97+EsWADNfaHdTZpA9+6xy/Vb9Bs3Rs/XoPAL/T7fiLC05jqA6kB+iEsnIPIA2/duZ/vesJd5oxZ/9/RKDlYk2g6GdkMgb2nVf2skFea6MWrMPfdUFPnOnaFfv9CpDAoKgst9+lSc5iAcf4fs119Hz9eg8A+S8gv9kb+Didug82kh2ePxwVfbfSMC3/lL7DzpraBJh+qVbzQaTOiNGlFUpLHvAZo0gSlTYPNmWLNG56+52pu0utg3O0CbNpWX7bfod+7UTyTy86tc7drDPyXwvsrnc4pHxGvkpz/sTDj6T5HTMtrCyFnQtGvkdCNpMKE3asSCBSr2Ae65B66/Pjj7ZNeu8PjjcNddoQOlmlTyZjuoGGK5cGHFPM7BSy9Vvd61hl80SxIj9DV6pSCo0B97N2R4rY2UDOh1GYz7HDqeUrOyjUaB+eiNGjF3bnC5VSu45prI+U47DZ55Jrgez7TD4fPezZ2rfQF+5s+HbxrSwM+mhwWXS3Ki5/OIR8RrZNED5U74eNvN/PuN37Dl2x1k7+xMh45pnHIKXHUVDBxYo+KNRoBZ9EaN+Pjj4PKwYdA0RnCIPy18GoRIhFv0L71UcRbLv/+98nLqFL9Fv+vLg9McRKO2XTeffw5HHaWhrP9+NoM587uxclUa8+Zp6+uooxpR/4dRbcyir0NycjTOPCdHXQ7du8Pw4dC/f+T8eXk6wGjvXn05x4ABsYW0Pvjii+DyCSfEzuuv+/aKwSUV6BH2VoMNGzRU85JLdH3uXJg5U8WqweD30ZfugcJ10DLKCSa+ztjqDppatw5GjtSJ5AK0a6cd4du3a+iqc7B/f/QyjOTAhL4OeP99nZp3/vzI76YeOxZmz9blsjJ4+ml44QX46CNdD5CeDqefDg89pDdrAOfUKvvkE50QLCMDevXSh0iHWgyocC40kqZXr9j5/Rb6xo1q1YfPaeNn4EDdZ/8UxTfdpP+zY4fG7Tc4MjsBwsHpD7bOiSn0tWnR//rXQZFPS9NO86uvDh7zrCz4U5R+WiO5aLRC7xzs2aPWbvPm6h+Oxq5dsGiRRmekpEC3bnDMMdCiRc3rsGoVLF+useSZmdC7NwwZAs2aaZ6nn4Yf/zhU4IcM0aiTjRs1MiUrS7fv3QvnnAMffhjMK6LWfGGhWl5vv62/6dNHBfD+++GJJ0KnFgiQkgK33gp33FGz/YxGWVmoK6WyDtajjw4uOwfffgtHHhk9f0aGpq9YEdy2aROc0pD7D1PSVOxLtun6N09D32sgJfIEZ3F1xlbDos/L02slwB13aCe5n0GDdDZRvzHRKCkvhR3zIPsNyF0ABwogtSk07w3th+nAMf+0zY0VV66T0+3bCQd2a6d6k/baioxyfQVoMEJfWqqx2Dt26HK7duqq8IfhlZVp033mTLWSt/re2tahg76a7vbbVcTLyuDJJ1VoFy2q2PmXlgaTJ6t1HODLL7Xcjz6CLVtUWNu00XqMGgUTJ6r45ObqZFyvvKLhg+FkZGiI4XnnwU9/GhT5MWNUlP3W+Jo1MG2aLv/ud0GRT01Va+snP4EuXXR/li/XudzT0mDfPp3a9913g2X166f1TEnRcufPj/wASBRpaaEWtz98MhKDBoWuZ2XFFnrQOW/8Qt8oaNE3KPR5S+DrBzSO3rkK880ERLxd03Z8/tPPQ9JOfvJksguyq2XRv/pq6EN40qToeRvNi9cjsXcjzJ8YeVBY/meQ/Rrs+hyGP1/3dUsUO+br+wxyZun1k9pUJ63DQcl2/T7mrphF1KvQOwdvvAFTp6rbIdLLJY4/HpYs0Rjq8eNDQ+z69dPmfUkJrFwJr7+uURlHHgnnnw9vvhnMO3y4+iubNFFL+sMPgwKyc6feCG+9FcyflqathKwsFf5HH1XxTE/XcjZs0HwtWuh/Dh6sN9bq1eqH37ABnnoq6P9s0UJvvtatQ/evf38NPczPh3/7XsB4551w883B9dRUbQk8+qget8cfD4p8air85z9w0UWhM0Hm5uqxq03atAm+EnDduth5e/fWlk4gHHPmTLjggor5ysv1Qda0KRx3nL5asFHR/kTY+b/g+oo/QMEqnRc+L/SEBES8fdP2dG8VOvq1Q7MO1RZ6f9/JwIEa5pow/BdZZUTyVSaK8jL4+CzY7TWJO42CAb+EdicC5XrMt8wKfetXdSgr0VDZ/flQthdSmugYhKbdIDXCrHzx4BzkLoQd/9NrYt8OnWU0vRU06wFtj4Nel+qcRHPP1d+0GwLffVOvr8Asqa4c9qyF8n3ADVH/rl6F/tprVbgCnHcenHqq+nLz8mDZMpgxQ9Ouvz4o8j17apPT3/nnnOZPSVHRC4h8RoYK+Omnh/63c/DVV/r9/e+r1Q9qbd93H5xxhrqEyso030svqWvm5puDIt+/vz4EDjsstOzSUrX0zz8/uG38+Ioi7+fll4MWcWqqunui4Zy6bAJcfrnO4x5O+/bwvepMbRLvjewcgwcHHziBYxiNlBTtOA08fF58USM/unQJzffAA3DWWfrAPu20iuU0eNqdGLpevg++eTJi1oCIB2ar9BOY5Kw6cfR7fNPstG0bPV+1CBdv//USTdid0wdd7qdqYR/YDaToSOIWfdXF0jq0V72kRAfe7doV7Gto1Updr126gOS8HRT5tsfD6Nmhk8o16w5dTo/9sCnbp5PPpTYNnWK6bD+seVhbBLkLVNxbH60C70q1xVb4LQx9CPpcEfNwVWDvBvjkIj0WoPvf7gQ9Fgf26Atq1j0GHUfAituCvxv8T+hwki5veBF2fhJMS4ntN603oS8oCBX5OXMqijGo+GVlqcsmwJQpFSM8RNTihVDRu+yyyOWKqCvh1VdDBeqtt0LdCamp8J3v6GfjRpg+PZh2220VRR60NdC7d6hrqbIJvPxzuXTvXjGG3M+2baEhcRMnxi4b9FpfuRKWLtV+hYICFYP9+9VybtdOH6DXXQep/hsjXPTDbpqRI4NCv3ix3pCxHmiBFhrojTx+vL6MpH17LfrRR+GGG1ToQd1whx2mrrRGQ4eT4856cC76phXV2D+DpXPu4Nun4sHfZ1XvE8LtXgmLJ6sLAlQsW/TV+X/27YQ9q3UeoPOyKSrS+/vNN/V6Ki/Xa6ZLF21N79qlLcdhw+DFX/puxsN/HBT5A3sqTijXtItazBtfgi1vaYurZAeUFanIl+9Xd0jLI2DM+7DoJ7DeG/gx6I8w6A8VrffyUrX2q4JzsODKoMgPeRD6Xxv2HgO0n0HSoc13YNdy3bZzQXCAW2ZnfYvYV3/Vt5elxe5wrDeh94fXnX56ZDEGFSG/EIsERSAS+/er+yTAyJGx6zFzZnB52LDYPuM1a0J9/SNGxC67ZcvgcmU3W6DzFtSF5Vx0wzo8Bt0/WVgkVqxQt86qVfoQuvZabcX06KG/LSnRh9LXX1e9pf3d7waXi4vh//5P3U5+ysvh+ee15XHhhep2CrBokT6gTz1V6xnujxeBcePUDdZoaNEb2hyrlmsMSstL2b1PTVX/FMUBAtv2le2juLSYZunNKuSJxvHHB5dXr9ZO7PBw1TqhtAg+PlOtWEmBE5+C3peGWt5lJZC3hLIyDUYITGt95pnaOm8X4R3rRUXAp76Rxy18HV+rp8CKW0N/MGE7/O9i2PYBIHDUzXD4j/Ql6SmpOtHc3o2Qv0zTN78R/G3fnwRFfuGPYOf8oG+89dEw9hPYlwubXtVO4YJVcGCXlpmSruU16aCti+7nww5vlGGLftD/uuCNnnW3PowCtBsKQ6aokG97H5bfANs/UhdVk46Q2oR4X25Tb0Lv7yiqbBZDfwiecyqE0eZKSU/Xh0PADeK3qiMRT6szQLilmpOjlns0Ro0KWt5vvKEuiWgTeZ3sMwJzc9VNdXIUw/Cww/QhEmiez58f28Vx440q8qAP1H/9q2KegQO1vlXlhBPUGg88yP76Vz32f/6zPkSWLtXIn4ICFfrRo1VwAtMPg7rCAu6wSFx+eWyhT7hrIhH0/EFsoU/JIH9f0OqM5boBtfybtY5f6M87T/uj9u3T9ccfjxx95Zx2pkd6AUxC+PYZFXmAnhfB4Vfp8r7c0H4MoGDbNj74oPPB9Ysuiizy4BlGmT6fX6Gvg6jVAOg+UV0bgU7xnNmeyAOdR8Oxf9Xl/bu1D8VPkw5qaX86SYX3f5fAoFugzXEwdIq6e2b0gtJC/ez6At4fDftzoVlPGPIv/Y90r1lVXgpF2TpSOs03mKSsWMsXT4a3fwz784LvFu5/PRw2Dsa8B3vWsX/9mxTmrKN8zVI4sIdSl0l5+rmkdOlDi55DgLGRDxZxjowVkXEi8rWIrBWRmyOkNxGRF730T0Wkd2Vl+q3dt9+OPVJy3LjQ/I89FquucIXPZfbEE7GjQc45J7i8aFEw1DESgweHDs6ZMiX2w+Haa4PLGzbojRae3zmN9Bk5MjQq5YYbQv2sAbZs0daQP4Z86tToro38fPjHP4IW3uzZGkv9yivB/V26VN0n999fceRpZWRkwC23hO7P/ffrQzEzE048MXSO+tRUfQhUhdGjo7f4mjcPTprWoOh9OaTGGCRw5I3k7Q9OEhTJoveLf1U7ZFu3Dr2277xTPwFfd1kZzJunYzhqdWRsmW8ipMCrDUHdNQuuhE8u1M7GuefStnwhU6cGXwz/29+qUZKVpYbEnj1qIHz4odcS73VpsLx1T6hfHaDH+XDqK+r3DtBqIKR6D8qClVC0WZdT0tXVJqmw5kH95C+HPpfD2Wtg8D9xmZ0oX3EHZXNGUTZjIKVvD6O0+SDKu18I/a7R/97vWTp9fwzdx6vI58yG2cP0M28CLL5ORb+n51su3gyf/U5DJgFGvwMnhopbQYFG4w06qS9Nj/sVJ/98Cn+YNZ2Hs2by7Lcv8/jyqdzx35u45FdRbhAPcZWYsSKSCqxGHxfZwGLgEufcV7481wHHOOd+KiIXAxOccxfFKvf444e6TZuWHLQEx46Fe+9VX7iICsb69doJesMNain+8Y+aNy1NB85cdpmGPgaibt54Q8X4+OM1ZjsQxTNsmB6skSNVmDZt0ubhV1/BI4+oC+Z/nnHRowfcfbe6Ntq0UVfQihVaj5//XEVxwoTgflx0kYquP+pm5kztUP71rzWaxx9NM2iQRpq0bq1++VmztJ6bN+vvxo8Puoc6dw7uY0EBfPqp+i5nzNAHzuDBwYiXTp20A3f0aD1+a9dqFFL79vDcc3o8N26M7KPPzNR8PXpofUX05t+wAcadGeo/euRhh3Paaho5Eg4/XC3CsWNDp0MIZ9iwYGd6ebm2QD76KHLe1FT1w/oHYC1erA+NcG6/XftKKlCVplpV8lYlf9ZfK7oQAFoPgjMWsSDnc4Y/NRyA+864j9+c/JuQbFOXTOXat9Ra+ODKDxjdZ3SV6pGdrW4xv5s0M1MjcHbuDBoSK1bofVetfawsb3EOvDVQfc5pLeGMBbr/AVbeqy4JgFNfh+7j2bxZ+33mzS1l3TrH7t2wa5cA4nXGChMmpnDN5HJ4Z0jQh93hFBjwC52Hv3wfLPxh0Do+Px/ylsFnv9X8ac2hyxnquklrri6bLV7Y3fH3s6zo1zz4oE4hkZOjUXuDBqkmBMKb8/L0Pr7p+g3wwRjtbG7SQaeG7jxa3SuSCst+Dd8+rWV/bym07Odt+7da9KmZ6sbJaKdumkIvJvqIXzL25n/w3nu6evbZqnHRvAIistQ5NzRiWhxCfzLwZ+fc97z13+v5dHf78rzr5VkgImnAVqCji1H40KFD3bRpS7jiCo0PD9CypYpWfn7w9XFlZXot3X23xq/Hsv6feEIFb+lSnbAploU+YoRaNXl5Ktavvhqa3qyZtgYCe7FmjYZ0vvCCin60aXNBrdy77lIRvOUWtaqjWcvduwddGW++qXXZti162bNnq7CuWqWtl1ghlJdcov7xEGLcmF99pS6cwAPk+ee1RdWmTfBngcFqIsGWVl4e/P73evzDxyykpOi5u/HG4LaiIo1Keued0LzNmumDMVLY5Y9+pOMiApxyij4sAhagc1ru3r3QqXNwH9esdogE3Xrhc+jUmtCX7Yd3jlMLMkCznnD6PGjek5mrZ3LOdDW7nx7/NJOOmxTy8xe/fJGLX1Hr778/+C8XHBV2UOKox9q16vr69NPIVUxP13Per19YQoyy331XDZRvvlH30OjRekzT0/VnInoNpKTAuecCm9+ChVeqWwKBzmN0tHBqU3VX5C/Tgke8ouvb3lfBO+IXwUiXvKXqJikrVv93jwug6/egaAt88oMKbqAQUtJxE3byxapWrFsHJfnZdEldQKc2uTRLy0WAspRW7EvpQlHqEXTufxRnn5t+sL/oqqvUixDLveVK91G47n3cto+RwtVI6S5SygpAUilLaUVZelfKmvalxeDrydvbgXXroGBnHs2KP6V708U0S82FlBTKU1pTktqLwvTjaNH9GObNT+Waa1RHunRRHRkzRscNBQzivLxAB3XNhP4CYJxz7mpv/QpgmHPuel+eL7082d76Oi9PVCkcOnSoW7JkCeXlaq3Nnaux9IEBU+3bB/3G55wTvO5yc7Xp9t57anUGRsb27avCfd55Qf99WZmK4OzZarH7R8YOGaKW+QDf+5PXrAkOmNq8WZ/agTlmRo/WJ2q614dUVKQPidmz9UG1Z4+KSJ8+6lufOLHiO09nzFCrPceb1LBbN7UULrwwNHrnwAG1jl99VcV8717dp2OO0Tp897uhortihZb9ySe6jxkZWo8RI7QeFaZBiHETO6fnYt48bQEUFwcHrqWkBG/i0lI9N8ceG1p0VpaGp65dqwJ8wgnqWok0n8/+/TpY7LXXtFUzZIg+QPv2rZg3ULeHHtKwzOHD1fcfiC555x21djZvVivrrLP0mGZmaj0CsfkHDui5jPd4RKQq+fflwRd/gt1fQIfhcORNkKGdPbPWzOKPH2oz9d6x91aw2BdsWsD1b+tt9odT/8CEIyeEll2Feixfrv0cWVl6PXXposfw0kuj9JHFKHvRIr3uv/1Wj+sJJ+i1HlXoQaNgtszSyJFdK1S0U9LVAm7RT2PDDzsTchfBtg+haKPGk7c4XB8IpXtV5De9CuktoNMYOPK3Xv3KIXcxZL+q32V7NQqlRV895t3OJrewA48+qtdlfr62KPv103s2cF07p/U+/HB9cL39tt5b69bpMeveXe+tgGER0Klzz9U+kPXr9fqaOFHLbtkyeDxKS7WMAQPUWFm9WjXhpJO0Zd6smeZLSQnWI/Dynu3b1QPx2WeqUYWFauwGjnfLlnrP3HdfAxF6EZkMTPZWBwCRvIMdgGR+XX2y7x/YPiYLto+Ni17OuYiB2fFE3WwG/IFZ3b1tkfJke66b1kCFgELn3GNAjK5UEJEl0Z5KyUCy7x/YPiYLto/JQzxRN4uB/iLSR0QygIuBGWF5ZgBe3BQXAB/E8s8bhmEYdUelFr1zrlRErgfeBVKBp5xzWSJyB7DEOTcDeBJ4VkTWAnnow8AwDMNoAMQ1YMo5NwuYFbbtNt9yCfCDBNUppmsnCUj2/QPbx2TB9jFJqLQz1jAMw2jc2DtjDcMwkpwGI/SVTbOQDIjIehH5QkSWi0gtzxRfN4jIUyKy3QuxDWxrJyJzRGSN990QZ6OJmyj7+GcR2eydy+UiEmOqvYaPiPQQkQ9F5CsRyRKRX3rbk+Jcxti/pDqP0WgQrpt4pllIBkRkPTA01kCyxoaIfBcoBJ5xzh3tbfs7kOecu8d7aLd1zt1Un/WsCVH28c9AoXPu3vqsW6IQka5AV+fcMhFpCSwFzgMmkQTnMsb+XUgSncdoNBSL/kRgrXPuG+fcfuAFYHw918mIA+fcXDTSys94IDDDz7/RG6rREmUfkwrnXI5zbpm3vAdYCXQjSc5ljP07JGgoQt8N8E1cSzbJeRIcMFtElnqjhJOVzs45b6IHtgKdY2VuxFwvIis8106jdGlEwpt99njgU5LwXIbtHyTpefTTUIT+UGGEc24wcCbwM88lkNR4A+fq3z+YeB4B+gLHATnAffVamwQhIi2AV4BfOecK/GnJcC4j7F9SnsdwGorQxzPNQqPHObfZ+94OvIa6rJKRbZ5PNOAb3V5J/kaHc26bc67MOVcOPE4SnEsRSUdF8DnnXGAu16Q5l5H2LxnPYyQaitDHM81Co0ZEmnudQIhIc+AM4MvYv2q0+KfEuAp4I0beRklA/Dwm0MjPpegLaZ8EVjrnfK+eT45zGW3/ku08RqNBRN0AeGFN/yA4zcJd9VujxCIih6NWPOiI5OeTYR9FZDowCp0FcBvwJ+B14CWgJ7ABuNA512g7M6Ps4yi0ue+A9cA1Pl92o0NERgDzgC+AwBsFbkH92I3+XMbYv0tIovMYjQYj9IZhGEbt0FBcN4ZhGEYtYUJvGIaR5JjQG4ZhJDkm9IZhGEmOCb1hGEaSY0JvGIaR5JjQG4ZhJDkm9IZhGEnO/wN144u9rF3nNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(4,4, figsize=(20,8), tight_layout=True)\n",
    "for plot_i, i in enumerate(np.argsort(list(AUCs.values()))[::-1]):\n",
    "    n = df[df[\"kernel\"] == i].shape[0]\n",
    "    if n > 0:\n",
    "        auc = AUCs[i]\n",
    "        ax = axes.flatten()[plot_i]\n",
    "        ppm = logomaker.alignment_to_matrix(df[df[\"kernel\"] == i].seq, to_type=\"counts\")\n",
    "        for nuc in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "            if nuc not in ppm.columns:\n",
    "                ppm[nuc] = 0.0\n",
    "        ppm = ppm[[\"A\", \"C\", \"G\", \"T\"]]\n",
    "        ppm = ppm.div(ppm.sum(axis=1), axis=0)\n",
    "        logomaker.Logo(ppm.applymap(get_information_content), ax=ax)\n",
    "        ax.set_ylim([0,2])\n",
    "        ax.set_title(\"Kernel {0}; N = {1}; auROC = {2:.2f}\".format(i, n, auc))\n",
    "    else:\n",
    "        ax = axes.flatten()[plot_i]\n",
    "        fig.delaxes(ax)\n",
    "        \n",
    "for i in range(n_motifs, 16):\n",
    "    fig.delaxes(axes.flatten()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7186db88-2c8f-4830-b2da-f614fd41f1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:17:38.240034Z",
     "iopub.status.busy": "2024-10-03T12:17:38.239859Z",
     "iopub.status.idle": "2024-10-03T12:17:38.843100Z",
     "shell.execute_reply": "2024-10-03T12:17:38.842496Z",
     "shell.execute_reply.started": "2024-10-03T12:17:38.240009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA6nUlEQVR4nO3dd1wVV/7/8de5l46ICthARQUrFhSxC3Zj7DGaaLopm8TNbsomJr/dbMrmm2RLmrpJTHVTVo1JjEYTO3ZRxIYiig1Ro6iAdLjc8/vjXllEElHK3Auf5+PBw3tn5s68GfUzc8+cOaO01gghhKg7TEYHEEIIUbOk8AshRB0jhV8IIeoYKfxCCFHHSOEXQog6xsXoAGX5+/vr4OBgo2MIIYRT2bVr1wWtdUBFlnW4wh8cHExcXJzRMYQQwqkopU5WdFlp6hFCiDpGCr8QQtQxUviFEKKOkcIvhBB1jBR+IYSoY6TwCyFEHSOFXwgh6hgp/EIIUcdUqPArpUYppZKUUslKqVnlzB+klIpXSlmUUpPLzLtXKXXE/nNvVQUXQghxc65b+JVSZmAucAvQCbhTKdWpzGIpwH3A12U+2wj4K9AbiAT+qpRqWPnYQvzP1A+3MfXDbUbHEMJpVOSMPxJI1lof01oXAguA8aUX0Fqf0FrvA6xlPjsSWK21vqS1TgdWA6OqILcQQoibVJHCHwicKvU+1T6tIirzWSGEENXAIS7uKqUeVkrFKaXi0tLSjI4jhBC1WkUK/2mgRan3QfZpFVGhz2qt52mtI7TWEQEBFRpVVAghxE2qSOHfCYQqpVorpdyAO4ClFVz/SmCEUqqh/aLuCPs0IYQQBrlu4ddaW4CZ2Ap2IrBIa31AKfWKUmocgFKql1IqFbgd+FApdcD+2UvAq9gOHjuBV+zThBBCGKRCD2LRWq8AVpSZ9mKp1zuxNeOU99lPgU8rkVEIIUQVcoiLu6J6WK1Wjh07RkpKClpro+MIIRyEFP5aqKCggL/97W80bdqUtm3b0qpVK4KDg5k7dy7FxcVGxxNCGMzhnrkrKufixYuMHj2aHTt2MGbMGMaPH09hYSELFy5k5syZ/PjjjyxevBhvb2+jowohDCJn/LXI5cuXGTx4MHv37mXOnDkMHDiQ/fv3ExUVRUxMDB988AGrVq1iwoQJ5OfnGx1XCGEQOeOvJaxWK/feey8HDhxg3Lhx/P73v0drTb169Zg8eTJKKaZNm4abmxsPPPAATz/9NHPnzjU6thDCAHLGX0t89NFHLFmyhNDQUJYsWcKjjz5KamoqWVlZDBw4kOLiYiZMmMCKFSt48skn+fe//833339vdGwhhAGk8NcCZ86c4dlnnyU6OpqHH36Yjz/+mLlz5xIY+L9hkcxmM6NHj2bx4sXk5OQQHh7OzJkzycrKMjC5EMIIUvhrgb/85S/k5+czb948nnrqKWbMmFHuck8//TTPPvss8+bNY9KkSZw9e5aXX365htMKIYwmhd/JHTp0iM8++wxXV1dSUlKuu/xrr71Gnz59+Ne//sWUKVOYM2cOqampNZBUCOEopPA7uRdffBGTyYTJZKJDhw7XXd7FxYX58+cTEhLCPffcQ3FxMa+//noNJBVCOAop/E4sKSmJb775huLiYl566aWr2vR/S7t27dixYwejR49mxowZfPTRRxX6tiCEqB2k8Duxd955B6UUgYGBPPbYYzf0WaUUFy9epH379litVmbPnl1NKYUQjkYKv5O6dOkSn332GVprXnnlFTw8PG54HVcuBg8ePJiPP/6YnJycakgqhHA0Uvid1CeffEJBQQGPvvQuLu2j+Dr2xptqHnvsMXx9fbFYLGRkZPCf//ynGpIKIRyNFH4n9NX2k7w990PadY1gwKgJuLi4AvB1bMpVP9fj6+vL448/zoYNGwgLC2POnDkyiqcQdYAUfieUnBDP2ZNHcffw/M3lyh4IyjsgPProo5hMJgIDAzl48CBxcXHVGV0I4QCk8DuhNd99BYB/04r14vktQUFBTJgwgYYNG+Lp6clnn31W6XUKIRybDNLmJK6cqefn5bJ9zY8ADLvt7ipZ98KFCzGbzdx1113897//5a233rqpi8VCCOcgZ/xOJn7TaixFBTQJCia4fViVrNNsNgMwadIkMjIyWLJkSZWsVwjhmKTwO5n1P/wXgBG331ul6/3000+ZOnUqzZs354svvqjSdQshHIsUfieSm5PF4X27aBnaif6jJlbpugcMGIDFYiE0NJTVq1eTnp5epesXQjgOKfxOZPfmtViKCrnv6Vfw8W1Ypetu164dkZGRnD17lqKiIn744YcqXb8QwnFI4XciG3/8hnq+DWnbuXu1rP+uu+7i8OHDNGvWjEWLFlXLNoQQxpNePQ6sdJ/7vJxsDu7aBgqKCgtwcXWtkvVO692y5PWUKVP44x//SHBwcElzT8OGVfvNQghhPDnjdxL7tsVgtRYTGtYDT+961bKNJk2asGDBAl544QUsFov07hGilpLC7yQ2LP8GgKETp1frdm6//XZuvfVWWrRoIe38QtRSUvidgNVqJTF+O8pkouegEdW6La018+fPp3PnzqxevZq8vLxq3Z4QouZJG78TOHZwL4UF+bQM7YiHl3eVrrtse79SitmzZ5OXl0dubi7r1q3j1ltvrdJtCiGMJWf8TmD3lrUok4nfvzq3RrY3adIkEhMT8fLyYtmyZTWyTSFEzZHC7wTiN6+lfbdeNA9uWyPbGzduHAAdOnRg2bJlMlSzELWMFH4Hd/HcGVKOHKy2njzlCQsLIygoCJPJxJkzZ4iPj6+xbQshqp8Ufge3Y/1PADT0b1Jj21RKMXr06JLB26S5R4japUKFXyk1SimVpJRKVkrNKme+u1JqoX1+rFIq2D7dVSk1Xym1XymVqJR6vorz13rbVtuK7qBbJ9fodufOncv27dvp06cPP//8c41uWwhRva5b+JVSZmAucAvQCbhTKdWpzGIzgHStdQjwNvCmffrtgLvWugvQE3jkykFBXJ/VauXk4QO4urnTtnN4jW7bxcXW4WvEiBHs3LmTS5cu1ej2hRDVpyJn/JFAstb6mNa6EFgAjC+zzHhgvv31YmCoUkoBGvBWSrkAnkAhcLlKktcBJ5ISsBQV0qp9Z0ymmm+Ve+mll/juu++wWq2sWbOmxrcvhKgeFakmgcCpUu9T7dPKXUZrbQEyAT9sB4Ec4CyQAvxTa33NqaNS6mGlVJxSKi4tLe2Gf4naas/W9QBE3Xq7Idv38fEhISEBHx8fVq1aZUgGIUTVq+4buCKBYqA50BDYpJRao7U+VnohrfU8YB5AREREne47WPqGqqQ9Owhq044hE6bV+Lan9W7JLbfcwjPPPENoaCgrV65Ea43ti5wQwplV5Iz/NNCi1Psg+7Ryl7E36/gCF4FpwM9a6yKt9XlgCxBR2dB1QWF+Pof27KBzrwGGZejYsSPNmjXDbDaTmprKoUOHDMsihKg6FSn8O4FQpVRrpZQbcAewtMwyS4ErzwKcDKzTtrt+UoAhAEopb6APINWjAg7u2oalqJDMi+cNy6CUYsiQIRw7ZvuCtnLlSsOyCCGqznULv73NfiawEkgEFmmtDyilXlFKjbMv9gngp5RKBp4CrnT5nAvUU0odwHYA+Uxrva+qf4naaPPP3wHQK/oWQ3NMmzaNRx55pKS5Rwjh/CrUxq+1XgGsKDPtxVKv87F13Sz7uezypovrO7hrKwBd+0YZmmP06NGMHj2a7OxsPvroI/Lz8/Hw8DA0kxCicuTOXQeUczmTzEsX8GvSHC9vH6PjkJeXR0hICHl5eWzevNnoOEKISpJhmR1A6d40APtiNwDQqWdfI+Jc409/+hPz58/HbDazbt06hg0bZnQkIUQlyBm/Azp2aD9KmRh1xwyjowAwZMgQsrOz6dixI+vWrTM6jhCikqTwO6BjB/fSumMXgtt1NjoKANHR0SilaNSoEXFxcVy+LDdfC+HMpPA7GEtRIUf276J5cIihOb6OTSn5adSoET169CAjI4Pi4mI2bdpkaDYhROVI4Xcwh/bupNhioTAv1+goV4mOjiYpKQlXV1fWr19vdBwhRCVI4XcwsWt+BKDXYGP775f1yCOPsHHjRvr16yft/EI4OSn8DubQ7lgAuvcbbHCSq4WGhhIZGcmQIUPYs2ePDNMshBOTwu9AtNacO52Ct48vXvXqGx3nGuvWrSMjIwOtNRs2bDA6jhDiJkk/fgdy9uQxii1FtAztaHSUq1y5z2D++/9h84pv8PT0ZP369UycONHgZEKImyFn/A7kSMIuAG576CmDk5SvQ4/e5ObmEhYWJu38QjgxKfwO5PDeOLzr+9K+Wy+jo5SrQ7dIAPz8/Dhw4ADnzp0zOJEQ4mZIU49Byg7TALBr42oa+DU25DGLFeHrF0D79u1LbuCKiYlh6tSpBqcSQtwox6wwddDl9ItkZV7C7OJqdJTfFBUVxYULF6hXr5409wjhpOSM30HsjLGNdd85op/BSX7b22+/jaenJ2PHjiUmJsboOEKImyBn/A5i10bbw8z7Dh93nSWN5eXlhVKK6OhoDh8+zJkzZ4yOJIS4QVL4HcTxpP2YTGZad+hidJTreuGFF9i5cyeA9OcXwglJ4XcAhQX5ZGVcoklQK4e9sFva6dOnWb9+PT4+PtLcI4QTcvwqUwccT9yPtlq5Y+bzRke5rq9jU3Bt3pG0tDR69uwphV8IJySF3wEk7bU1m7Tr0tPgJBXTrmsEAI0bN5Z2fiGckBR+BxCzbCGubu7U821odJQKaR4cgpdPfQoKCgBp5xfC2UjhN5jVaiXt7Ck8ves5Rfs+gMlkot+I8XTr1o369etLc48QTkb68Rss5Ugi1uJigts7xmMWK+r+P/2Nab1bEh8fL4VfCCfjHKeYtdjWVUsB6DFwuMFJbpzVaqVfv37Szi+Ek5HCb7ADcZsBiBw82uAkN6bYYiEwMJDDhw8D0s4vhDORwm+wjAvnaRzYEt9G/kZHuSFmFxdatmxJcnKytPML4WSk8Bso82IaGRfPM2zSXUZHuSn9+/cnLi6OAQMGSOEXwolI4TfQvlhb88iVfvHOpn///uTn59O2bVtp5xfCiUivnhpUdgz+TT99B4CXj+M9X7ciznm2AuDI+RzA1s5/5513GhlJCFEBcsZvoJOHD2J2caV5qxCjo9yUBn6NmfK7P9Fn2Fjq16/P+vXrjY4khKgAOeM3SEF+HtmZ6TRr2QallNFxbtr4+2YCMGjQIGnnF8JJyBm/QXZvsT29qkN4pMFJKqeosICkPTvp1asXR44c4fTp00ZHEkJcR4UKv1JqlFIqSSmVrJSaVc58d6XUQvv8WKVUcKl5XZVS25RSB5RS+5VSHlWY32ntXL8CgD5DxxqcpHJ+OXWCV343Ga01IP35hXAG1y38SikzMBe4BegE3KmU6lRmsRlAutY6BHgbeNP+WRfgS+B3WuvOQDRQVGXpndjl9Es0DGhChx69jY5SKYGtQ/HyqU9qaqr05xfCSVTkjD8SSNZaH9NaFwILgPFllhkPzLe/XgwMVbaG6xHAPq31XgCt9UWtdXHVRHdeVquVE0kJhPcfiouDP1z9ekwmE6FhPdiyZYu08wvhJCpS+AOBU6Xep9qnlbuM1toCZAJ+QDtAK6VWKqXilVLPlrcBpdTDSqk4pVRcWlrajf4OTufogT3kZl+mebBz9uYpq13XCBITE4mMjJR2fiGcQHVf3HUBBgDT7X9OVEoNLbuQ1nqe1jpCax0REBBQzZGMt3XVDwDUq9/A2CBV5MoNaD4+PoC08wvh6CpS+E8DLUq9D7JPK3cZe7u+L3AR27eDjVrrC1rrXGAF0KOyoZ1dYvw2AHoOGmFwkqoR0jmcTZs28dBDD+Hr6yvNPUI4uIoU/p1AqFKqtVLKDbgDWFpmmaXAvfbXk4F12tbNYyXQRSnlZT8gRAEHqya68/rl1Am86tXHq56P0VGqhJuHBwMGDMDb21va+YVwAtct/PY2+5nYingisEhrfUAp9YpSapx9sU8AP6VUMvAUMMv+2XTgLWwHjz1AvNZ6eZX/Fk4k7UwqRYUFtAjpYHSUKvXGV6sYd89jeLcMk3Z+IRxchdr4tdYrtNbttNZttdav2ae9qLVean+dr7W+XWsdorWO1FofK/XZL7XWnbXWYVrrci/u1iWx62zHvW59oo0NUsXOphxl2Rfv49vQD5B2fiEcmdy5W8MunT+Lm7sHwybfbXSUKnXlAu/ljEvSzi+Eg5PCX8OS9u4kJCwc73rOOSLnr2no34TGgS05sn+XtPML4eCk8Neg3OwsTiQl4OPbyOgo1aJd1wgO79tFVFSUtPML4cCk8Fezr2NTSn7iNqwEoIFf7bxXoV3XCKzFFrp06QJIO78QjkoKfw26Uvj7jiw74kXtEDXmdt7/eTdDhw6Vdn4hHJiMx1+DjiXuw2Qy0bZTd6OjVAsXVzcAzGaztPML4cDkjL+GFFssZFw4R6PGzTCZau9uX7noM8aPH090dLS08wvhoGpvBXIwx5P2o7WmfXfnfvDK9eTlZLN06VLCw8MBaecXwhFJ4a8hxw7uBWDK7/5kcJLqdaU/f1ZWFr6+vvIcXiEckBT+GnJoz04aNW6Gf9OyI1rXLm07d8dsduHjb1fStksvaecXwgFJ4a8BWmt2bVyJi6tzP3SlItw9PGnVvjOH98XRqWdfkpOTSU1NNTqWEKIUKfw14MzJY1iKimjcvKXRUWpEZPQtBLYOpWOPPoC08wvhaKQ7Zw3YuvJ7ALr3H2Jwkpox9p5HAbAWF9OgQQNiYmKYPn26wamEEFfIGX8N2L9jEwB9h4+7zpK1h9aawoJ86c8vhAOSM/5q8HVsylXvTx8/grunV60dqqE8f31wIg39GzNt/EiWLl1KamoqQUFBRscSQiBn/NUuN/sy+bk5Je3ddUXTFsElA7aBtPML4Uik8FezI/vjAbjljhkGJ6lZ7bv14nL6BdYnZ+LlU5+PF/1odCQhhJ0U/moWv3E1ymQiJKxuPWP+yo1cRxJ206F7bxLjtxucSAhxhRT+aha7fgUmkxkPTy+jo9SowNahePnUt/Xn79GHc6knpD+/EA5CCn81KizIJyvjEo2btzA6So0zmUzc8dgs+gwdI/35hXAwUvir0e4tawHoEN7b4CTGGDpxOmGRA2gZ0hEvn/rSrVMIByHdOatR7NoVAPQbUTsfvHI91uJijiXuw6dBIzp07y2FXwgHIWf81Sg5IR5lMtXZM/6iokJeeWQyMcsW0qlHHxm3RwgHIYW/mlitVnJzsugSObBWP3jlt5QesO1KO7+c9QthvLpZkWrA6eNHyMvOqlPDNJSnXdcIjh7cQ/NWbalXvwHz/vvDNXc2CyFqlhT+arL5Z9vAbK07dDU4ibHad42gqKCAlORDdOk9iH3bN2C1Wo2OJUSdJoW/muzasBKApi1aGZzEWO262W7kOrQnlm59o8m8lEbKkYMGpxKibpNePVWkbPPF+TOn8GnQCFc3d4MSOYYGfo158YPFBHcIIz8nG4C922LgntHGBhOiDpPCXw1Sjx+m2FJEcPswo6M4hPbdewG2i73B7cPYu11u5BLCSNLUUw02rfgOgJ4DhxucxDGkXzjHovf/zpkTyXTrE8WR/bvIyMgwOpYQdZYU/mqQtGcHUHdv3LqGhh/mz2Xvthi69R2MtbiYl95fwNexKdLDRwgDSOGvBnk5WXSO6I93fV+joziEhgFNaBIUzMH47YSEheNVr76tnV8IYYgKFX6l1CilVJJSKlkpNauc+e5KqYX2+bFKqeAy81sqpbKVUs9UUW6HlXnpAqnHDhMWOcDoKA6lY3hvkvbuQJlMhEUOYO/2DWitjY4lRJ103cKvlDIDc4FbgE7AnUqpTmUWmwGka61DgLeBN8vMfwv4qfJxHd/6JV8D0KxFG4OTOJaOPfqQczmTlOREuvWJIj3tF04dTTI6lhB1UkXO+COBZK31Ma11IbAAKNt4PR6Yb3+9GBiqlFIASqkJwHHgQJUkdnBxm1YD0D480uAkjqVjjz54evtw4WwqXftEA0hzjxAGqUjhDwROlXqfap9W7jJaawuQCfgppeoBzwEv/9YGlFIPK6XilFJxaWlpFc3ukE4fO4yntw/1GzQyOopD8WvSnHmr9hIRNZJGjZvSql0ndm9eY3QsIeqk6r64+xLwttY6+7cW0lrP01pHaK0jAgICqjlS9Uk7fYrCgnxatSvbEiYATGZzyeueg0ZyeF8cmZcuGJhIiLqpIoX/NFD6EVJB9mnlLqOUcgF8gYtAb+DvSqkTwB+BF5RSMysX2XHF/LgQgIiokQYncUxHD+xh1vSRnDqaRM9Bw9FalzysRghRcypS+HcCoUqp1kopN+AOYGmZZZYC99pfTwbWaZuBWutgrXUw8A7wf1rrOVUT3fGcOHwQZTIx8JZJRkdxSPUb+nHq6CES47fRKrQT/k2D2LVhldGxhKhzrlv47W32M4GVQCKwSGt9QCn1ilLqypjDn2Br008GngKu6fJZF5xPPUG3PlHU821odBSHFNC8Bf5NgzgYvx2lFD0HDWf/zk3k5OQYHU2IOqVCY/VorVcAK8pMe7HU63zg9uus46WbyOc0LvxymjMnjxI9bqrRURxaxx692bN1PVarlZ6DhrNy0Wf8ec5X9IoeBcC03i0NTihE7Sd37laRtd9/CYBPAz+Dkzi2jj36kpVxiVNHD9G+eyTe9X3ZtXG10bGEqFOk8FeRK33SIwaNMDaIg+sSOZC+w8ehlAkXF1e69xvC7s1rKLZYjI4mRJ0hhb+KnDlxFG8fX7x86hsdxaE1atyUma/OpmVIB8B2oMy+nMGhvTsMTiZE3SHj8VfClZElTx45SFFhAR3DexucyHmcP5NCo4CmdO0bjbuHJ7FrltO5Zz+jYwlRJ8gZfxVY+52tfb//qIkGJ3EO+7Zv4MlJA0naF4eHpxfhA4axY/0Kae4RooZI4a8CF86dwdO7Hr2HjTE6ilMI6dIDk9nMgR2bAeg7fCxZGZc4sGurwcmEqBuk8FeStbiYowm76RV9C66ubkbHcQpe3j6EdA5n/05b4e/aJwpPbx+2r1lW8nAWeUCLENVHCn8l7d2+gezLGXSQ0ThvSFivARxP3EfO5Uzc3D2IiBrBzpifsRQVGh1NiFpPCn8lrVvyFQDNW4UYnMS5hEUOQGvNgbgtAPQZNobcrMvsi91kcDIhaj8p/JV0eN8uzC6uhISFGx3FqbTt3J3HX3mPTvaePGG9BlCvfgO2rfrB4GRC1H5S+CshJyuT7Mx0AoNDsD93RlSQi4sr/UaMp55vA9t7Vzd6Dx1D3IaV5GZfNjacELWcFP5KiFm6AICeA4cbnMQ5ZWVc4qf/fsyFX2yjfEeNnUJhQT7b1/xocDIhajcp/JUQZx9SeMik6QYncU652Vl8+e6rJfuxTceuBLVpx4YfFwFIDx8hqokU/krIuJhGh+6RNApoanQUp9QkqBXNWrVlz9Z1ACilGHTr7SQn7Ob08SMGpxOi9pLCf5MOHz7M+dMn6TNsrNFRnFr3foNJjN9Ofl4uAANGTcRsdmHDj98YnEyI2ksK/w260vTw4JMvANCxRx+DEzm38P5DsBQVcmCnrVunr18A3fsPYfNP32GxFBmcTojaSQr/TdqzdT1mswuBrUONjuLU2nfrhXd9X86cPFoyLXrsVDIvpRFfapx+ae8XourI6Jw3ITP9IlkZl2gZ2km6cVaSi6sbc5buwM3Do2Ra936DCWgWxKpv5hM5ZLSB6YSoneSM/yas/uZzwHa3qai80kUfwGQ2M+y2u0ncvZ2UI4kGpRKi9pLCfxN2rP8JgOGT7jY4Se1gKSrk/2beyfKvPiyZFj32DtzcPVhpP8gKIaqOFP4bZLVaufDLaQKat5SnbVURF1c3crOz2BmzsmRaPd8G9B81ka0rl5CdmWFcOCFqISn8Nyg5IZ6CvFxuf+Rpo6PUKj0HDSc5IZ70C+dKpo24/T4KC/JZ/e1/DEwmRO0jhf8GbVrxLSazC+H9hxgdpVbpFX0LWmt2lerJ0zKkA+H9h/Lzwk/Jz80pmS49fISoHCn8N0BrzcYfv6FefV+86kkzT1UKbB1Ks5Zt2Bnz81XTx9/3ONmZ6axb8rVByYSofaTw34Dly5djsRTJTVvVQCnF6GkPXfNNKrRLTzr17Mvyr+dRVFhgUDohahcp/Dfg3XffBeDW6Y8YnKR2GjJhGqOmPnDN9PH3zSTjwnnW20dDFUJUjhT+G7BlyxY8PL1p26mb0VFqrdzsyyTGb79qWueI/nQI7833n7xHXk62QcmEqD2k8FfQxo0bycvLo0N4b6Oj1GrffvQ2bz55z1UXc5VSTJv5ApfTL7D863kGphOidpDCXwFfx6bw3pe2RwJOuP/3Bqep3SKiRlJUUMDuLWuvmt62c3d6D72VFV/Nu6rL59exKZzPKuB8lrT/C1FRUvgrQGvN9jXLaNc1gtAuPYyOU6u17x5Jw4CmbF157bN3p/zuWSxFRSx6/x8GJBOi9pDCXwF7tq7j9PEjhPcfanSUWs9kMtFvxDj2boshKzP9qnlNWwQzetpDbFz+DQd3bTMooRDOTwp/BSyd/28AuvWNMjhJ3dBvxHiKiy3sj914zbyJM/5AQPMWfPrmC9d075Qbu4SomAoVfqXUKKVUklIqWSk1q5z57kqphfb5sUqpYPv04UqpXUqp/fY/ne521+LiYpIP7Ma7fgNatetsdJw6oVW7zvxj4Tr6jRh/zTx3D08eeO7/OJtyjB8+n2NAOiGc33ULv1LKDMwFbgE6AXcqpTqVWWwGkK61DgHeBt60T78AjNVadwHuBb6oquA15ZNPPsFaXEyv6JFGR6kzlFI0b9X2V+d37T2IgaNvY8nnc0jau7MGkwlRO1TkjD8SSNZaH9NaFwILgLKnYuOB+fbXi4GhSimltd6ttT5jn34A8FRKuVdF8Joye/ZsACbO+KOxQeoYi6WIuS8+wc8LPyt3/j1Pv0xAsyDm/vUPFOVm1XA6IZxbRQp/IHCq1PtU+7Ryl9FaW4BMwK/MMrcB8Vrra/rdKaUeVkrFKaXi0tLSKpq92uXl5XHkyBH8mwbh36S50XHqFBcXV9LOprL2uy/QWl8z38vbh5mvzCYj7RyJ/30NbS2+ar609wvx62rk4q5SqjO25p9yxzrQWs/TWkdorSMCAgJqIlKFLF68mIKCAh7689+NjlInRY+bypmTRzm8N67c+W07d+eep17i0qFYji7/sNxlhBDXqkjhPw20KPU+yD6t3GWUUi6AL3DR/j4I+B64R2t9FCcyZ84cQkJC6Nyzn9FR6qQ+w8bi4VWP9Uv/+6vLDLvtbgL7TyR14ze/2ixU+uxfvgEIUbHCvxMIVUq1Vkq5AXcAS8sssxTbxVuAycA6rbVWSjUAlgOztNZbqihzjYiJiWHHjh107txZHqhuEA9PL/qPnEDs2uXkZGX+6nJtxz6Of9hAvnj7JdZ+/1UNJhTCOblcbwGttUUpNRNYCZiBT7XWB5RSrwBxWuulwCfAF0qpZOAStoMDwEwgBHhRKfWifdoIrfX5qv5FqtrDTzwDQI9x9xucpG4bOmk6Hl7eWIuLf3UZk9lMp+l/4fKPb/Dpmy+Qm53FmLsekQO2EL9ClXfhzEgRERE6Lq78Nt2akpubSz0fH3x8G/L+T/GGZhHX99GmYwDcG9mcD199hu1rljFk4nTuefKvuLpd24lsWu+WNR1RiGqnlNqltY6oyLLXPeOvi2bNmoW2WhkycbrRUQS2sZL2x26kgV9jWoZ2/NXl3Nw9ePyV9whoFsSyL97n6IHdzHx1zjX3BJRu55eDgKiLZMiGMrTWzJ8/H7OLC5Me+IPRcQRQkJ/H7L/M5PvP3rvusiaTiTsen8VTf/+Yi+fO8vxdo1j0/t/Jz8utgaRCOAcp/GVs3ryZy5cvM+auRzG7yBciR+Dh6cWQ8dPYGfMzaWdOXf8DQM9Bw3njq5X0HnorP8yfy9O3R7Hsiw/Izb5czWmFcHxS+Mt45513aNSoEePve9zoKKKUEbffi1KKld98XuHPNPRvwmMvvcOLHy4mqHU7Fsx9nZljezP3xSeI37RGnuYl6iw5pS1l+/btfPfdd9x11124e3gaHUeU4tekOb2H3ErM0oVMevCPeHn7VPiz7bv14vnZX3H80H7Wfv8VO9avYOuqHzCZzbzevgttOnWleau2TB/Zl+bNm+Pv74+fnx8u8o1P1FLSq6eULl26kJCQwNatWzluKjsqhTDa0YN7efu5h3n6Hx/TukOXkulXevU8NLBNhdZjKSrk0O4dHIzfxqHdsaQkHyIv59rxflxdXXFzc8PV1RVXV1dMJhNKqZIfsF0TslqtJT8+Pj6YTCZycnLIy8srGW7CZDLh6upK9+7dadq0KcXFxbi4uNC+fXu6d+9O3759adSoUWV3kajDpFfPTYiPjychIYH27dvTt29fjssdng6nbaduvPP9ZlxcXCu1HhdXN8IiBxAWOQCwFe+Mi+f5JeU4mZcucDnjIq3rWcnPz6ewsJDCwkIyMzNJS0ujWbNmmM1mEhISiIuLw2KxXLXuwYMH4+7uzr59+zhy5EjJwcJisZCbm0tmZiZHjx7l1KlT13zWbDYzYMAAwsPD8fPzo2fPngwbNgxX18r9vkKUJYXf7sEHHwRg8hMvy239DszFxRVLUSFnTh6jZUiHKlmnUoqG/k1o6N+kZFp62jli1y0nMX43KYm7OX/eds/htm3b6NOnD2vXrmXZsmW0bduWNm3a0KxZM/z9/QkMDMRsNl93mxcuXCAxMZG9e/eSkJDAoUOHOH/+PHl5eXzwwQfk5+eXLOvn50e3bt2YPHky9913H56e0gwpKkeaeoCEhAS6dOlC48CWvP3tphrdtrhx77/8JPu2b+Sd7zfj7uF5w0095UlPO8fuLeto27kbrUI7kbg7lr89OoWAZkGMGjaYiIgIwsPD6dmzJ15eXlX1q5TLYrGwdOlSli9fzvbt2zl27FjJgcDNzY1Bgwbh7+/PPffcw6hRo+QOZQFIU88Ne/fddzGbzTzw7GtGRxEVMGT8NDb/9B2rFn3O2Hseval1aK05fmg/8ZvXsHvzWk4kJQC2Rzu2Cu1EaFg47y7Zin/TwKtu8qqJm79cXFyYNGkSkyZNKpl27Ngx1qxZw+HDh1m+fDlr1qxhwYIFuLq6EhYWxn333ccjjzyCu7tTPe5CGKTOn/EfOHCArl278vvf/57IO5+qse2Kyvnn0w9waM8O3lq8gYUJtgHcrnfGr7UmK+MS9Rv6YSkq5LFbI8jNzqJdl5507z+E8P5DCWrTrsJn0DV9QCht69atvPfee6xbt44rz7Dw8fHh7rvvZuLEiURFRcm1gTrmRs7463zhb9WqFefPnyc1NZWVyTk1tl1ROWdOJPPc9BEMmTANS58HgF8v/GdOJLN11Q9sX/MjVmsx//pmA0opkvbupHlwCD6+Das0W3UU/t86sJw9e5Z//OMfHDlyhLVr15KXl4dSJlqGdmT0nQ8y58UnMJnklp3aTpp6Kmj27NmkpKQwdOhQ/Pz8QAq/02geHMKQCdM4fiiBwF4WTOZr/ynvj93Etx+/xZH98Sil6NSzL32GjcVaXIzZxYX23XpVe86KdBT4tQNFRT57ZZmIqX/krd4tycnJ4R//+Afvznmfk4cP8P7LT/LJ688xdOhQPvjgA1q2lLGJRB0+48/Ozsbf3x+LxcIvv/yCv7+/9OZxMgX5ebi6ufPJlhMAPDigNUf27yKgeQsa+jdhZ8zPLHr/70SPm0q/kROu6rXjSH6tyaiyzqemsPjjt9i1cRX5uTkok4kRw4czcOBAZs6cia+vb5VtSxhPmnoqYMSIEaxevZqJDzzB5IefrvbtierzzrfrSd38PepMAmdOHmXSg09y24N/xGq1XnWzVV2WciSR2HXL2bB8MennzwKKLl3CePbZZ5k+fTpKqV896MgIps5BCv91xMbG0qdPHwIDA3lz8RYpDE7KarUy5y+/J3btjwC07tiFYRPvovfQMXh61zM4nWOyFBXx04KPWfPtF1z4xfYEVVc3d6LGTmXSA0/g63ftM6+l8DsHKfy/oaioiIEDB3LgwAFiYmJIsjjOw93F9Z1LPcmBuC0MmTANgE/eeJ4Dpy5yfvcaekWP5InX/i0H8gpKO3OKhe//nV2bVlOYn4fZ7EK7br3oFNGXMXf9DrdyHmJTmhwQHItc3P0NzzzzDLGxsSxcuJCePXuSJO36Dq8wP58d61cQs2whifHbMZnNhA8YSkP/JsyY9TofbTqGd5NW7Fgxj5ilCxg8/k6jIzuFgOYtmPnqbABOHz/Chh+/YdU380mM38Z3H79DaFg4tz30NGG9+hucVFS1OnXG/8EHH/Doo48SHR3N+vXrgaq9mCaq3oFdW3nnuUfIzb5M48CWRI2ZwsBbJ+PXuFnJMh9tOoa2Wrm4+K8c3hfHq58vI6h1OwNTO6/cnCy+//hdNv30LVkZlwDwadCIyQ8/Td/hY/H2Kf+CsJz9G0+aesqxa9cuIiMjMZlMHD9+nKCgIEAKv6PJyrjElpVL8G8WRMSgEWRlpvPlO68waMztdAzvU25/9CtDNkzu6M2CuW8w/Yk/U7+hX01Hr3WSD+zmmw/+yaljSWReTMPFzY2mQcGMumMGUWOmXPV3IYXfeFL4y7h48SItW7YkNzeXxYsXc9ttt5XMk8JvvGKLhf07NhKzbBHxm9ZQbCli8Pg7efD5Nyr0+fLG6iksyMdsdpGnqFWBK8NbLPvyA3asXQ5cGeF0IFMf/RMtQ65+DrIcBIwhhb+UoqIi2rRpQ2pqKi+88AKvvfaaFHsH84+n72fPlnX4NGjEgFETGTRmyg2NvFm28BcW5PN/M++kdfsu3PP0y3KxtwpdOHeGRe//nbgNKymwP8e4TadujLz9PnoNvuU3H2AkB4TqJRd37YqLi3nggQdITU1l7NixvPaaDMJmtLSzqWxbvZSd639i1ntf4u3jy/Db7iF6zBTCBwzFxdWt0ttwc/cgtEtPVnz9ER7e9Zjyuz9J8a8i/k2a89hL76C1ZtfGVfz45YdkXDzP+y8/yUevzyKodSijpz9Mv+HjZJ87sFpb+IuKipg2bRqLFy/mtdde44UXXjA6Up2Vm32ZrauWsnXlEpL27gQgtEtPMi6cx9vHl+79Blf5Nqf9/v+Rn5fL0vlzMZvN3PbQU1KIqpBSioiokUREjcRqtZK0Zwfz/vYnTiQl8O8Xn2Deq8/QsUdfxt37KJ169AVqfiA78etqZVNPeno6Xbp04fTp08ycOZPZs2dL804NSzubiqWokGYt23A25RjPTBlMYOtQ+o+cQN8R42jcvOr+4//aePxWq5WPX3+ODcsWMfnhp5n4wBNVtk1RvrOnjvPtvLfYvWUt+bm2sa+C23WmZ9RIWnfsQve+g685AMtBoGrU6aaePXv2MGjQILKyspgwYQLvvfee0ZHqBK01p5IPsWvTauI2rOREUgJ9h49j5quzadayDW/+dzWBwaE1etZtMpl48Pk38W8aRN/h42psu3VZsxatS+4NSNqzk91b1nJozw6+/egtAMwurgS378yg0ZOJGjMFV3l+gCFq1Rn/G2+8wQsvvIDWmieffJK33nqrZJ6c8Ve9wvx83Dw8APjbY1NJjN8OQEhYOL2iRxERNYqmLYKrPUdFn8CltebLd1+l77CxhISFV3su8T+njiax5LPZJOzYRPblDNtEpQiL6M9Tjz7A8OHDS7pYi5tTJ3v1rF27lmHDhuHu7s6iRYsYN+7qMzwp/JWXl5PN4X1xHNqzg4O7tvJLynH+vWIXZhcXNv30HdZiC117R9EwoGZHwaxo4c+8mMaLMyaQnvYLY+56hAn3P1Fy4BI1J+1MKj8v+pT4TaspyMsl89IFwNZFNDA4hG79BhM1ZgpPTR5kcFLnUicLf3FxMf/85z954IEHCAiwjb8jxb5y0tPO4V3fFzd3D1Ytns9/3noJbbViMptp3aEr3fpEcev0h/Hw8jY05408czfnciZfvPMym1Z8S+PAltzx+Cx6Rd8iDyoxiNaaU0eT+PHL99m7NeZ/3wYAV3d3Bt4ymY49evPg2IF07dpV/p5+Q50s/OWRwl9xeTnZJO3dwbHE/RxP3MexQ/vIuHCe52d/RVivARw9uJfdm9fQoXtvQsLCDS/2pd3Mw9YP7NrK/H++SM7lDN76dtNv9j8XNSc3O4tNKxYTv3kN506dJCsznfzcbMDWk8i7fgO6du5Iv379mDhxIn379pXeWnZS+O2k8F9Na012Zjqpxw5z+vgRUo8fpseAYXTtE8XxQ/v5831jUErRrFVb2nTsSusOXekVPRK/Js2Njv6bbqbwA1iLizl3+iTNWrah2GLhld9NpmvvKPqNHE+zlje2LlE9ii0WUpIT+XnBpxxJiOfS+bMUFRaUzG/QoAGdOnUiKyuLtm3bEhERwdChQ+nVqxdms9nA5DWvThf+ulzstdbk5WSRnnaOtLOpnD+TQpPAVnTrG01uThZPjOtLXk5WyfKe3j5MfvhpRk29H0tRIUcSdhPcrrPTjWV/s4W/tIyL5/nw1WfYH7sRrTVBbdvTK2okA0dPpklQq6qKKqpATmYmcZtWcTblKLlZWZw6eojD+3YBV9eyJk2aEBkZSVBQEGfPnqVjx45069aN8PBwQkJCal2zUZV351RKjQLeBczAx1rrN8rMdwf+A/QELgJTtdYn7POeB2YAxcATWuuVFfw96jytNUUFBSUXIJP27iTj4nlyLmeSlZlOxoXzNAkKZtTU+wF4/NZeZF5Ku2odA0ffRre+0Xh61SN63FT8GjcjsHUogW3a0SigacnXZBdXNzqG967ZX9CBNPBrzHPv/IeL586wM2YlO2N+Ysnnc+gQ3psmQa1I2ruTzT9/T8u2HWgR0oEmga3w9QuodcXDGXj7+hI15varpmmtOX38CAk7N5N8YDeFF0/j6urKyZMnWbNmDXl5eSxZsuSqz/j5+REaGoqvry+pqakEBATQpEkTmjVrRmBgID169KB169b4+vri6+tbq75BXLfwK6XMwFxgOJAK7FRKLdVaHyy12AwgXWsdopS6A3gTmKqU6gTcAXQGmgNrlFLttNbFVf2LlEdrjda65PF7xRYLxcUW23Sr1T7fiqe3D0op8vNyKczPQ1utWLUVNFi1tWQI4KyMS+TlZKO1pthShKWoCI2mVWgnwPZ4u0tpv2CxFFFsKaLYYsHFxZXIIaMB2Ln+J86cPEpBXi4F+Xnk5+Xi06ARdzz2HAAfvfYsxxL3UZCfS252FjlZmYSG9eDFDxcD8PHrszhzIrnk9/P09iEiakTJ+5FT78fFxZUG/o0JaBZEQPMWNPBrDNjaR+/6w1+qf6c7Ob8mzRk19X5GTb2f3OzLuLnbDrrnUk+yfc0y1n3/VcmyZhdX/vVNDAHNgtgZ8zP7YzdRz7cBPr4N8a7fAHdPTyIGjcTs4kLa2dSS9bm6uWMymTGbzSVPvCosyEdrjclkwmQyYzKbpe36BiilCGrTjqA21w7HbbVaOZd6guSE3dTLPcPRo0c5deoUjRs3prCwkMOHD3PixAmu1/qhlMJkMuHi4oKrqysdO3akWbNm5OXlkZKSgqenJ15eXnh4eODp6Unfvn3x9/cnPT2d06dP4+HhUfLj6elJeHg4Pj4+XL58mczMTLy8vOjWrRtNmzatrt1UoiJn/JFAstb6GIBSagEwHihd+McDL9lfLwbmKNu/2vHAAq11AXBcKZVsX9+2qol/tX/9618899wse9HWJX+Rc5fvpIFfY77/9F2+//TaG7o+WZ+Ih6cX33z4T35e8MlV85RSfLntBAAL5r5BzLKFV8339Pbh47UJACz5fE7JYwCvaBjQtKTwxyxbyJ6t6zGZzbh7eOHu6XnVuPFePvUJaN4Cdw9PvOr54O3jS9NSbc2Pv/wuJrML3vV9qVe/wTUXJMff+/iN7C5xHV716pe8HnTrZAaOvo1Lab9wKvkQF86mcuHcGRr62w6sZ04eZcf6FWRfzkBbrSWfm7/ZdqD+8Yv3WfPdl1et39Xdnc83HAZsB/UtP39/1fwGfgHMXW5r9nxn1iPEb1pz1fzGgS355yLbcyXe/OM9HNz1v/9WSilahHTg1U+XAvDK727neOK+knkAIWE9eGHO1wD8+b4xnE05dtX6w3oN4Mk35wHwzNQhpKf9ctX8iKiRPPrXtwGYObb3Vc2IAANGTeT+Z23jYz00rAtW69Xne8Mm3c2dM5+nMD+fR0f3oKxbpz/CpBl/4HL6RZ68beA18yc/9BS33Pkg50+n8Pzdo66ZP/2JvzBkwp2kHk3i5Uduu2b+jHH30G/EeI7s38Ubf7i75ETxyknh8Mn30KJtB9KTYvnuu+/QWmO1WiksLKSgoID09HSKioo4ceIEGRkZ16x/+fLl10y7nilTprBw4cLrL1hJ123jV0pNBkZprR+0v78b6K21nllqmQT7Mqn290eB3tgOBtu11l/ap38C/KS1XlxmGw8DD9vftgeSKvE7+QMXKvH56uTI2UDyVZYj53PkbCD5KuNKtlZa6wo9S9YhhmzQWs8D5lXFupRScRW9wFHTHDkbSL7KcuR8jpwNJF9l3Ey2ilyZOg20KPU+yD6t3GWUUi6AL7aLvBX5rBBCiBpUkcK/EwhVSrVWSrlhu1i7tMwyS4F77a8nA+u0rQ1pKXCHUspdKdUaCAV2VE10IYQQN+O6TT1aa4tSaiawElt3zk+11geUUq8AcVrrpcAnwBf2i7eXsB0csC+3CNuFYAvweA306KmSJqNq4sjZQPJVliPnc+RsIPkq44azOdwNXEIIIaqX3H0ihBB1jBR+IYSoY2pN4VdKjVJKJSmlkpVSs4zOU5ZS6oRSar9Sao9SqmpGoatcnk+VUuft92BcmdZIKbVaKXXE/mdDB8v3klLqtH0f7lFKjTYoWwul1Hql1EGl1AGl1B/s0x1i//1GPsP3n1LKQym1Qym1157tZfv01kqpWPv/34X2jiQ17jfyfa6UOl5q33U3Il+pnGal1G6l1I/29ze2/0ruVnPiH2wXnY8CbQA3YC/QyehcZTKeAPyNzlEqzyCgB5BQatrfgVn217OANx0s30vAMw6w75oBPeyvfYDDQCdH2X+/kc/w/QcooJ79tSsQC/QBFgF32Kd/ADzqYPk+ByYb/W+vVM6ngK+BH+3vb2j/1ZYz/pJhJbTWhcCVYSXEr9Bab8TWA6u08cB8++v5wISazFTar+RzCFrrs1rrePvrLCARCMRB9t9v5DOctsm2v3W1/2hgCLbhXsDYffdr+RyGUioIuBX42P5ecYP7r7YU/kDgVKn3qTjIP/RSNLBKKbXLPkSFI2qitT5rf/0LULPPUKyYmUqpffamIMOaoq5QSgUD4djODB1u/5XJBw6w/+zNFHuA88BqbN/WM7TWFvsihv7/LZtPa31l371m33dvK9uIxEZ5B3gWuDIolB83uP9qS+F3BgO01j2AW4DHlVIO/UBRbfvO6FBnOsD7QFugO3AW+JeRYZRS9YBvgT9qrS+XnucI+6+cfA6x/7TWxVrr7tju5I8EOhiR49eUzaeUCgOex5azF9AIeM6IbEqpMcB5rfWuyqynthR+hx8aQmt92v7neeB7bP/gHc05pVQzAPuf5w3OcxWt9Tn7f0or8BEG7kOllCu2ovqV1vo7+2SH2X/l5XOk/WfPkwGsB/oCDezDvYCD/P8tlW+UvflMa9tIw59h3L7rD4xTSp3A1qQ9BNuzUm5o/9WWwl+RYSUMo5TyVkr5XHkNjAASfvtThig99Ma9wA8GZrnGlaJqNxGD9qG9TfUTIFFr/VapWQ6x/34tnyPsP6VUgFKqgf21J7bnfCRiK7CT7YsZue/Ky3eo1AFdYWs/N+Tfntb6ea11kNY6GFudW6e1ns6N7j+jr05X4VXu0dh6LxwF/p/Recpka4Otp9Fe4IAj5AP+i+3rfhG2NsEZ2NoK1wJHgDVAIwfL9wWwH9iHrcg2MyjbAGzNOPuAPfaf0Y6y/34jn+H7D+gK7LZnSABetE9vg20cr2TgG8DdoH33a/nW2fddAvAl9p4/Rv4A0fyvV88N7T8ZskEIIeqY2tLUI4QQooKk8AshRB0jhV8IIeoYKfxCCFHHSOEXQog6Rgq/EELUMVL4hRCijvn/1C8BP9bdzT4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GaussianMixture(2).fit(df.score.values.reshape(-1,1))\n",
    "x = np.linspace(np.min(df.score.values), np.max(df.score.values), 1000)\n",
    "logprob = model.score_samples(x.reshape(-1, 1))\n",
    "probs = model.predict_proba(x.reshape(-1, 1))\n",
    "means = model.means_\n",
    "index = np.argmax(means)\n",
    "thresh = x[np.where(probs[:,index] > 0.5)[0]][0]\n",
    "responsibilities = model.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.hist(df.score, 100, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.vlines(x = thresh ,ymin=ymin, ymax=ymax)\n",
    "tmp_df = df[(df[\"score\"] > thresh)]\n",
    "ppm = logomaker.alignment_to_matrix(tmp_df.seq, to_type=\"counts\")\n",
    "ppm = ppm[[\"A\", \"C\", \"G\", \"T\"]]\n",
    "ppm = ppm.div(ppm.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb3037fc-5280-4b5a-a059-c01c38443fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:17:43.334915Z",
     "iopub.status.busy": "2024-10-03T12:17:43.334719Z",
     "iopub.status.idle": "2024-10-03T12:17:43.718671Z",
     "shell.execute_reply": "2024-10-03T12:17:43.718161Z",
     "shell.execute_reply.started": "2024-10-03T12:17:43.334888Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,2))\n",
    "logomaker.Logo(ppm.applymap(get_information_content), ax=ax)\n",
    "ax.set_ylim([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70656cb4-0e09-4761-ac57-74943227896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:38:52.943753Z",
     "iopub.status.busy": "2024-10-03T12:38:52.943547Z",
     "iopub.status.idle": "2024-10-03T12:38:52.953671Z",
     "shell.execute_reply": "2024-10-03T12:38:52.953162Z",
     "shell.execute_reply.started": "2024-10-03T12:38:52.943727Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(motif_out, \"w\") as f:\n",
    "    print(\">{0} {0}_{1}\".format(TF, cycle), file=f)\n",
    "    for i in range(ppm.shape[0]):\n",
    "        print(\" \".join([\"%.5f\" % v for v in ppm.values[i,:]]), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
