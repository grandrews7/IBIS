{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b173f532-ef4a-4da1-b3bf-ba7a390be0a2",
   "metadata": {},
   "source": [
    "# PWM-A2G Pipeline\n",
    "This notebook will walk you through how to reproduce our results for a single TF and cycle of HT-SELEX. For each combination of TF and cycle, we perform our CNN-based motif discovery algorithm (adapted here for HT-SELEX data) and add an additional step to derive a new threshold for inclusion of seqlets in the final motif. This helps increase the information content of the final motif, especially for earlier HT-SELEX cycles. The notebook is self-contained, so there are a decent amount of helper functions and classes. The notebook as is presented takes abount 20 minutes to run top to bottom, but this is only training the model for 200 epochs, but we trained for 1000 epochs in the actual pipeline. This is so you can more quickly ensure it runs to completion. I chose to highlight LEF1 C3 in this notebook as it demonstrates both the sensitivity of CNNs and the large gain information content of the final motif after applying the Gaussian mixture model derived threshold. Outline of what is performed below\n",
    "1. HT-SELEX reads are imported (all replicates for a given TF / cycle combination are used)\n",
    "2. Gapped kmer enrichment is performed to find the single most enriched gapped kmer\n",
    "3. Data generators are initilized (each batch consists of a random subset of reads with an equal number of negative sequences generated by dinucleotide shuffling)\n",
    "4. Single kernel CNN is initialized and seeded with the one-hot encoded, enriched gapped kmer\n",
    "5. Model is trained for 200 epochs\n",
    "6. Input sequeces are scanned with the convolution kernel and any seqlet with an activation > 0 is saved\n",
    "7. A 2-component Gaussian mixture model is used to derive a new threshold for the inclusion of seqlets in the final motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497c6f76-4065-4460-99c3-449d6f7a76e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:49.783478Z",
     "iopub.status.busy": "2024-10-03T12:41:49.783330Z",
     "iopub.status.idle": "2024-10-03T12:41:51.875123Z",
     "shell.execute_reply": "2024-10-03T12:41:51.874360Z",
     "shell.execute_reply.started": "2024-10-03T12:41:49.783456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv1D, maximum, GlobalMaxPooling1D, Dense, GaussianNoise, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "\n",
    "import random\n",
    "from tqdm import trange\n",
    "from subprocess import Popen, PIPE, run\n",
    "import sys\n",
    "import pickle\n",
    "from pyfaidx import Fasta\n",
    "from  tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "import glob\n",
    "import bioframe\n",
    "import os\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import gzip\n",
    "import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce66087-fa31-4836-a096-54d3753e5a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:52.954655Z",
     "iopub.status.busy": "2024-10-03T12:41:52.954490Z",
     "iopub.status.idle": "2024-10-03T12:41:52.971359Z",
     "shell.execute_reply": "2024-10-03T12:41:52.970881Z",
     "shell.execute_reply.started": "2024-10-03T12:41:52.954632Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Models\n",
    "def construct_model(num_kernels=32,\n",
    "                    kernel_width=24,\n",
    "                    seq_len=None,\n",
    "                    dropout_prop=0.0,\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.0001, seed=12),\n",
    "                    optimizer='adam',\n",
    "                    activation='linear',\n",
    "                    num_classes=1,\n",
    "                    l1_reg=0.0,\n",
    "                    l2_reg= 0.0,\n",
    "                    gaussian_noise = 0.1,\n",
    "                    spatial_dropout = 0.0,\n",
    "                    rc = True,\n",
    "                    padding=\"same\",\n",
    "                    conv_name=\"shared_conv\"):\n",
    "    if rc:\n",
    "        seq_input = Input(shape=(seq_len,4))\n",
    "        rc_op = Lambda(lambda x: K.reverse(x,axes=(1,2)))\n",
    "        seq_rc = rc_op(seq_input)\n",
    "        if gaussian_noise > 0.0:\n",
    "            noisy_seq = GaussianNoise(gaussian_noise)(seq_input)\n",
    "            noisy_seq_rc = rc_op(noisy_seq)\n",
    "        \n",
    "        shared_conv = Conv1D(num_kernels, kernel_width,\n",
    "                             strides=1,\n",
    "                             padding=padding, \n",
    "                             activation=activation,\n",
    "                             use_bias=use_bias,\n",
    "                             kernel_initializer=kernel_initializer,\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=l1_reg,\n",
    "                                                                   l2=l2_reg),\n",
    "                             bias_initializer='zeros',\n",
    "                             name=conv_name)\n",
    "\n",
    "        if gaussian_noise > 0:\n",
    "            conv_for = shared_conv(noisy_seq)\n",
    "            conv_rc = shared_conv(noisy_seq_rc)\n",
    "        else:\n",
    "            conv_for = shared_conv(seq_input)\n",
    "            conv_rc = shared_conv(seq_rc)\n",
    "            \n",
    "\n",
    "        merged = maximum([conv_for, conv_rc])\n",
    "        pooled = GlobalMaxPooling1D()(merged)\n",
    "        if dropout_prop > 0.0:\n",
    "            dropout = Dropout(dropout_prop)(pooled)\n",
    "            output = Dense(1, activation='sigmoid',\n",
    "                       use_bias=True,\n",
    "                       kernel_initializer=initializers.RandomUniform(minval=0.0, maxval=0.001, seed=12), \n",
    "                       kernel_constraint=non_neg(), \n",
    "                       bias_initializer='zeros',\n",
    "                       name=\"dense_1\")(dropout)\n",
    "        else:\n",
    "            output = Dense(1, activation='sigmoid',\n",
    "                           use_bias=True,\n",
    "                           kernel_initializer=initializers.RandomUniform(minval=0.0, maxval=0.001, seed=12), \n",
    "                           kernel_constraint=non_neg(), \n",
    "                           bias_initializer='zeros',\n",
    "                           name=\"dense_1\")(pooled)\n",
    "        model = Model(inputs=seq_input, outputs=output)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "def construct_scan_model(conv_weights):\n",
    "    kernel_width = conv_weights.shape[0]\n",
    "    num_kernels = conv_weights.shape[2]\n",
    "    seq = Input(shape=(None,4))\n",
    "    conv = Conv1D(num_kernels, kernel_width, \n",
    "                  name = 'scan_conv',\n",
    "                  strides=1, \n",
    "                  padding='valid', \n",
    "                  activation='linear', \n",
    "                  use_bias=False, \n",
    "                  kernel_initializer='zeros', \n",
    "                  bias_initializer='zeros',\n",
    "                  trainable=False)\n",
    "    \n",
    "    conv_seq = conv(seq)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=seq, outputs=conv_seq)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.get_layer('scan_conv').set_weights([conv_weights])\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_score_model(conv_weights):\n",
    "    kernel_width = conv_weights.shape[0]\n",
    "    num_kernels = conv_weights.shape[2]\n",
    "    seq = Input(shape=(None,4))\n",
    "    rc_op = Lambda(lambda x: K.reverse(x,axes=(1,2)))\n",
    "    seq_rc = rc_op(seq)\n",
    "    \n",
    "    conv = Conv1D(num_kernels, kernel_width, \n",
    "                  name = 'score_conv',\n",
    "                  strides=1, \n",
    "                  padding='valid', \n",
    "                  activation='linear', \n",
    "                  use_bias=use_bias, \n",
    "                  kernel_initializer='zeros', \n",
    "                  bias_initializer='zeros',\n",
    "                  trainable=False)\n",
    "    \n",
    "    conv_for = conv(seq)\n",
    "    conv_rc = conv(seq_rc)\n",
    "    \n",
    "    merged = maximum([conv_for, conv_rc])\n",
    "    pooled = GlobalMaxPooling1D()(merged)\n",
    "    model = Model(inputs=seq, outputs=pooled)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.get_layer(\"score_conv\").set_weights([conv_weights])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a24514-8408-4aa0-b633-2f761a277a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:53.382389Z",
     "iopub.status.busy": "2024-10-03T12:41:53.382233Z",
     "iopub.status.idle": "2024-10-03T12:41:53.400196Z",
     "shell.execute_reply": "2024-10-03T12:41:53.399697Z",
     "shell.execute_reply.started": "2024-10-03T12:41:53.382367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# P. Clote, Oct 2003\n",
    "\n",
    "def computeCountAndLists(s):\n",
    "\n",
    "    #Initialize lists and mono- and dinucleotide dictionaries\n",
    "    List = {} #List is a dictionary of lists\n",
    "    List['A'] = []; List['C'] = [];\n",
    "    List['G'] = []; List['T'] = [];\n",
    "    # FIXME: is this ok?\n",
    "    List['N'] = []\n",
    "    nuclList   = [\"A\",\"C\",\"G\",\"T\",\"N\"]\n",
    "    s       = s.upper()\n",
    "    #s       = s.replace(\"U\",\"T\")\n",
    "    nuclCnt    = {}  #empty dictionary\n",
    "    dinuclCnt  = {}  #empty dictionary\n",
    "    for x in nuclList:\n",
    "        nuclCnt[x]=0\n",
    "        dinuclCnt[x]={}\n",
    "        for y in nuclList:\n",
    "            dinuclCnt[x][y]=0\n",
    "\n",
    "    #Compute count and lists\n",
    "    nuclCnt[s[0]] = 1\n",
    "    nuclTotal     = 1\n",
    "    dinuclTotal   = 0\n",
    "    for i in range(len(s)-1):\n",
    "        x = s[i]; y = s[i+1]\n",
    "        List[x].append( y )\n",
    "        nuclCnt[y] += 1; nuclTotal  += 1\n",
    "        dinuclCnt[x][y] += 1; dinuclTotal += 1\n",
    "    assert (nuclTotal==len(s))\n",
    "    assert (dinuclTotal==len(s)-1)\n",
    "    return nuclCnt,dinuclCnt,List\n",
    "\n",
    "\n",
    "def chooseEdge(x,dinuclCnt):\n",
    "    z = random.random()\n",
    "    denom=dinuclCnt[x]['A']+dinuclCnt[x]['C']+dinuclCnt[x]['G']+dinuclCnt[x]['T']+dinuclCnt[x]['N']\n",
    "    numerator = dinuclCnt[x]['A']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['A'] -= 1\n",
    "        return 'A'\n",
    "    numerator += dinuclCnt[x]['C']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['C'] -= 1\n",
    "        return 'C'\n",
    "    numerator += dinuclCnt[x]['G']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['G'] -= 1\n",
    "        return 'G'\n",
    "    numerator += dinuclCnt[x]['T']\n",
    "    if z < float(numerator)/float(denom):\n",
    "        dinuclCnt[x]['T'] -= 1\n",
    "        return 'T'\n",
    "    dinuclCnt[x]['N'] -= 1\n",
    "    return 'N'\n",
    "\n",
    "def connectedToLast(edgeList,nuclList,lastCh):\n",
    "    D = {}\n",
    "    for x in nuclList: D[x]=0\n",
    "    for edge in edgeList:\n",
    "        a = edge[0]; b = edge[1]\n",
    "        if b==lastCh: D[a]=1\n",
    "    for i in range(3):\n",
    "        for edge in edgeList:\n",
    "            a = edge[0]; b = edge[1]\n",
    "            if D[b]==1: D[a]=1\n",
    "    ok = 0\n",
    "    for x in nuclList:\n",
    "        if x!=lastCh and D[x]==0: return 0\n",
    "    return 1\n",
    "\n",
    "def eulerian(s):\n",
    "    nuclCnt,dinuclCnt,List = computeCountAndLists(s)\n",
    "    #compute nucleotides appearing in s\n",
    "    nuclList = []\n",
    "    for x in [\"A\",\"C\",\"G\",\"T\",\"N\"]:\n",
    "        if x in s: nuclList.append(x)\n",
    "    #create dinucleotide shuffle L\n",
    "    firstCh = s[0]  #start with first letter of s\n",
    "    lastCh  = s[-1]\n",
    "    edgeList = []\n",
    "    for x in nuclList:\n",
    "        if x!= lastCh: edgeList.append( [x,chooseEdge(x,dinuclCnt)] )\n",
    "    ok = connectedToLast(edgeList,nuclList,lastCh)\n",
    "    return ok,edgeList,nuclList,lastCh\n",
    "\n",
    "\n",
    "def shuffleEdgeList(L):\n",
    "    n = len(L); barrier = n\n",
    "    for i in range(n-1):\n",
    "        z = int(random.random() * barrier)\n",
    "        tmp = L[z]\n",
    "        L[z]= L[barrier-1]\n",
    "        L[barrier-1] = tmp\n",
    "        barrier -= 1\n",
    "    return L\n",
    "\n",
    "def dinuclShuffle(s):\n",
    "    ok = 0\n",
    "    while not ok:\n",
    "        ok,edgeList,nuclList,lastCh = eulerian(s)\n",
    "    nuclCnt,dinuclCnt,List = computeCountAndLists(s)\n",
    "\n",
    "    #remove last edges from each vertex list, shuffle, then add back\n",
    "    #the removed edges at end of vertex lists.\n",
    "    for [x,y] in edgeList: List[x].remove(y)\n",
    "    for x in nuclList: shuffleEdgeList(List[x])\n",
    "    for [x,y] in edgeList: List[x].append(y)\n",
    "\n",
    "    #construct the eulerian path\n",
    "    L = [s[0]]; prevCh = s[0]\n",
    "    for i in range(len(s)-2):\n",
    "        ch = List[prevCh][0]\n",
    "        L.append( ch )\n",
    "        del List[prevCh][0]\n",
    "        prevCh = ch\n",
    "    L.append(s[-1])\n",
    "    #t = string.join(L,\"\")\n",
    "    t = \"\".join(L)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566cfc97-5802-4c1b-a51e-799cad8c6b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:53.741377Z",
     "iopub.status.busy": "2024-10-03T12:41:53.741222Z",
     "iopub.status.idle": "2024-10-03T12:41:53.747771Z",
     "shell.execute_reply": "2024-10-03T12:41:53.747282Z",
     "shell.execute_reply.started": "2024-10-03T12:41:53.741355Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_information_content(x):\n",
    "    ic = x * np.log2((x + .001) / .25)\n",
    "    if ic > 0:\n",
    "        return(ic)\n",
    "    else:\n",
    "        return(0.0)\n",
    "    \n",
    "def get_info_content(ppm):\n",
    "    w = ppm.shape[0]\n",
    "    info = np.zeros(w)\n",
    "    for i in range(w):\n",
    "        for j in range(4):\n",
    "            info[i] += ppm[i,j] * np.log2((ppm[i,j] + .001) / 0.25)\n",
    "    return(info)\n",
    "    \n",
    "def trim_ppm(ppm, min_info=0.0):\n",
    "    info = get_info_content(ppm)\n",
    "    start_index = 0\n",
    "    w = ppm.shape[0]\n",
    "    stop_index = w\n",
    "    for i in range(w):\n",
    "        if info[i] < min_info:\n",
    "            start_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for i in range(w):\n",
    "        if info[w-i-1] < 0.25:\n",
    "            stop_index -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if np.max(info) < 0.25:\n",
    "        return(ppm, 0, w)\n",
    "    else:\n",
    "        return(ppm[start_index:stop_index,:], start_index, stop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1ec541-b340-4148-ab86-95280c9bd2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:54.102499Z",
     "iopub.status.busy": "2024-10-03T12:41:54.102348Z",
     "iopub.status.idle": "2024-10-03T12:41:54.108740Z",
     "shell.execute_reply": "2024-10-03T12:41:54.108254Z",
     "shell.execute_reply.started": "2024-10-03T12:41:54.102477Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DNA_SEQ_DICT = {\n",
    "    'A' : [1, 0, 0, 0],\n",
    "    'C' : [0, 1, 0, 0],\n",
    "    'G' : [0, 0, 1, 0],\n",
    "    'T' : [0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "def encode_sequence(seq, N = [0, 0, 0, 0], seq_dict = None, useN = None):\n",
    "    if seq_dict is None:\n",
    "        seq_dict = DNA_SEQ_DICT\n",
    "    if useN == 'uniform':\n",
    "        N = [(1/len(seq_dict)) for _ in seq_dict]\n",
    "    elif useN == 'zeros':\n",
    "        N = [0 for _ in seq_dict]\n",
    "    d = { **seq_dict, 'N' : N }\n",
    "    return np.array([d[nuc] for nuc in list(seq)]).astype('float32')\n",
    " \n",
    "def decode_sequence(encoded_seq, seq_dict = None):\n",
    "    if seq_dict is None:\n",
    "        seq_dict = DNA_SEQ_DICT\n",
    "    seq_list = encoded_seq.astype('int').tolist()\n",
    "    def decode_base(encoded_base):\n",
    "        for letter,onehot in seq_dict.items():\n",
    "            if np.array_equal(encoded_base, onehot):\n",
    "                return letter\n",
    "        return \"N\"\n",
    "    return \"\".join(decode_base(b) for b in encoded_seq.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875247d2-9421-43b3-9cca-7f0e09fc7041",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:54.574692Z",
     "iopub.status.busy": "2024-10-03T12:41:54.574542Z",
     "iopub.status.idle": "2024-10-03T12:41:54.590360Z",
     "shell.execute_reply": "2024-10-03T12:41:54.589864Z",
     "shell.execute_reply.started": "2024-10-03T12:41:54.574671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from  tensorflow.keras.callbacks import Callback\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2,\n",
    "                 shape=\"cosine\"):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.history = {}\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        #print(fraction_to_restart)\n",
    "        if self.shape == \"cosine\":\n",
    "            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        else:\n",
    "            if fraction_to_restart < 0.5:\n",
    "                lr = fraction_to_restart * (self.max_lr - self.min_lr) / 0.5 + self.min_lr\n",
    "            else:\n",
    "                lr = (1 - fraction_to_restart) * (self.max_lr - self.min_lr) / 0.5 + self.min_lr\n",
    "        self.learning_rates.append(lr)\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        if self.shape == \"cosine\":\n",
    "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "class SWA(Callback):\n",
    "\n",
    "    def __init__(self, epochs_to_train, prop = 0.2, interval = 1):\n",
    "        super(SWA, self).__init__()\n",
    "        self.epochs_to_train = epochs_to_train\n",
    "        self.prop = prop\n",
    "        self.interval = interval\n",
    "        self.n_models = 0\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        self.weights = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        if epoch % self.interval == 0:\n",
    "            self.weights.append(self.model.get_weights())\n",
    "            self.n_models += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        num_models_to_average = int(np.ceil(self.prop * self.epoch))\n",
    "        new_weights = list()\n",
    "        for weights_list_tuple in zip(*self.weights[-num_models_to_average:]): \n",
    "            new_weights.append(\n",
    "                np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n",
    "            )\n",
    "        self.model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6773dd6-0fec-4722-b42a-d0e43d1c4d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:55.115146Z",
     "iopub.status.busy": "2024-10-03T12:41:55.114995Z",
     "iopub.status.idle": "2024-10-03T12:41:55.125887Z",
     "shell.execute_reply": "2024-10-03T12:41:55.125399Z",
     "shell.execute_reply.started": "2024-10-03T12:41:55.115125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rc(re):\n",
    "    \"\"\"\n",
    "    Return the reverse complement of a DNA/RNA RE.\n",
    "    \"\"\"\n",
    "    return re.translate(str.maketrans('ACGTURYKMBVDHSWN', 'TGCAAYRMKVBHDSWN'))[::-1]\n",
    "\n",
    "\n",
    "def count_seqs_with_words(seqs, halflength, ming, maxg, alpha, revcomp, desc):\n",
    "    if alpha == 'protein':\n",
    "        ambiguous_character = 'X'\n",
    "    else:\n",
    "        ambiguous_character = 'N'\n",
    "    gapped_kmer_dict = {}  # each key is the gapped k-mer word\n",
    "    for g in trange(ming, maxg + 1, 1, desc=desc):\n",
    "        w = g+2*halflength # length of the word\n",
    "        gap = g * ambiguous_character\n",
    "        for seq in seqs:\n",
    "            slen = len(seq)\n",
    "            for i in range(0, slen-w+1):\n",
    "                word = seq[i : i+w]\n",
    "                # skip word if it contains an ambiguous character\n",
    "                if ambiguous_character in word:\n",
    "                    continue\n",
    "                # convert word to a gapped word. Only the first and last half-length letters are preserved\n",
    "                word = word[0:halflength] + gap + word[-halflength:]\n",
    "                update_gapped_kmer_dict(gapped_kmer_dict, word, revcomp)\n",
    "    return gapped_kmer_dict\n",
    "\n",
    "\n",
    "def update_gapped_kmer_dict(gapped_kmer_dict, word, revcomp):\n",
    "    # use the lower alphabet word for rc\n",
    "    if revcomp:\n",
    "        word = min(word, get_rc(word))\n",
    "    if word in gapped_kmer_dict:  # word has been encountered before, add 1\n",
    "        gapped_kmer_dict[word] += 1\n",
    "    else:  # word has not been encountered before, create new key\n",
    "        gapped_kmer_dict[word] = 1\n",
    "\n",
    "\n",
    "def get_zscores(pos_seq_counts, neg_seq_counts):\n",
    "    zscores_dict = {}\n",
    "    for word in pos_seq_counts:\n",
    "        p = pos_seq_counts[word]\n",
    "        if word in neg_seq_counts:\n",
    "            n = neg_seq_counts[word]\n",
    "        else:\n",
    "            n = 1\n",
    "        zscore = 1.0*(p - n)/np.sqrt(n)\n",
    "        zscores_dict[word] = zscore\n",
    "    return zscores_dict\n",
    "\n",
    "\n",
    "# returns the words in order, from largest to smallest, by z-scores\n",
    "def sorted_zscore_keys(zscores_dict):\n",
    "    sorted_keys = sorted(zscores_dict, key=zscores_dict.__getitem__, reverse=True)\n",
    "    return sorted_keys\n",
    "\n",
    "\n",
    "def find_n_top_words(zscores_dict, num_find):\n",
    "    keys = np.array(list(zscores_dict.keys()))\n",
    "    values = np.array(list(zscores_dict.values()))\n",
    "    ind = np.argpartition(values, -num_find)[-num_find:]\n",
    "    top_words = list(keys[ind])\n",
    "    return top_words\n",
    "\n",
    "\n",
    "def find_enriched_gapped_kmers(pos_seqs, neg_seqs, halflength, ming, maxg, alpha, revcomp, num_find):\n",
    "    pos_seq_counts = count_seqs_with_words(pos_seqs, halflength, ming, maxg, alpha, revcomp,\n",
    "                                           'Searching positive sequences')\n",
    "    neg_seq_counts = count_seqs_with_words(neg_seqs, halflength, ming, maxg, alpha, revcomp,\n",
    "                                           'Searching negative sequences')\n",
    "    zscores = get_zscores(pos_seq_counts,neg_seq_counts)\n",
    "    top_words = find_n_top_words(zscores, num_find)\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351a4b1f-8407-4c42-a324-d35870141e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:55.542409Z",
     "iopub.status.busy": "2024-10-03T12:41:55.542259Z",
     "iopub.status.idle": "2024-10-03T12:41:55.545175Z",
     "shell.execute_reply": "2024-10-03T12:41:55.544713Z",
     "shell.execute_reply.started": "2024-10-03T12:41:55.542388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppm_to_pwm(ppm):\n",
    "    pwm = ppm + 1e-5\n",
    "    pwm = pwm / 0.25\n",
    "    pwm = np.log2(pwm)\n",
    "    return(pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1fbce72-7ce3-4f49-9208-a9bc12eb2ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:55.970034Z",
     "iopub.status.busy": "2024-10-03T12:41:55.969854Z",
     "iopub.status.idle": "2024-10-03T12:41:55.977478Z",
     "shell.execute_reply": "2024-10-03T12:41:55.976984Z",
     "shell.execute_reply.started": "2024-10-03T12:41:55.970011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataGen(Sequence):\n",
    "    def __init__(self, posSeqs, \n",
    "                 negSeqs=None,\n",
    "                 batchSize = 32,\n",
    "                 seqsPerEpoch=20000,\n",
    "                 padBy = 24):\n",
    "        \n",
    "        self.posSeqs = posSeqs\n",
    "        self.L = np.max([len(x) for x in self.posSeqs])\n",
    "        print(\"Maximum sequence length = {}\".format(self.L))\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.b2 = self.batchSize // 2\n",
    "        self.seqsPerEpoch = seqsPerEpoch\n",
    "        self.padBy = padBy\n",
    "        \n",
    "        self.labels = np.array([1 for i in range(self.b2)] + [0 for i in range(self.b2)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(int(np.floor(self.seqsPerEpoch / self.batchSize)))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        posSample = random.sample(self.posSeqs, self.b2)\n",
    "        negSample = [dinuclShuffle(_) for _ in posSample]\n",
    "            \n",
    "        X = 0.25 * np.ones((self.batchSize, 2*self.padBy + self.L, 4))\n",
    "            \n",
    "        for i,seq in enumerate(posSample + negSample):\n",
    "            l = len(seq)\n",
    "            start = self.padBy + (self.L - l) // 2\n",
    "            stop = start + l\n",
    "            X[i,start:stop,:] = encode_sequence(seq)\n",
    "            \n",
    "        return(X, self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82baf254-86af-4380-9701-16319e532778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:18:50.037129Z",
     "iopub.status.busy": "2024-10-02T15:18:50.036854Z",
     "iopub.status.idle": "2024-10-02T15:18:50.039781Z",
     "shell.execute_reply": "2024-10-02T15:18:50.039231Z",
     "shell.execute_reply.started": "2024-10-02T15:18:50.037091Z"
    }
   },
   "source": [
    "## Parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48191a4-35f5-4190-ad3c-23bf45c5e28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:57.701811Z",
     "iopub.status.busy": "2024-10-03T12:41:57.701658Z",
     "iopub.status.idle": "2024-10-03T12:41:57.705069Z",
     "shell.execute_reply": "2024-10-03T12:41:57.704567Z",
     "shell.execute_reply.started": "2024-10-03T12:41:57.701790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change me\n",
    "TF = \"LEF1\"\n",
    "cycle = \"C3\"\n",
    "epochs = 200\n",
    "\n",
    "# you can leave me alone\n",
    "n_kmers = 1\n",
    "w = 30\n",
    "n_motifs = 1\n",
    "if n_kmers > n_motifs:\n",
    "    n_kmers = n_motifs\n",
    "use_bias = True\n",
    "l2 = 0.00001\n",
    "l1 = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b977dd7-d1de-484f-981b-fee23aeefeb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:58.783397Z",
     "iopub.status.busy": "2024-10-03T12:41:58.783248Z",
     "iopub.status.idle": "2024-10-03T12:41:58.788435Z",
     "shell.execute_reply": "2024-10-03T12:41:58.787956Z",
     "shell.execute_reply.started": "2024-10-03T12:41:58.783376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../demo/PWM-A2G/LEF1-C3.ibis-formatted.txt\n",
      "FASTQ files:\n",
      " ../data//HTS/LEF1/LEF1_R0_C3_lf5ACGACGCTCTTCCGATCTAT_rf3AGCCTCAGATCGGAAGAGCA.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../demo/PWM-A2G\"\n",
    "if not os.path.exists(output_dir):\n",
    "    run(\"mkdir -p {}\".format(output_dir), shell=True)\n",
    "\n",
    "weights_file = output_dir + \"/weights.h5\"\n",
    "# temporary file to dump motif instances\n",
    "motifs_bed = output_dir + \"/motifs.bed\"\n",
    "\n",
    "# final output file of motifs instances\n",
    "motifs_file = output_dir + \"/motifs.txt.gz\"\n",
    "\n",
    "# final motif in IBIS format\n",
    "motif_out = output_dir + \"/{}-{}.ibis-formatted.txt\".format(TF, cycle)\n",
    "print(motif_out)\n",
    "\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "data_dir = \"{}/HTS/{}/\".format(data_dir, TF)\n",
    "dataFiles = glob.glob(data_dir + TF + \"_*_\" + cycle + \"*\")\n",
    "print(\"FASTQ files:\\n\", *dataFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962bfa97-f36f-4b77-b99d-7ef56a48ad20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:41:59.413432Z",
     "iopub.status.busy": "2024-10-03T12:41:59.413261Z",
     "iopub.status.idle": "2024-10-03T12:42:00.620809Z",
     "shell.execute_reply": "2024-10-03T12:42:00.620185Z",
     "shell.execute_reply.started": "2024-10-03T12:41:59.413402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 199083 total sequences\n"
     ]
    }
   ],
   "source": [
    "seqs = []\n",
    "for fastq in dataFiles:\n",
    "    with gzip.open(fastq) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i % 4 == 1:\n",
    "                seqs.append(str(line, encoding=\"utf-8\").strip().split()[0])\n",
    "\n",
    "print(\"There are {} total sequences\".format(len(seqs)))\n",
    "n = len(seqs)\n",
    "random.shuffle(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22d75c9-5cef-42c8-ab7c-405851386b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:00.622179Z",
     "iopub.status.busy": "2024-10-03T12:42:00.622010Z",
     "iopub.status.idle": "2024-10-03T12:42:04.221777Z",
     "shell.execute_reply": "2024-10-03T12:42:04.221121Z",
     "shell.execute_reply.started": "2024-10-03T12:42:00.622154Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding enriched gapped kmers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching positive sequences: 100%|██████████| 19/19 [00:01<00:00, 13.31it/s]\n",
      "Searching negative sequences: 100%|██████████| 19/19 [00:01<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched kmers...\n",
      "\tCTTNGAT\n"
     ]
    }
   ],
   "source": [
    "if n_kmers > 0:\n",
    "    print(\"Finding enriched gapped kmers\")\n",
    "    tmpPosSeqs = seqs[:5000]\n",
    "    kmers = find_enriched_gapped_kmers(tmpPosSeqs, [dinuclShuffle(_) for _ in tmpPosSeqs],  3, 0, 18, \"dna\", False, n_kmers)\n",
    "print(\"Enriched kmers...\\n\" + \"\\n\".join([\"\\t\" + _ for _ in kmers[::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2963718e-6371-4252-9e5c-47d986a86ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:04.223346Z",
     "iopub.status.busy": "2024-10-03T12:42:04.223179Z",
     "iopub.status.idle": "2024-10-03T12:42:04.279315Z",
     "shell.execute_reply": "2024-10-03T12:42:04.278783Z",
     "shell.execute_reply.started": "2024-10-03T12:42:04.223322Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length = 40\n",
      "Maximum sequence length = 40\n"
     ]
    }
   ],
   "source": [
    "holdOutP = .1\n",
    "n = len(seqs)\n",
    "\n",
    "\n",
    "trainGen = dataGen(seqs[:int((1 - holdOutP)*n)],\n",
    "                   seqsPerEpoch=5000, \n",
    "                   padBy=w)\n",
    "testGen = dataGen(seqs[int((1 - holdOutP)*n):],\n",
    "                  seqsPerEpoch=1000, padBy=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd01abb-1300-47c1-a21e-b1a4fe9d0beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:04.280518Z",
     "iopub.status.busy": "2024-10-03T12:42:04.280362Z",
     "iopub.status.idle": "2024-10-03T12:42:04.411913Z",
     "shell.execute_reply": "2024-10-03T12:42:04.411408Z",
     "shell.execute_reply.started": "2024-10-03T12:42:04.280495Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 4)      0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_conv (Conv1D)            (None, None, 1)      121         gaussian_noise[0][0]             \n",
      "                                                                 lambda[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maximum (Maximum)               (None, None, 1)      0           shared_conv[0][0]                \n",
      "                                                                 shared_conv[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 1)            0           maximum[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           global_max_pooling1d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = construct_model(num_kernels=n_motifs,\n",
    "                        kernel_width=w,\n",
    "                        use_bias=use_bias, \n",
    "                        l2_reg=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97719c9f-4ce0-4af7-b2a9-7b304edecbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:04.412882Z",
     "iopub.status.busy": "2024-10-03T12:42:04.412730Z",
     "iopub.status.idle": "2024-10-03T12:42:04.487754Z",
     "shell.execute_reply": "2024-10-03T12:42:04.487207Z",
     "shell.execute_reply.started": "2024-10-03T12:42:04.412860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_kmers > 0:\n",
    "    if use_bias:\n",
    "        conv_weights, conv_bias = model.get_layer(\"shared_conv\").get_weights()\n",
    "    else:\n",
    "        conv_weights = model.get_layer(\"shared_conv\").get_weights()[0]\n",
    "\n",
    "\n",
    "    for i in range(n_kmers):\n",
    "        kmer = kmers[i]\n",
    "        l = len(kmer)\n",
    "        conv_weights[((w - l)//2):(((w - l)//2)+l),:,i] = encode_sequence(kmer)\n",
    "\n",
    "\n",
    "    if use_bias:\n",
    "        model.get_layer(\"shared_conv\").set_weights([conv_weights, conv_bias])\n",
    "    else:\n",
    "        model.get_layer(\"shared_conv\").set_weights([conv_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80a7bcbf-e71d-499c-b7bd-c0274d42500c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:04.488789Z",
     "iopub.status.busy": "2024-10-03T12:42:04.488630Z",
     "iopub.status.idle": "2024-10-03T12:42:04.492435Z",
     "shell.execute_reply": "2024-10-03T12:42:04.491928Z",
     "shell.execute_reply.started": "2024-10-03T12:42:04.488767Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_lr = .001\n",
    "max_lr = .1\n",
    "lr_decay = (min_lr / max_lr) ** (1 / epochs)\n",
    "schedule = SGDRScheduler(min_lr=min_lr,\n",
    "                             max_lr=max_lr,\n",
    "                             steps_per_epoch=trainGen.__len__(),\n",
    "                             lr_decay=lr_decay,\n",
    "                             cycle_length=1,\n",
    "                             mult_factor=1.0, \n",
    "                             shape=\"triangular\")\n",
    "\n",
    "swa = SWA(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d592a27d-c46f-42f8-8033-45e48f6d18be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:42:04.493810Z",
     "iopub.status.busy": "2024-10-03T12:42:04.493661Z",
     "iopub.status.idle": "2024-10-03T12:46:02.275052Z",
     "shell.execute_reply": "2024-10-03T12:46:02.274201Z",
     "shell.execute_reply.started": "2024-10-03T12:42:04.493789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6911 - acc: 0.5209Epoch 1/200\n",
      "156/156 [==============================] - 1s 9ms/step - loss: 0.6915 - acc: 0.5204 - val_loss: 0.6917 - val_acc: 0.5393\n",
      "Epoch 2/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6880 - acc: 0.5400Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6884 - acc: 0.5369 - val_loss: 0.6873 - val_acc: 0.5403\n",
      "Epoch 3/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6859 - acc: 0.5468Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6862 - acc: 0.5465 - val_loss: 0.6787 - val_acc: 0.5635\n",
      "Epoch 4/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6803 - acc: 0.5540Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6804 - acc: 0.5549 - val_loss: 0.6810 - val_acc: 0.5605\n",
      "Epoch 5/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6815 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6808 - acc: 0.5585 - val_loss: 0.6770 - val_acc: 0.5635\n",
      "Epoch 6/200\n",
      "154/156 [============================>.] - ETA: 0s - loss: 0.6825 - acc: 0.5503Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6825 - acc: 0.5505 - val_loss: 0.6709 - val_acc: 0.5655\n",
      "Epoch 7/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6815 - acc: 0.5562Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6817 - acc: 0.5545 - val_loss: 0.6854 - val_acc: 0.5202\n",
      "Epoch 8/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6817 - acc: 0.5469Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6809 - acc: 0.5507 - val_loss: 0.6783 - val_acc: 0.5635\n",
      "Epoch 9/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6804 - acc: 0.5562Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6808 - acc: 0.5563 - val_loss: 0.6744 - val_acc: 0.5575\n",
      "Epoch 10/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6809 - acc: 0.5472Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6801 - acc: 0.5479 - val_loss: 0.6671 - val_acc: 0.5746\n",
      "Epoch 11/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6855 - acc: 0.5425Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6845 - acc: 0.5433 - val_loss: 0.6766 - val_acc: 0.5635\n",
      "Epoch 12/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6830 - acc: 0.5519Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6824 - acc: 0.5537 - val_loss: 0.6816 - val_acc: 0.5595\n",
      "Epoch 13/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6756 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6756 - acc: 0.5659 - val_loss: 0.6796 - val_acc: 0.5363\n",
      "Epoch 14/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6819 - acc: 0.5485Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6806 - acc: 0.5507 - val_loss: 0.6771 - val_acc: 0.5494\n",
      "Epoch 15/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6784 - acc: 0.5578Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6793 - acc: 0.5561 - val_loss: 0.6796 - val_acc: 0.5544\n",
      "Epoch 16/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6774 - acc: 0.5560Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6773 - acc: 0.5567 - val_loss: 0.6748 - val_acc: 0.5585\n",
      "Epoch 17/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6775 - acc: 0.5575Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6774 - acc: 0.5587 - val_loss: 0.6681 - val_acc: 0.5625\n",
      "Epoch 18/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6750 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6749 - acc: 0.5679 - val_loss: 0.6802 - val_acc: 0.5433\n",
      "Epoch 19/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6811 - acc: 0.5601Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6802 - acc: 0.5609 - val_loss: 0.6790 - val_acc: 0.5746\n",
      "Epoch 20/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6763 - acc: 0.5636Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6760 - acc: 0.5645 - val_loss: 0.6643 - val_acc: 0.5978\n",
      "Epoch 21/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6791 - acc: 0.5668Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6785 - acc: 0.5671 - val_loss: 0.6726 - val_acc: 0.5615\n",
      "Epoch 22/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6777 - acc: 0.5610Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6776 - acc: 0.5583 - val_loss: 0.6712 - val_acc: 0.5665\n",
      "Epoch 23/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6795 - acc: 0.5557Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6792 - acc: 0.5565 - val_loss: 0.6682 - val_acc: 0.5837\n",
      "Epoch 24/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6791 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6790 - acc: 0.5615 - val_loss: 0.6766 - val_acc: 0.5524\n",
      "Epoch 25/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6781 - acc: 0.5502Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6780 - acc: 0.5525 - val_loss: 0.6695 - val_acc: 0.5867\n",
      "Epoch 26/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6758 - acc: 0.5647Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6760 - acc: 0.5637 - val_loss: 0.6755 - val_acc: 0.5575\n",
      "Epoch 27/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6776 - acc: 0.5588Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6780 - acc: 0.5571 - val_loss: 0.6718 - val_acc: 0.5806\n",
      "Epoch 28/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6773 - acc: 0.5497Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6762 - acc: 0.5505 - val_loss: 0.6718 - val_acc: 0.5736\n",
      "Epoch 29/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6756 - acc: 0.5580Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6756 - acc: 0.5579 - val_loss: 0.6783 - val_acc: 0.5645\n",
      "Epoch 30/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6773 - acc: 0.5567Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6785 - acc: 0.5551 - val_loss: 0.6712 - val_acc: 0.5625\n",
      "Epoch 31/200\n",
      "142/156 [==========================>...] - ETA: 0s - loss: 0.6748 - acc: 0.5682Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6740 - acc: 0.5697 - val_loss: 0.6662 - val_acc: 0.5837\n",
      "Epoch 32/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6759 - acc: 0.5685Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6749 - acc: 0.5695 - val_loss: 0.6706 - val_acc: 0.5716\n",
      "Epoch 33/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6747 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6741 - acc: 0.5693 - val_loss: 0.6637 - val_acc: 0.5766\n",
      "Epoch 34/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6762 - acc: 0.5553Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6745 - acc: 0.5603 - val_loss: 0.6776 - val_acc: 0.5595\n",
      "Epoch 35/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6772 - acc: 0.5547Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6769 - acc: 0.5557 - val_loss: 0.6819 - val_acc: 0.5716\n",
      "Epoch 36/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6769 - acc: 0.5629Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6768 - acc: 0.5635 - val_loss: 0.6678 - val_acc: 0.5685\n",
      "Epoch 37/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6730 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6750 - acc: 0.5585 - val_loss: 0.6708 - val_acc: 0.5776\n",
      "Epoch 38/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6732 - acc: 0.5664Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5645 - val_loss: 0.6716 - val_acc: 0.5786\n",
      "Epoch 39/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6794 - acc: 0.5554Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6789 - acc: 0.5587 - val_loss: 0.6720 - val_acc: 0.5595\n",
      "Epoch 40/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6786 - acc: 0.5601Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6789 - acc: 0.5577 - val_loss: 0.6759 - val_acc: 0.5645\n",
      "Epoch 41/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6696 - acc: 0.5794Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5801 - val_loss: 0.6636 - val_acc: 0.5605\n",
      "Epoch 42/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6750 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6738 - acc: 0.5607 - val_loss: 0.6631 - val_acc: 0.5776\n",
      "Epoch 43/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6704 - acc: 0.5789Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6704 - acc: 0.5779 - val_loss: 0.6765 - val_acc: 0.5514\n",
      "Epoch 44/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6742 - acc: 0.5689Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6744 - acc: 0.5669 - val_loss: 0.6726 - val_acc: 0.5625\n",
      "Epoch 45/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6764 - acc: 0.5683Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6754 - acc: 0.5707 - val_loss: 0.6698 - val_acc: 0.5595\n",
      "Epoch 46/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6732 - acc: 0.5725Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6721 - acc: 0.5739 - val_loss: 0.6753 - val_acc: 0.5454\n",
      "Epoch 47/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6753 - acc: 0.5578Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5603 - val_loss: 0.6727 - val_acc: 0.5766\n",
      "Epoch 48/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6790 - acc: 0.5514Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6779 - acc: 0.5551 - val_loss: 0.6728 - val_acc: 0.5716\n",
      "Epoch 49/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6735 - acc: 0.5720Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6721 - acc: 0.5719 - val_loss: 0.6735 - val_acc: 0.5675\n",
      "Epoch 50/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6724 - acc: 0.5673Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6718 - acc: 0.5681 - val_loss: 0.6798 - val_acc: 0.5504\n",
      "Epoch 51/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6742 - acc: 0.5544Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6726 - acc: 0.5579 - val_loss: 0.6701 - val_acc: 0.5514\n",
      "Epoch 52/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6778 - acc: 0.5551Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6776 - acc: 0.5563 - val_loss: 0.6772 - val_acc: 0.5645\n",
      "Epoch 53/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6760 - acc: 0.5591Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6755 - acc: 0.5595 - val_loss: 0.6822 - val_acc: 0.5534\n",
      "Epoch 54/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6796 - acc: 0.5454Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6796 - acc: 0.5449 - val_loss: 0.6665 - val_acc: 0.5575\n",
      "Epoch 55/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5737Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6728 - acc: 0.5727 - val_loss: 0.6710 - val_acc: 0.5847\n",
      "Epoch 56/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6709 - acc: 0.5664Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6716 - acc: 0.5655 - val_loss: 0.6597 - val_acc: 0.5938\n",
      "Epoch 57/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6732 - acc: 0.5618Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6730 - acc: 0.5621 - val_loss: 0.6715 - val_acc: 0.5635\n",
      "Epoch 58/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6787 - acc: 0.5519Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6779 - acc: 0.5541 - val_loss: 0.6746 - val_acc: 0.5665\n",
      "Epoch 59/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6757 - acc: 0.5557Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5547 - val_loss: 0.6681 - val_acc: 0.5454\n",
      "Epoch 60/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6714 - acc: 0.5659Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6715 - acc: 0.5673 - val_loss: 0.6669 - val_acc: 0.5625\n",
      "Epoch 61/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6753 - acc: 0.5586Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6754 - acc: 0.5573 - val_loss: 0.6746 - val_acc: 0.5756\n",
      "Epoch 62/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6737 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6736 - acc: 0.5669 - val_loss: 0.6702 - val_acc: 0.5736\n",
      "Epoch 63/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6720 - acc: 0.5681Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5691 - val_loss: 0.6716 - val_acc: 0.5433\n",
      "Epoch 64/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6718 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6713 - acc: 0.5699 - val_loss: 0.6755 - val_acc: 0.5464\n",
      "Epoch 65/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6742 - acc: 0.5627Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6741 - acc: 0.5617 - val_loss: 0.6698 - val_acc: 0.5645\n",
      "Epoch 66/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6706 - acc: 0.5718Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5705 - val_loss: 0.6730 - val_acc: 0.5514\n",
      "Epoch 67/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6709 - acc: 0.5688Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6700 - acc: 0.5691 - val_loss: 0.6727 - val_acc: 0.5776\n",
      "Epoch 68/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6747 - acc: 0.5601Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5597 - val_loss: 0.6666 - val_acc: 0.5837\n",
      "Epoch 69/200\n",
      "142/156 [==========================>...] - ETA: 0s - loss: 0.6734 - acc: 0.5676Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6725 - acc: 0.5675 - val_loss: 0.6794 - val_acc: 0.5343\n",
      "Epoch 70/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5562Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6731 - acc: 0.5547 - val_loss: 0.6772 - val_acc: 0.5444\n",
      "Epoch 71/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6708 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6708 - acc: 0.5673 - val_loss: 0.6745 - val_acc: 0.5635\n",
      "Epoch 72/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6745 - acc: 0.5700Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6743 - acc: 0.5685 - val_loss: 0.6667 - val_acc: 0.5756\n",
      "Epoch 73/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6701 - acc: 0.5694Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6704 - acc: 0.5681 - val_loss: 0.6731 - val_acc: 0.5696\n",
      "Epoch 74/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6719 - acc: 0.5638Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6713 - acc: 0.5675 - val_loss: 0.6671 - val_acc: 0.5756\n",
      "Epoch 75/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6724 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5647 - val_loss: 0.6726 - val_acc: 0.5716\n",
      "Epoch 76/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6723 - acc: 0.5532Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6720 - acc: 0.5547 - val_loss: 0.6687 - val_acc: 0.5665\n",
      "Epoch 77/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6731 - acc: 0.5603Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6739 - acc: 0.5595 - val_loss: 0.6578 - val_acc: 0.5897\n",
      "Epoch 78/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6736 - acc: 0.5631Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6739 - acc: 0.5631 - val_loss: 0.6663 - val_acc: 0.5817\n",
      "Epoch 79/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6706 - acc: 0.5703Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6710 - acc: 0.5707 - val_loss: 0.6754 - val_acc: 0.5565\n",
      "Epoch 80/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6635 - acc: 0.5822Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6648 - acc: 0.5807 - val_loss: 0.6697 - val_acc: 0.5756\n",
      "Epoch 81/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6737 - acc: 0.5522Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6731 - acc: 0.5543 - val_loss: 0.6729 - val_acc: 0.5494\n",
      "Epoch 82/200\n",
      "154/156 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.5578Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6749 - acc: 0.5587 - val_loss: 0.6693 - val_acc: 0.5786\n",
      "Epoch 83/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6714 - acc: 0.5642Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6713 - acc: 0.5627 - val_loss: 0.6627 - val_acc: 0.5877\n",
      "Epoch 84/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6714 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6713 - acc: 0.5625 - val_loss: 0.6666 - val_acc: 0.5645\n",
      "Epoch 85/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6661 - acc: 0.5694Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6672 - acc: 0.5683 - val_loss: 0.6718 - val_acc: 0.5534\n",
      "Epoch 86/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5601Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6721 - acc: 0.5613 - val_loss: 0.6686 - val_acc: 0.5736\n",
      "Epoch 87/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6698 - acc: 0.5759Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6705 - acc: 0.5747 - val_loss: 0.6802 - val_acc: 0.5474\n",
      "Epoch 88/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6728 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6727 - acc: 0.5635 - val_loss: 0.6605 - val_acc: 0.5938\n",
      "Epoch 89/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6772 - acc: 0.5489Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6768 - acc: 0.5471 - val_loss: 0.6715 - val_acc: 0.5575\n",
      "Epoch 90/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6696 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5681 - val_loss: 0.6707 - val_acc: 0.5625\n",
      "Epoch 91/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6699 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6699 - acc: 0.5633 - val_loss: 0.6697 - val_acc: 0.5615\n",
      "Epoch 92/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6733 - acc: 0.5545Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6736 - acc: 0.5541 - val_loss: 0.6783 - val_acc: 0.5575\n",
      "Epoch 93/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6678 - acc: 0.5748Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6674 - acc: 0.5767 - val_loss: 0.6590 - val_acc: 0.5917\n",
      "Epoch 94/200\n",
      "155/156 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.5647Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6687 - acc: 0.5647 - val_loss: 0.6595 - val_acc: 0.5766\n",
      "Epoch 95/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6679 - acc: 0.5728Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6666 - acc: 0.5771 - val_loss: 0.6641 - val_acc: 0.5786\n",
      "Epoch 96/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6746 - acc: 0.5536Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6756 - acc: 0.5515 - val_loss: 0.6707 - val_acc: 0.5605\n",
      "Epoch 97/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6695 - acc: 0.5724Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6701 - acc: 0.5701 - val_loss: 0.6677 - val_acc: 0.5675\n",
      "Epoch 98/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6736 - acc: 0.5672Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6737 - acc: 0.5669 - val_loss: 0.6690 - val_acc: 0.5756\n",
      "Epoch 99/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6679 - acc: 0.5749Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6688 - acc: 0.5725 - val_loss: 0.6640 - val_acc: 0.5978\n",
      "Epoch 100/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6732 - acc: 0.5640Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5623 - val_loss: 0.6774 - val_acc: 0.5544\n",
      "Epoch 101/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6712 - acc: 0.5716Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6709 - acc: 0.5703 - val_loss: 0.6623 - val_acc: 0.5746\n",
      "Epoch 102/200\n",
      "153/156 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.5707Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6706 - acc: 0.5701 - val_loss: 0.6644 - val_acc: 0.5716\n",
      "Epoch 103/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6704 - acc: 0.5541Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5533 - val_loss: 0.6662 - val_acc: 0.5726\n",
      "Epoch 104/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6684 - acc: 0.5671Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6682 - acc: 0.5667 - val_loss: 0.6641 - val_acc: 0.5726\n",
      "Epoch 105/200\n",
      "153/156 [============================>.] - ETA: 0s - loss: 0.6731 - acc: 0.5529Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6724 - acc: 0.5541 - val_loss: 0.6725 - val_acc: 0.5565\n",
      "Epoch 106/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6727 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6726 - acc: 0.5605 - val_loss: 0.6733 - val_acc: 0.5615\n",
      "Epoch 107/200\n",
      "154/156 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.5789Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6669 - acc: 0.5781 - val_loss: 0.6677 - val_acc: 0.5726\n",
      "Epoch 108/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6770 - acc: 0.5498Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6759 - acc: 0.5511 - val_loss: 0.6635 - val_acc: 0.5968\n",
      "Epoch 109/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6755 - acc: 0.5471Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5499 - val_loss: 0.6710 - val_acc: 0.5605\n",
      "Epoch 110/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6686 - acc: 0.5704Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6688 - acc: 0.5699 - val_loss: 0.6718 - val_acc: 0.5726\n",
      "Epoch 111/200\n",
      "151/156 [============================>.] - ETA: 0s - loss: 0.6703 - acc: 0.5685Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6706 - acc: 0.5683 - val_loss: 0.6681 - val_acc: 0.5857\n",
      "Epoch 112/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6732 - acc: 0.5521Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6729 - acc: 0.5505 - val_loss: 0.6624 - val_acc: 0.5655\n",
      "Epoch 113/200\n",
      "153/156 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.5652Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6716 - acc: 0.5649 - val_loss: 0.6600 - val_acc: 0.5877\n",
      "Epoch 114/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6726 - acc: 0.5578Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6725 - acc: 0.5577 - val_loss: 0.6727 - val_acc: 0.5575\n",
      "Epoch 115/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6689 - acc: 0.5703Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6696 - acc: 0.5717 - val_loss: 0.6620 - val_acc: 0.5786\n",
      "Epoch 116/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6721 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6735 - acc: 0.5629 - val_loss: 0.6721 - val_acc: 0.5474\n",
      "Epoch 117/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6697 - acc: 0.5666Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5659 - val_loss: 0.6747 - val_acc: 0.5544\n",
      "Epoch 118/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6694 - acc: 0.5705Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6689 - acc: 0.5685 - val_loss: 0.6659 - val_acc: 0.5756\n",
      "Epoch 119/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6716 - acc: 0.5655Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6716 - acc: 0.5653 - val_loss: 0.6617 - val_acc: 0.5968\n",
      "Epoch 120/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6720 - acc: 0.5606Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6730 - acc: 0.5589 - val_loss: 0.6726 - val_acc: 0.5776\n",
      "Epoch 121/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6745 - acc: 0.5603Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6754 - acc: 0.5575 - val_loss: 0.6730 - val_acc: 0.5544\n",
      "Epoch 122/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6758 - acc: 0.5608Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6755 - acc: 0.5637 - val_loss: 0.6665 - val_acc: 0.5756\n",
      "Epoch 123/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6712 - acc: 0.5719Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6715 - acc: 0.5705 - val_loss: 0.6715 - val_acc: 0.5615\n",
      "Epoch 124/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6752 - acc: 0.5580Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6731 - acc: 0.5607 - val_loss: 0.6687 - val_acc: 0.5645\n",
      "Epoch 125/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6703 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6709 - acc: 0.5607 - val_loss: 0.6635 - val_acc: 0.5796\n",
      "Epoch 126/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6707 - acc: 0.5612Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6697 - acc: 0.5635 - val_loss: 0.6725 - val_acc: 0.5474\n",
      "Epoch 127/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6700 - acc: 0.5591Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6717 - acc: 0.5563 - val_loss: 0.6584 - val_acc: 0.5786\n",
      "Epoch 128/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6698 - acc: 0.5696Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6691 - acc: 0.5705 - val_loss: 0.6618 - val_acc: 0.5665\n",
      "Epoch 129/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6727 - acc: 0.5667Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6722 - acc: 0.5679 - val_loss: 0.6764 - val_acc: 0.5464\n",
      "Epoch 130/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6743 - acc: 0.5514Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6747 - acc: 0.5517 - val_loss: 0.6647 - val_acc: 0.5806\n",
      "Epoch 131/200\n",
      "148/156 [===========================>..] - ETA: 0s - loss: 0.6734 - acc: 0.5657Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6729 - acc: 0.5659 - val_loss: 0.6620 - val_acc: 0.5796\n",
      "Epoch 132/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6689 - acc: 0.5680Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6693 - acc: 0.5661 - val_loss: 0.6685 - val_acc: 0.5696\n",
      "Epoch 133/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6693 - acc: 0.5700Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6682 - acc: 0.5705 - val_loss: 0.6748 - val_acc: 0.5514\n",
      "Epoch 134/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6688 - acc: 0.5750Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6684 - acc: 0.5757 - val_loss: 0.6847 - val_acc: 0.5252\n",
      "Epoch 135/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6677 - acc: 0.5740Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6681 - acc: 0.5711 - val_loss: 0.6771 - val_acc: 0.5665\n",
      "Epoch 136/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6677 - acc: 0.5675Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6670 - acc: 0.5695 - val_loss: 0.6670 - val_acc: 0.5776\n",
      "Epoch 137/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6687 - acc: 0.5749Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6698 - acc: 0.5727 - val_loss: 0.6747 - val_acc: 0.5595\n",
      "Epoch 138/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6698 - acc: 0.5668Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6694 - acc: 0.5659 - val_loss: 0.6648 - val_acc: 0.5716\n",
      "Epoch 139/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6702 - acc: 0.5647Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6694 - acc: 0.5659 - val_loss: 0.6702 - val_acc: 0.5625\n",
      "Epoch 140/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6684 - acc: 0.5703Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6694 - acc: 0.5659 - val_loss: 0.6769 - val_acc: 0.5403\n",
      "Epoch 141/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5685Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6717 - acc: 0.5675 - val_loss: 0.6573 - val_acc: 0.5887\n",
      "Epoch 142/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6749 - acc: 0.5612Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6742 - acc: 0.5617 - val_loss: 0.6738 - val_acc: 0.5675\n",
      "Epoch 143/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6662 - acc: 0.5713Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6662 - acc: 0.5721 - val_loss: 0.6670 - val_acc: 0.5726\n",
      "Epoch 144/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6696 - acc: 0.5603Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5593 - val_loss: 0.6710 - val_acc: 0.5645\n",
      "Epoch 145/200\n",
      "152/156 [============================>.] - ETA: 0s - loss: 0.6708 - acc: 0.5654Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6714 - acc: 0.5647 - val_loss: 0.6693 - val_acc: 0.5565\n",
      "Epoch 146/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6733 - acc: 0.5564Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6723 - acc: 0.5573 - val_loss: 0.6659 - val_acc: 0.5655\n",
      "Epoch 147/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6751 - acc: 0.5567Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6745 - acc: 0.5571 - val_loss: 0.6596 - val_acc: 0.5837\n",
      "Epoch 148/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6710 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6706 - acc: 0.5633 - val_loss: 0.6612 - val_acc: 0.5837\n",
      "Epoch 149/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6711 - acc: 0.5668Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6703 - acc: 0.5687 - val_loss: 0.6609 - val_acc: 0.6008\n",
      "Epoch 150/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6739 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6741 - acc: 0.5575 - val_loss: 0.6650 - val_acc: 0.5766\n",
      "Epoch 151/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6716 - acc: 0.5619Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6713 - acc: 0.5623 - val_loss: 0.6735 - val_acc: 0.5585\n",
      "Epoch 152/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6674 - acc: 0.5691Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6673 - acc: 0.5709 - val_loss: 0.6620 - val_acc: 0.6008\n",
      "Epoch 153/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6720 - acc: 0.5545Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6705 - acc: 0.5567 - val_loss: 0.6752 - val_acc: 0.5585\n",
      "Epoch 154/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6690 - acc: 0.5627Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6685 - acc: 0.5633 - val_loss: 0.6687 - val_acc: 0.5665\n",
      "Epoch 155/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6756 - acc: 0.5543Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6738 - acc: 0.5569 - val_loss: 0.6663 - val_acc: 0.5665\n",
      "Epoch 156/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6670 - acc: 0.5776Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6673 - acc: 0.5759 - val_loss: 0.6598 - val_acc: 0.5635\n",
      "Epoch 157/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6703 - acc: 0.5660Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6698 - acc: 0.5657 - val_loss: 0.6812 - val_acc: 0.5474\n",
      "Epoch 158/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6702 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6700 - acc: 0.5613 - val_loss: 0.6664 - val_acc: 0.5625\n",
      "Epoch 159/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6703 - acc: 0.5668Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5671 - val_loss: 0.6718 - val_acc: 0.5575\n",
      "Epoch 160/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6652 - acc: 0.5735Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6652 - acc: 0.5717 - val_loss: 0.6604 - val_acc: 0.5917\n",
      "Epoch 161/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6724 - acc: 0.5590Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6731 - acc: 0.5579 - val_loss: 0.6669 - val_acc: 0.5595\n",
      "Epoch 162/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6681 - acc: 0.5634Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6679 - acc: 0.5663 - val_loss: 0.6678 - val_acc: 0.5635\n",
      "Epoch 163/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6654 - acc: 0.5809Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6641 - acc: 0.5833 - val_loss: 0.6715 - val_acc: 0.5605\n",
      "Epoch 164/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6693 - acc: 0.5623Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6702 - acc: 0.5593 - val_loss: 0.6694 - val_acc: 0.5565\n",
      "Epoch 165/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6759 - acc: 0.5528Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6751 - acc: 0.5559 - val_loss: 0.6677 - val_acc: 0.5575\n",
      "Epoch 166/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6682 - acc: 0.5692Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6687 - acc: 0.5651 - val_loss: 0.6668 - val_acc: 0.5736\n",
      "Epoch 167/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6649 - acc: 0.5755Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6653 - acc: 0.5749 - val_loss: 0.6553 - val_acc: 0.5907\n",
      "Epoch 168/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6739 - acc: 0.5580Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6731 - acc: 0.5605 - val_loss: 0.6726 - val_acc: 0.5575\n",
      "Epoch 169/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6733 - acc: 0.5614Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6740 - acc: 0.5585 - val_loss: 0.6708 - val_acc: 0.5433\n",
      "Epoch 170/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6713 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6715 - acc: 0.5685 - val_loss: 0.6747 - val_acc: 0.5575\n",
      "Epoch 171/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6694 - acc: 0.5676Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6693 - acc: 0.5675 - val_loss: 0.6718 - val_acc: 0.5675\n",
      "Epoch 172/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6754 - acc: 0.5608Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6747 - acc: 0.5617 - val_loss: 0.6629 - val_acc: 0.5887\n",
      "Epoch 173/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6688 - acc: 0.5665Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6686 - acc: 0.5661 - val_loss: 0.6684 - val_acc: 0.5645\n",
      "Epoch 174/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6676 - acc: 0.5742Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6686 - acc: 0.5699 - val_loss: 0.6706 - val_acc: 0.5554\n",
      "Epoch 175/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6729 - acc: 0.5586Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6723 - acc: 0.5603 - val_loss: 0.6620 - val_acc: 0.5927\n",
      "Epoch 176/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6703 - acc: 0.5728Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6693 - acc: 0.5747 - val_loss: 0.6747 - val_acc: 0.5433\n",
      "Epoch 177/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6712 - acc: 0.5599Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5609 - val_loss: 0.6709 - val_acc: 0.5494\n",
      "Epoch 178/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6705 - acc: 0.5662Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6696 - acc: 0.5701 - val_loss: 0.6576 - val_acc: 0.6200\n",
      "Epoch 179/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6690 - acc: 0.5689Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6692 - acc: 0.5689 - val_loss: 0.6755 - val_acc: 0.5685\n",
      "Epoch 180/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6767 - acc: 0.5532Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6751 - acc: 0.5555 - val_loss: 0.6576 - val_acc: 0.6018\n",
      "Epoch 181/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6726 - acc: 0.5616Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6721 - acc: 0.5629 - val_loss: 0.6605 - val_acc: 0.5877\n",
      "Epoch 182/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6719 - acc: 0.5612Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6717 - acc: 0.5617 - val_loss: 0.6590 - val_acc: 0.5806\n",
      "Epoch 183/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6693 - acc: 0.5701Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6696 - acc: 0.5695 - val_loss: 0.6700 - val_acc: 0.5726\n",
      "Epoch 184/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6706 - acc: 0.5667Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6705 - acc: 0.5673 - val_loss: 0.6657 - val_acc: 0.5746\n",
      "Epoch 185/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6695 - acc: 0.5670Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6700 - acc: 0.5669 - val_loss: 0.6614 - val_acc: 0.5948\n",
      "Epoch 186/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6711 - acc: 0.5692Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5695 - val_loss: 0.6866 - val_acc: 0.5333\n",
      "Epoch 187/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6734 - acc: 0.5603Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6732 - acc: 0.5587 - val_loss: 0.6688 - val_acc: 0.5625\n",
      "Epoch 188/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6682 - acc: 0.5690Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6677 - acc: 0.5681 - val_loss: 0.6618 - val_acc: 0.5796\n",
      "Epoch 189/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6724 - acc: 0.5547Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6734 - acc: 0.5521 - val_loss: 0.6601 - val_acc: 0.5938\n",
      "Epoch 190/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6666 - acc: 0.5716Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6663 - acc: 0.5735 - val_loss: 0.6697 - val_acc: 0.5716\n",
      "Epoch 191/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6746 - acc: 0.5584Epoch 1/200\n",
      "156/156 [==============================] - 1s 7ms/step - loss: 0.6749 - acc: 0.5581 - val_loss: 0.6665 - val_acc: 0.5726\n",
      "Epoch 192/200\n",
      "146/156 [===========================>..] - ETA: 0s - loss: 0.6731 - acc: 0.5597Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6733 - acc: 0.5601 - val_loss: 0.6645 - val_acc: 0.5655\n",
      "Epoch 193/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6753 - acc: 0.5437Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6748 - acc: 0.5461 - val_loss: 0.6648 - val_acc: 0.5786\n",
      "Epoch 194/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6650 - acc: 0.5756Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6649 - acc: 0.5761 - val_loss: 0.6689 - val_acc: 0.5867\n",
      "Epoch 195/200\n",
      "143/156 [==========================>...] - ETA: 0s - loss: 0.6664 - acc: 0.5750Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6667 - acc: 0.5735 - val_loss: 0.6737 - val_acc: 0.5575\n",
      "Epoch 196/200\n",
      "147/156 [===========================>..] - ETA: 0s - loss: 0.6616 - acc: 0.5738Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6616 - acc: 0.5735 - val_loss: 0.6766 - val_acc: 0.5675\n",
      "Epoch 197/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6746 - acc: 0.5580Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6743 - acc: 0.5585 - val_loss: 0.6560 - val_acc: 0.5766\n",
      "Epoch 198/200\n",
      "142/156 [==========================>...] - ETA: 0s - loss: 0.6707 - acc: 0.5687Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6712 - acc: 0.5655 - val_loss: 0.6597 - val_acc: 0.5867\n",
      "Epoch 199/200\n",
      "144/156 [==========================>...] - ETA: 0s - loss: 0.6666 - acc: 0.5803Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6673 - acc: 0.5777 - val_loss: 0.6723 - val_acc: 0.5554\n",
      "Epoch 200/200\n",
      "145/156 [==========================>...] - ETA: 0s - loss: 0.6675 - acc: 0.5688Epoch 1/200\n",
      "156/156 [==============================] - 1s 8ms/step - loss: 0.6688 - acc: 0.5659 - val_loss: 0.6744 - val_acc: 0.5796\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(trainGen, \n",
    "                              steps_per_epoch = trainGen.__len__(), \n",
    "                              verbose=1, \n",
    "                              epochs=epochs,\n",
    "                              workers=4,\n",
    "                              callbacks = [schedule, swa], validation_data = testGen,\n",
    "                              validation_steps = testGen.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6ece8a-491a-4630-adfc-eddd87f568a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:46:02.276474Z",
     "iopub.status.busy": "2024-10-03T12:46:02.276296Z",
     "iopub.status.idle": "2024-10-03T12:46:02.298187Z",
     "shell.execute_reply": "2024-10-03T12:46:02.297649Z",
     "shell.execute_reply.started": "2024-10-03T12:46:02.276448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0169ac83-8f56-406c-8b3a-6236d03b729e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:46:02.299579Z",
     "iopub.status.busy": "2024-10-03T12:46:02.299421Z",
     "iopub.status.idle": "2024-10-03T12:46:02.302868Z",
     "shell.execute_reply": "2024-10-03T12:46:02.302367Z",
     "shell.execute_reply.started": "2024-10-03T12:46:02.299557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_bias:\n",
    "    conv_weights, conv_bias = model.get_layer(\"shared_conv\").get_weights()\n",
    "else:\n",
    "    conv_weights = model.get_layer(\"shared_conv\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbb2eb1b-a444-4724-aecd-bb96639605ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:46:02.303989Z",
     "iopub.status.busy": "2024-10-03T12:46:02.303836Z",
     "iopub.status.idle": "2024-10-03T12:46:05.677880Z",
     "shell.execute_reply": "2024-10-03T12:46:05.677288Z",
     "shell.execute_reply.started": "2024-10-03T12:46:02.303967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6068940781250001}\n"
     ]
    }
   ],
   "source": [
    "AUCs = {}\n",
    "nSteps = 500\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for i in range(n_motifs):\n",
    "    tmp_conv_weights = np.zeros(conv_weights.shape)\n",
    "    if use_bias:\n",
    "        tmp_bias = np.zeros(n_motifs)\n",
    "    tmp_conv_weights[:,:,i] = conv_weights[:,:,i]\n",
    "    \n",
    "    if use_bias:\n",
    "        tmp_bias[i] = conv_bias[i]\n",
    "    \n",
    "    if use_bias:\n",
    "        model.get_layer(\"shared_conv\").set_weights([tmp_conv_weights, tmp_bias])\n",
    "    else:\n",
    "        model.get_layer(\"shared_conv\").set_weights([tmp_conv_weights])\n",
    "    yPred = model.predict(testGen, steps=nSteps)\n",
    "    yTest = np.array(nSteps*([1 for i in range(16)] + [0 for i in range(16)]))\n",
    "    AUCs[i] = roc_auc_score(yTest, yPred)\n",
    "print(AUCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6179085b-d928-4ea8-9e08-fb50b6f3892f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:46:05.679049Z",
     "iopub.status.busy": "2024-10-03T12:46:05.678881Z",
     "iopub.status.idle": "2024-10-03T12:46:05.763959Z",
     "shell.execute_reply": "2024-10-03T12:46:05.763403Z",
     "shell.execute_reply.started": "2024-10-03T12:46:05.679025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_model = construct_scan_model(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bf09d91-d1f4-4c73-9ff1-a03a9df0e766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:46:05.765029Z",
     "iopub.status.busy": "2024-10-03T12:46:05.764850Z",
     "iopub.status.idle": "2024-10-03T12:55:46.556870Z",
     "shell.execute_reply": "2024-10-03T12:55:46.556234Z",
     "shell.execute_reply.started": "2024-10-03T12:46:05.765004Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199083/199083 [09:40<00:00, 342.87it/s]\n"
     ]
    }
   ],
   "source": [
    "anr = False\n",
    "thresh = 0\n",
    "random.shuffle(seqs)\n",
    "with open(motifs_bed, \"w\") as f:\n",
    "    for i in tqdm.trange(len(seqs)):\n",
    "        seq = seqs[i]\n",
    "        chrom = \"seq_\" + str(i)\n",
    "        start = 1\n",
    "        stop = len(seq)\n",
    "        encoded_seq = np.vstack((0.25*np.ones((w,4)), encode_sequence(seq), 0.25*np.ones((w,4))))\n",
    "        encoded_seq_rc = encoded_seq[::-1,::-1]\n",
    "\n",
    "        conv_for = scan_model.predict(np.expand_dims(encoded_seq, axis = 0), verbose=0)[0]\n",
    "        conv_rc = scan_model.predict(np.expand_dims(encoded_seq_rc, axis = 0), verbose=0)[0]\n",
    "\n",
    "        for k in range(n_motifs):\n",
    "            if anr:\n",
    "                matches_for = np.argwhere(conv_for[:,k] > thresh)[:,0].tolist()\n",
    "                matches_rc = np.argwhere(conv_rc[:,k] > thresh)[:,0].tolist()\n",
    "                for x in matches_for:\n",
    "                    motif_start = x - w \n",
    "                    motif_end = motif_start + w\n",
    "                    score = conv_for[x,k]\n",
    "                    pfms[k] += encoded_seq[x:x+w,:]\n",
    "\n",
    "                for x in matches_rc:\n",
    "                    motif_end = x + w\n",
    "                    motif_start = motif_end - w \n",
    "                    score = conv_rc[x,k] \n",
    "                    pfms[k] += encoded_seq_rc[x:x+w,:]\n",
    "                    n_instances[k] += 1\n",
    "                \n",
    "            else:\n",
    "                maxFor = np.max(conv_for[:,k])\n",
    "                maxRC = np.max(conv_rc[:,k])\n",
    "\n",
    "                if maxFor > thresh or maxRC > thresh:\n",
    "                    if maxFor > maxRC:\n",
    "                        x = np.argmax(conv_for[:,k])\n",
    "                        motif_start = x - w \n",
    "                        motif_end = motif_start + w\n",
    "                        score = conv_for[x,k]\n",
    "                        motifSeq = decode_sequence(encoded_seq[x:x+w,:])\n",
    "                        print(chrom, start+motif_start, start+motif_end, k, score, \"+\", motifSeq, seq, file=f, sep=\"\\t\")\n",
    "                    else:\n",
    "                        x = np.argmax(conv_rc[:,k])\n",
    "                        motif_end = x + w\n",
    "                        motif_start = motif_end - w \n",
    "                        score = conv_rc[x,k] \n",
    "                        motifSeq = decode_sequence(encoded_seq_rc[x:x+w,:])\n",
    "                        print(chrom, stop-motif_start, stop-motif_start+w, k, score, \"-\", motifSeq, seq, file=f, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40919af0-b641-499b-8a5b-08819eb5b3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:55:46.558052Z",
     "iopub.status.busy": "2024-10-03T12:55:46.557886Z",
     "iopub.status.idle": "2024-10-03T12:55:52.041430Z",
     "shell.execute_reply": "2024-10-03T12:55:52.040868Z",
     "shell.execute_reply.started": "2024-10-03T12:55:46.558028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(motifs_bed, sep=\"\\t\", names=[\"chrom\", \"start\", \"stop\", \"kernel\", \"score\", \"strand\", \"seq\", \"og_seq\"])\n",
    "df[\"auc\"] = df.kernel.map(AUCs)\n",
    "df.to_csv(motifs_file, sep=\"\\t\", header=True, index=False)\n",
    "os.remove(motifs_bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13c73e4b-77e8-4845-8532-6fca12272889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:55:52.043273Z",
     "iopub.status.busy": "2024-10-03T12:55:52.043107Z",
     "iopub.status.idle": "2024-10-03T12:55:54.382742Z",
     "shell.execute_reply": "2024-10-03T12:55:54.382240Z",
     "shell.execute_reply.started": "2024-10-03T12:55:52.043250Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(4,4, figsize=(20,8), tight_layout=True)\n",
    "for plot_i, i in enumerate(np.argsort(list(AUCs.values()))[::-1]):\n",
    "    n = df[df[\"kernel\"] == i].shape[0]\n",
    "    if n > 0:\n",
    "        auc = AUCs[i]\n",
    "        ax = axes.flatten()[plot_i]\n",
    "        ppm = logomaker.alignment_to_matrix(df[df[\"kernel\"] == i].seq, to_type=\"counts\")\n",
    "        for nuc in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "            if nuc not in ppm.columns:\n",
    "                ppm[nuc] = 0.0\n",
    "        ppm = ppm[[\"A\", \"C\", \"G\", \"T\"]]\n",
    "        ppm = ppm.div(ppm.sum(axis=1), axis=0)\n",
    "        logomaker.Logo(ppm.applymap(get_information_content), ax=ax)\n",
    "        ax.set_ylim([0,2])\n",
    "        ax.set_title(\"Kernel {0}; N = {1}; auROC = {2:.2f}\".format(i, n, auc))\n",
    "    else:\n",
    "        ax = axes.flatten()[plot_i]\n",
    "        fig.delaxes(ax)\n",
    "        \n",
    "for i in range(n_motifs, 16):\n",
    "    fig.delaxes(axes.flatten()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7186db88-2c8f-4830-b2da-f614fd41f1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:55:54.383828Z",
     "iopub.status.busy": "2024-10-03T12:55:54.383673Z",
     "iopub.status.idle": "2024-10-03T12:55:54.994237Z",
     "shell.execute_reply": "2024-10-03T12:55:54.993704Z",
     "shell.execute_reply.started": "2024-10-03T12:55:54.383806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GaussianMixture(2).fit(df.score.values.reshape(-1,1))\n",
    "x = np.linspace(np.min(df.score.values), np.max(df.score.values), 1000)\n",
    "logprob = model.score_samples(x.reshape(-1, 1))\n",
    "probs = model.predict_proba(x.reshape(-1, 1))\n",
    "means = model.means_\n",
    "index = np.argmax(means)\n",
    "thresh = x[np.where(probs[:,index] > 0.5)[0]][0]\n",
    "responsibilities = model.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.hist(df.score, 100, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.vlines(x = thresh ,ymin=ymin, ymax=ymax)\n",
    "tmp_df = df[(df[\"score\"] > thresh)]\n",
    "ppm = logomaker.alignment_to_matrix(tmp_df.seq, to_type=\"counts\")\n",
    "ppm = ppm[[\"A\", \"C\", \"G\", \"T\"]]\n",
    "ppm = ppm.div(ppm.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb3037fc-5280-4b5a-a059-c01c38443fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:55:54.995218Z",
     "iopub.status.busy": "2024-10-03T12:55:54.995064Z",
     "iopub.status.idle": "2024-10-03T12:55:55.362793Z",
     "shell.execute_reply": "2024-10-03T12:55:55.362312Z",
     "shell.execute_reply.started": "2024-10-03T12:55:54.995196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,2))\n",
    "logomaker.Logo(ppm.applymap(get_information_content), ax=ax)\n",
    "ax.set_ylim([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70656cb4-0e09-4761-ac57-74943227896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T12:55:55.363732Z",
     "iopub.status.busy": "2024-10-03T12:55:55.363581Z",
     "iopub.status.idle": "2024-10-03T12:55:55.372865Z",
     "shell.execute_reply": "2024-10-03T12:55:55.372398Z",
     "shell.execute_reply.started": "2024-10-03T12:55:55.363710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(motif_out, \"w\") as f:\n",
    "    print(\">{0} {0}_{1}\".format(TF, cycle), file=f)\n",
    "    for i in range(ppm.shape[0]):\n",
    "        print(\" \".join([\"%.5f\" % v for v in ppm.values[i,:]]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af879525-3ad8-45a8-a4ae-0b4827e1a3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
